{
  "hash": "3611ccecfd25599686785890bcb526c2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Algorithms Template\"\nauthor: \"Saffron Evergreen\"\ndate: \"2023-08-25\"\ncategories: [code, machine learning, templates]\n---\n\n\nThe brains behind this template goes to [Jason Brownlee](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/). Everything you find below is *most definitely not* my work; it is either Jason's brains, the encyclopedia of notes I've accumulated from various courses or ChatGPT. I'm keeping this as a rough outline for coding and deciphering which algorithms to use in the future, which as I pick away at these, I can add my own code and thoughts but until then... *nothing below is authentically mine and I take minimal credit.*\n\n# Set-up and Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(eval = FALSE, fig.height=5, fig.width=7, message = F, warning = F)\n###\n# code is unable to run given \"eval = FALSE\"\n# input items as needed and change the r setup to \"echo = TRUE\" when trying this out elsewhere\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(readr, tidyr, magrittr, knitr, tidyverse, janitor, broom, dplyr, \n       GGally, # ggpairs\n       forcats, # factor manipulation\n       caret, \n  ### below are for caret ### \n       lattice, \n       kernlab, \n       ellipse, \n       randomForest)\n```\n:::\n\n\n# Categorizing Algorithms by Learning Style\n\nCan be grouped by learning style or similarity \\[1\\].\n\nTerms and definitions from \\[1\\].\\\nR Code from \\[\\*\\].\n\n## Supervised Learning\n\nExample problems:\\\n\\* Classification\\\n\\* Regression\n\nExample algorithms:\\\n\\* Logistic regression\\\n\\* Back Propagation Neural Network\n\n## Unsupervised Learning\n\nExample problems:\\\n\\* Clustering\\\n\\* Dimensionality reduction\\\n\\* Association rule learning\n\nExample algorithms:\\\n\\* Apriori algorithm\\\n\\* K-Means\n\n## Semi-Supervised Learning\n\nExample problems:\\\n\\* Classification\\\n\\* Regression\n\nExample algorithms:\\\n\\* flexible methods that make assumptions about how to model unlabeled data\n\n# Categorizing Algorithms by Similarity\n\nTerms and definitions from \\[1\\].\\\nMix of my code and ChatGPT code \\[\\*\\]\n\n## Regression Algorithms\n\n-   modeling the relationship between variables, iteratively refined using a measure of error in the predictions made by the model \\[1\\]\n\n### Ordinary Least Squares Regression (OLSR)\n\n-   predicting y based on x \\[\\*\\]\\\n-   goal is to minimize the sum of squared residuals, which means finding the line that best fits the data by minimizing the vertical distance between the data points and the line \\[\\*\\]\\\n-   broader concept than linear regression; estimates the coefficients (the intercept and slope) in a linear regression model... linear regression encompasses the entire process of modelling the relationship between variables \\[\\*\\]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Generate example data\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100)\n\n# Create a data frame from the data\ndata <- data.frame(x = x, y = y)\n\n\n### Use existing data\n# Fit the OLS regression model   \n\n### linear relationship\nmodel <- lm(y ~ x, data = data)  \n\n### nonlinear relationships\nmodel <- glm(y ~ x, \n             family = \"\", \n             data = \"\")\n\n# Print model summary\nsummary(model)\n```\n:::\n\n\n### Linear Regression\n\n-   predicting y based on x \\[\\*\\]\\\n-   goal is to minimize the values of $\\beta_0$ and $\\beta_1$ that minimize the sum of squared residuals, which is the differences between the observed and predicted values \\[\\*\\]\n\n$$\ny = \\beta_0 ~+~ \\beta_1x ~+~ \\epsilon\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Generate example data where mean = 0 and sd = 1\nset.seed(123)\nx <- seq(1, 10, by = 0.1)\ny <- 2 * x + 3 + rnorm(length(x), mean = 0, sd = 1)\n\n# Create a data frame from the data\ndata <- data.frame(x = x, y = y)\n\n### Use existing data\n# Fit the linear regression model\nmodel <- lm(y ~ x, data = data)\n\n# Print model summary\nsummary(model)\n\n# Make predictions using the model\nnew_data <- data.frame(x = c(11, 12, 13))\npredictions <- predict(model, newdata = new_data)\n```\n:::\n\n\n### Logistic Regression\n\n-   for binary/binomial outcome variable (0 or 1)\\\n-   predicts 0 or 1 for y based off of several x's\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Generate example data\nset.seed(123)\nn <- 100\nx <- rnorm(n)\ny <- as.factor(ifelse(2 * x + rnorm(n) > 0, 1, 0))\n\n# Create a data frame from the data\ndata <- data.frame(x = x, y = y)\n\n### Use existing data\n# Fit the logistic regression model\nmodel <- glm(y ~ x, \n             data = data, \n             family = \"binomial\")\n\n# Print model summary\nsummary(model)\n\n# Make predictions using the model\nnew_data <- data.frame(x = c(0.5, 1.0, 1.5))\n\npredicted_probs <- predict(model, \n                           newdata = new_data, \n                           type = \"response\") # p-values/probabilities\n\n# Print predicted probabilities\ncat(\"Predicted Probabilities:\", predicted_probs, \"\\n\")\n```\n:::\n\n\n### Stepwise Regression\n\n-   a process where predictor variables are added or removed from a regression model based on statistical criteria \\[\\*\\]\\\n-   can be sensitive to the order of predictor variables \\[\\*\\]\\\n-   has some drawbacks, should look more into this...\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Generate example data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\ny <- 2 * x1 + 3 * x2 + rnorm(n)\n\n# Create a data frame from the data\ndata <- data.frame(x1 = x1, x2 = x2, y = y)\n\n### Use existing data \n# Fit the initial full model\nfull_model <- lm(y ~ ., data = data)\n\n# Perform stepwise regression\nstepwise_model <- step(full_model, \n                       direction = \"both\") # can add or remove predictor variables\n\n# Print the summary of the stepwise model\nsummary(stepwise_model)\n```\n:::\n\n\n### Multivariate Adaptive Regression Splines (MARS)\n\n-   a flexible and powerful technique for capturing complex relationships between variables \\[\\*\\]\\\n-   can be useful for capturing nonlinear relationships in your data \\[\\*\\]\\\n-   look more into intrepretation and validation...\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the necessary package\ninstall.packages(\"earth\")\nlibrary(earth)\n\n### Generate example data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\ny <- 2 * x1 + 3 * x2 + rnorm(n)\n\n# Create a data frame from the data\ndata <- data.frame(x1 = x1, x2 = x2, y = y)\n\n### Use existing data\n# Fit the MARS model\nmars_model <- earth(y ~ x1 + x2, \n                    data = data)\n\n# Print the summary of the MARS model\nsummary(mars_model)\n```\n:::\n\n\n### Locally Estimated Scatterplot Smoothing (LOESS)\n\n-   non-parametric tecnique used for fitting smooth curves to scatterplots \\[\\*\\]\\\n-   the span parameter in the loess function controls the amount of smoothing; smaller span values result in more smoothing, larger span values result in less smoothing \\[\\*\\]\\\n-   LOESS can be influenced by the span parameter and is more computationally intensive than some other regression techniques \\[\\*\\]\\\n-   more ideal for smaller datasets and when assessing EDA and visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Generate example data\nset.seed(123)\nx <- seq(0, 2 * pi, length.out = 100)\ny <- sin(x) + rnorm(100, mean = 0, sd = 0.2)\n\n### Use existing data \n# Fit the LOESS model\nloess_model <- loess(y ~ x) # default: span = 0.75\n\n# Generate predicted values using the LOESS model\npredicted_values <- predict(loess_model, \n                            data.frame(x = x))\n\n# Plot the original data and the LOESS curve\nplot(x, y, \n     main = \"LOESS Smoothing\")\n\nlines(x, \n      predicted_values, \n      col = \"red\", \n      lwd = 2) \n```\n:::\n\n\n## Instance-based Algorithms\n\n-   instances like building up a database of example data and compare new data to the example database \\[1\\]\n    -   uses a similarity measure in order to find the best match and make a prediction \\[1\\]\\\n    -   also called memory-based learning \\[1\\]\\\n    -   focuses on the representation of stored instances and similarity measures used between instances \\[1\\]\n\n### k-Nearest Neighbor (kNN)\n\n-   class labels are determined based on a linear relationship between the predictor variables (x1 and x2) \\[\\*\\]\\\n-   using the 'class' package, you can use kNN regression using the 'knn.reg' function \\[\\*\\]\\\n-   a simple method\\\n-   non-parametric\\\n-   can handle numerical and categorical data; can capture non-linear relationships \\[\\*\\]\\\n-   can be effective for detecting outliers and anomalies; also sensitive to outliers and scaling \\[\\*\\]\\\n-   not ideal for large datasets\\\n-   look more into finding the optimal k value...\n\n**One method: rando data, not from a dataset \\[\\*\\]**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load the necessary package\ninstall.packages(\"class\")\nlibrary(class)\n\n### Generate example data\nset.seed(123)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\nclass_labels <- factor(ifelse(2 * x1 + x2 + rnorm(n) > 0, \"A\", \"B\"))\n\n# Create a data frame from the data\ndata <- data.frame(x1 = x1, x2 = x2, class = class_labels)\n\n### Use existing data   \n# Split data into training and test sets\ntrain_indices <- sample(n, n * 0.7)\ntrain_data <- data[train_indices, ]\ntest_data <- data[-train_indices, ]\n\n# Fit the kNN classifier\nk <- 3  # Set the number of neighbors\nknn_model <- knn(train_data[, c(\"x1\", \"x2\")], test_data[, c(\"x1\", \"x2\")], train_data$class, k)\n\n# Print the predicted class labels\ncat(\"Predicted Class Labels:\", knn_model, \"\\n\")\n\n# Calculate accuracy\naccuracy <- sum(knn_model == test_data$class) / nrow(test_data)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n:::\n\n\n**Another method: from a dataset \\[\\*\\]**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data <- data[sample_indices, ]\n\ntest_data <- data[-sample_indices, ]\n\n# Define the number of neighbors (k) for KNN\nk <- 3  # You can choose an appropriate value for your problem\n\n# Train the KNN model\nknn_model <- knn(train = \n                   train_data[, -target_column],  # Exclude target variable\n                 test = \n                   test_data[, -target_column],    # Exclude target variable\n                 cl = \n                   train_data$target_column,        # Target variable\n                 k = k)\n\n# Evaluate the model\n# Here, you can use metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\n\naccuracy <- sum(knn_model == \n                  test_data$target_column) / \n                  length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n:::\n\n\n### Learning Vector Quantization (LVQ)\n\n-   supervised ML algorithm used for classification\\\n-   purpose: classify input data into **predefined categories** or classes by adjusting a set of prototype vectors (reference points for each class)\n    -   the algorithm learns to adjust them during training to make accurate predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lvq)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data <- data[sample_indices, ]\n\ntest_data <- data[-sample_indices, ]\n\n# Define the number of prototypes per class and other hyperparameters\nnum_prototypes <- 2  # Number of prototypes per class\nlearning_rate <- 0.1  # Learning rate\nepochs <- 100  # Number of training epochs\n\n# Extract the features (independent variables) and labels (target variable)\ntrain_features <- train_data[, -target_column]  # Exclude target variable\ntrain_labels <- train_data$target_column  # Target variable\n\n# Train the LVQ model\nlvq_model <- lvq.train(train_features, \n                       train_labels, \n                       num_prototypes = num_prototypes, \n                       learning.rate = learning_rate, \n                       epochs = epochs)\n\n# Make predictions on the test data\ntest_features <- test_data[, -target_column]  # Exclude target variable\npredictions <- lvq.predict(lvq_model, test_features)\n\n# Evaluate the model\n# You can use metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\naccuracy <- sum(predictions == test_data$target_column)/ \n                length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n:::\n\n\n### Self-Organizing Map (SOM)\n\n-   unsupervised ML algorithm used for dimensionality reduction and visualization of high-dimensional data\n    -   purpose: map complex, high-dimensional data onto a lower-dimensional grid in such a way that it preserves the topological properties and relationships of the original data\\\n    -   often used for clustering, visualization, and data exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(kohonen)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Normalize your data (if needed)\n# SOMs are sensitive to data scaling, so it's often a good idea to normalize your data\n# You can use functions like scale() or min-max normalization\n\n# Create a SOM grid\n# You need to specify the grid dimensions (e.g., grid rows and columns)\n# and other parameters like the learning rate and neighborhood function type\ngrid_rows <- 5\ngrid_cols <- 5\nsom_grid <- somgrid(grid_rows, grid_cols, \"hexagonal\")\n\n# Train the SOM\nsom_model <- som(data, grid = som_grid, rlen = 100, alpha = c(0.05, 0.01))\n\n# Plot the SOM\n# This step helps visualize the resulting SOM and cluster assignments\nplot(som_model)\n\n# You can also identify cluster assignments for your data points\ncluster_assignments <- predict(som_model, newdata = data)\n\n# Explore and analyze the results further based on your problem's objectives\n```\n:::\n\n\n### Locally Weighted Learning (LWL)\n\n-   gives more weight to nearby data points and less weight to those farther away\n    -   purpose: provide flexible and adaptive modeling, where the prediction for a new data point is based on the contributions of its neighboring data points\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(locfit)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data <- data[sample_indices, ]\n\ntest_data <- data[-sample_indices, ]\n\n# Define the bandwidth parameter for LWL\nbandwidth <- 0.2  # You can choose an appropriate value for your problem\n\n# Train the LWL model\nlwl_model <- locfit::locfit(target_variable ~ predictors, \n                            data = train_data, \n                            alpha = bandwidth)\n\n# Make predictions on the test data\npredictions <- predict(lwl_model, \n                       newdata = data.frame(predictors = \n                                              test_data$predictors))\n\n# Evaluate the model\n# You can use appropriate evaluation metrics based on your problem\n# For regression, you might use mean squared error, and for classification, you might use accuracy.\n```\n:::\n\n\n### Support Vector Machines (SVM)\n\n-   supervised ML algorithm used for classification and regression\n    -   purpose: to find a hyperplane (or decision boundary) that best separates different classes in a dataset\n        -   goal is to maximize the margin between the data points of different classes while minimizing classification errors\\\n        -   useful for high-dimensional data; is effective in handling complex classification problems\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)  # for SVM\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data <- data[sample_indices, ]\n\ntest_data <- data[-sample_indices, ]\n\n# Define the SVM model\n# Here, we'll use a linear kernel for simplicity\nsvm_model <- svm(target_column ~ ., \n                 data = train_data, \n                 kernel = \"linear\")\n\n# Make predictions on the test data\npredictions <- predict(svm_model, \n                       newdata = \n                         test_data[, -target_column])\n\n# Evaluate the model\n# Here, you can use metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\naccuracy <- sum(predictions == test_data$target_column) / length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n:::\n\n\n## Regularization Algorithms\n\n-   an extension to another method (i.e., regression methods) \\[1\\]\\\n-   penalizes models based on complexity; favors simpler models that are more generalizable \\[1\\]\n\n### Ridge Regression\n\n-   used in linear regression to prevent overfitting and improve the models generalization by adding a \"penalty term\" to the regression equation\n    -   purpose: to skrink the coefficients of the features towards zero while still maintaining all the geatures in the model\n        -   helps reduce multicollinearity\\\n        -   leads to more stable and reliable predictions\\\n-   for datasets where features are highly correlated\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data <- data[sample_indices, ]\n\ntest_data <- data[-sample_indices, ]\n\n# Separate the target variable and predictors\ny_train <- train_data$target_column\nX_train <- train_data[, -target_column]\n\n# Fit a ridge regression model using glmnet\n# You can specify the lambda (penalty) parameter to control the strength of regularization\n# Smaller values of lambda result in stronger regularization\n# cv.glmnet performs cross-validation to select an optimal lambda value\nridge_model <- cv.glmnet(x = as.matrix(X_train), \n                         y = y_train, \n                         alpha = 0, \n                         nfolds = 10)\n\n# Print the optimal lambda value selected by cross-validation\nbest_lambda <- ridge_model$lambda.min\ncat(\"Optimal Lambda:\", best_lambda, \"\\n\")\n\n# Fit the final ridge regression model with the optimal lambda\nfinal_ridge_model <- glmnet(x = as.matrix(X_train), \n                            y = y_train, \n                            alpha = 0, \n                            lambda = best_lambda)\n\n# Make predictions on the test set\nX_test <- test_data[, -target_column]\npredictions <- predict(final_ridge_model, \n                       newx = as.matrix(X_test))\n\n# Evaluate the model's performance\n# You can use appropriate regression metrics like RMSE, R-squared, etc., depending on your problem\n# For simplicity, we'll use Mean Squared Error (MSE) as an example\nmse <- mean((predictions - test_data$target_column)^2)\ncat(\"Mean Squared Error:\", mse, \"\\n\")\n```\n:::\n\n\n### Least Absolute Shrinkage and Selection Operator (LASSO)\n\n-   used to prevent overfitting and perform feature selection by adding a penalty term to the linear regression cost function\\\n-   encourages sparse models by pushing some of the coefficient values to exactly 0, which effectively removes those features from the model\n    -   it balances a trade-off between model complexity and goodness-of-fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into predictors (X) and target variable (y)\nX <- data[, -target_column]  # Exclude the target variable\ny <- data$target_column      # Target variable\n\n# Standardize predictors (recommended for LASSO)\nX <- scale(X)\n\n# Set up the lambda values (penalty parameter)\n# You can use cross-validation to determine the optimal lambda value\nlambda_values <- 10^seq(10, -2, length = 100)\n\n# Fit LASSO regression model\nlasso_model <- glmnet(X, \n                      y, \n                      alpha = 1, \n                      lambda = lambda_values)\n\n# Plot the LASSO coefficient paths (optional)\nplot(lasso_model)\n\n# Select the best lambda value using cross-validation (optional)\ncv_model <- cv.glmnet(X, \n                      y, \n                      alpha = 1)\nbest_lambda <- cv_model$lambda.min\ncat(\"Best Lambda:\", best_lambda, \"\\n\")\n\n# Fit the LASSO model with the best lambda\nlasso_model_best_lambda <- glmnet(X, \n                                  y, \n                                  alpha = 1, \n                                  lambda = best_lambda)\n\n# Print the coefficients of the LASSO model\ncoef(lasso_model_best_lambda)\n```\n:::\n\n\n### Elastic Net\n\n-   combines L1 (LASSO) and L2 (ridge regression) methods\n    -   purpose: to overcome some of the limitations of each of those two methods by adding a regularization term that is a linear combination of L1 and L2 regularization penalties\n-   there are two hyperparameters, alpha and lambda, that control the balance\n    -   when alpha is 0, it's equal to ridge regression and when alpha is 1, it's equal to LASSO\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into predictors (X) and the target variable (Y)\nX <- data[, -target_column]  # Exclude target variable\nY <- data$target_column\n\n# Define the alpha and lambda values\nalpha <- 0.5  # You can choose a value between 0 (Ridge) and 1 (Lasso)\nlambda <- 0.01  # Regularization strength parameter\n\n# Create an Elastic Net model\nelastic_net_model <- glmnet(X, \n                            Y, \n                            alpha = alpha, \n                            lambda = lambda)\n\n# Make predictions using the model\n# You can replace 'new_data' with your test data if needed\npredicted_values <- predict(elastic_net_model, \n                            new_data, \n                            s = lambda)\n\n# Optionally, you can also plot the regularization path\nplot(elastic_net_model)\n```\n:::\n\n\n### Least-Angle Regression (LARS)\n\n-   used in linear regression\\\n-   purpose: to produce a sparse model by gradually adding predictors (X's) to the model while controlling the size of the coefficients\n    -   ideal for high-dimensional data where you want to identify the most relevant features while avoiding overfitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lars)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Separate predictors (features) and the target variable\nX <- data[, -target_column]  # Exclude the target variable\ny <- data$target_column      # Target variable\n\n# Fit the LARS model\nlars_model <- lars(X, \n                   y, \n                   type = \"lasso\")  # LARS with L1 (Lasso) regularization\n\n# Print the model summary\nprint(summary(lars_model))\n\n# Get the selected features (variables)\nselected_features <- names(X)[which(lars_model$active != 0)]\ncat(\"Selected Features:\", selected_features, \"\\n\")\n```\n:::\n\n\n## Decision Tree Algorithms\n\n-   typically for classification and regression problems \\[1\\]\\\n-   often fast, accurate and tres popular\n-   construct a model of decisions made based on actual values in the data \\[1\\]\\\n-   create tree structures until a prediction decision is made for a given record \\[1\\]\n\n### Classification and Regression Tree (CART)\n\n-   used both classification and regression\\\n-   purpose: to create a tree-like model that can make predictions by recursively splitting the data into subsets based on the values of input features\\\n-   commonly used for predicting categories (classification) or predicting numeric values (regression) based on input features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)  # for CART algorithm\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data <- data[sample_indices, ]\n\ntest_data <- data[-sample_indices, ]\n\n# Define the CART model\n# For classification tasks, specify the target variable as a factor\n# For regression tasks, specify the target variable as numeric\n# Replace 'target_column' with the actual name of your target variable\ncart_model <- rpart(target_column ~ ., data = train_data, method = \"class\")\n\n# Make predictions using the CART model\n# For classification tasks, replace 'new_data' with your test data and specify 'type = \"class\"'\n# For regression tasks, replace 'new_data' with your test data and specify 'type = \"vector\"'\npredictions <- predict(cart_model, newdata = test_data, type = \"class\")\n\n# Evaluate the model\n# Here, you can use metrics like accuracy (for classification) or RMSE (for regression), depending on your problem\n# For simplicity, we'll use accuracy as an example for classification\naccuracy <- sum(predictions == test_data$target_column) / length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n:::\n\n\n### Iterative Dichotomiser 3 (ID3)\n\n-   used for building decision trees and data mining\\\n-   purpose: to classify or predict a target variable based on a set of input features\n    -   recursively selects the best attribute to split the data into subsets that are as pure as possible in terms of the target variable\\\n-   an old algorithm, doesn't exist in R's core libraries\\\n-   can create an algorithm similar to ID3 using recursive functions in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\n# Define a recursive function to build the ID3 decision tree\nid3 <- function(data, target_attr, attributes) {\n  # Create a new node for the decision tree\n  node <- list()\n  \n  # If all instances have the same class label, return that label as a leaf node\n  if (length(unique(data[[target_attr]])) == 1) {\n    node$leaf <- TRUE\n    node$class <- unique(data[[target_attr]])[1]\n    return(node)\n  }\n  \n  # If there are no more attributes to split on, return the majority class as a leaf node\n  if (length(attributes) == 0) {\n    node$leaf <- TRUE\n    node$class <- names(sort(table(data[[target_attr]]), decreasing = TRUE))[1]\n    return(node)\n  }\n  \n  # Select the best attribute to split on based on information gain or entropy\n  best_attr <- select_best_attribute(data, target_attr, attributes)\n  \n  # Set the current node's attribute to the best attribute\n  node$attribute <- best_attr\n  \n  # Create child nodes for each value of the best attribute\n  node$children <- list()\n  for (value in unique(data[[best_attr]])) {\n    # Create a subset of the data where the best attribute has the specified value\n    subset_data <- data %>%\n      filter(data[[best_attr]] == value)\n    \n    # Recursively build the tree for the subset\n    child <- id3(subset_data, target_attr, setdiff(attributes, best_attr))\n    \n    # Add the child node to the current node\n    node$children[[as.character(value)]] <- child\n  }\n  \n  return(node)\n}\n\n# Define a function to select the best attribute based on information gain or entropy\nselect_best_attribute <- function(data, target_attr, attributes) {\n  # Implement your method for attribute selection here (e.g., using information gain or entropy)\n  # This function should return the name of the best attribute to split on\n  # You can replace this with your specific attribute selection logic\n  # Example:\n  # return(attributes[1])\n}\n\n# Example usage\n# Replace 'your_dataset.csv' with the actual file path or URL to your dataset\n# Replace 'target_attribute' with the name of your target attribute\ndata <- read.csv(\"your_dataset.csv\")\ntarget_attribute <- \"target_attribute\"\n\n# Get a list of all attributes except the target attribute\nall_attributes <- setdiff(names(data), target_attribute)\n\n# Build the ID3 decision tree\ndecision_tree <- id3(data, target_attribute, all_attributes)\n\n# Print the decision tree or use it for predictions\nprint(decision_tree)\n```\n:::\n\n\n### Iterative C4.5 and C5.0\n\n-   iterative enhancements which are used in ML and data classification\\\n-   purpose: to improve the decision tree's accuracy and generalization by iteratively refining the tree through processes like pruning and re-splitting nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(C50)  # for C5.0 algorithm\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\nset.seed(123)  # for reproducibility\nsample_indices <- sample(nrow(data), size = 0.8 * nrow(data))\ntrain_data <- data[sample_indices, ]\ntest_data <- data[-sample_indices, ]\n\n# Train the C5.0 model\nc50_model <- C5.0(train_data[, -target_column], train_data$target_column)\n\n# Predict using the trained model\npredictions <- predict(c50_model, test_data)\n\n# Evaluate the model\n# You can use various metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\naccuracy <- sum(predictions == test_data$target_column) / length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n```\n:::\n\n\n### Chi-squared Automatic Interaction Detection (CHAID)\n\n-   iterative decision tree for predictive modeling and classification\\\n-   purpose: create a decision tree that recursively splits the data into homogenous groups based on the most significant categorical predictor variables (X's)\n    -   useful for analyzing categorical data\\\n    -   identifies relationships between categorical predictor variables and a categorical target variable by performing chi-squared tests for independence to find the most influential predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(partykit)\n\n# Load your dataset\ndata <- read.csv(\"your_dataset.csv\")\n\n# Define your target variable and predictor variables\n# Replace 'target_column' and 'predictor_columns' with the actual column names\ntarget_column <- \"target_variable\"\npredictor_columns <- c(\"predictor_1\", \"predictor_2\", \"predictor_3\")\n\n# Create a formula for the CHAID model\nformula <- as.formula(paste(target_column, \"~\", paste(predictor_columns, collapse = \"+\")))\n\n# Build the CHAID decision tree model\nchaide_tree <- chaid(formula, data = data)\n\n# Visualize the CHAID decision tree\nplot(chaide_tree)\n\n# Summary of the tree\nprint(chaide_tree)\n\n# Predict using the CHAID decision tree model\n# Replace 'new_data' with the data you want to make predictions on\nnew_data <- data.frame(predictor_1 = c(...), predictor_2 = c(...), predictor_3 = c(...))\npredictions <- predict(chaide_tree, newdata = new_data)\n\n# Evaluate the model and make further analyses as needed\n```\n:::\n\n\n### Decision Stump\n\n### M5\n\n### Conditional Decision Trees\n\n## Bayesian Algorithms   \n\n- conditional probability for two events (A|B)   \n\n$$\nP(A \\mid B) = \\frac{P(B \\cap A)}{P(B)}\n$$   \n\n$$\n{P(B \\cap A)} = {P(B|A)}{P(A)}\n$$   \n\n\n- the formula above expresses that the probability that B occurs and causes A is equivalent to the probability of B and A occurring, divided by the probability of B occurring. \n\n-   explictly apply Bayes' Theorem for classification and regression probems \\[1\\]   \n- \n\n### Naive Bayes\n\n### Gaussian Naive Bayes\n\n### Multinomial Naive Bayes\n\n### Averaged One-Dependence Estimators (AODE)\n\n### Bayesian Belief Network (BBN)\n\n### Bayesian Network (BN)\n\n## Clustering Algorithms\n\n-   describes the class of problem and class of methods \\[1\\]\\\n-   organized by modeling approaches such as 'centroid-based' and 'hierarchal' \\[1\\]\\\n-   all methods use structures in the data to best organize the data into groups of maximum commonality \\[1\\]\n\n### k-Means\n\n### k-Medians\n\n### Expectation Maximization (EM)\n\n### Hierarchical Clustering\n\n## Association Rule Learning Algorithms\n\n-   extract rules that best explain observed relationshipns between variables in the dataset \\[1\\]\\\n-   can discover useful associations in multidimensional datasets \\[1\\]\n\n### Apriori algorithm\n\n### Eclat algorithm\n\n## Artifical Neural Network Algorithms\n\n-   inspired by the structure and/or function of biological neural networks \\[1\\]\\\n-   class of pattern matching used in regression and classification problems \\[1\\]\\\n-   subfield of hundreds of algorithms and variations for types of problems \\[1\\]\n\n### Perceptron\n\n### Multilayer Perceptrons (MLP)\n\n### Back-Propagation\n\n### Stochastic Gradient Descent\n\n### Hopfield Network\n\n### Radial Basis Function Network (RBFN)\n\n## Deep Learning Algorithms\n\n-   a modern update to 'Artificial Neural Networks' that exploit abundant cheap computation \\[1\\]\\\n-   much larger and more complex neural networks \\[1\\]\\\n-   use very large datasets of labelled analog data such as image, text, audio and video \\[1\\]\n\n### Convolutional Neural Network (CNN)\n\n### Recurrent Neural Networks (RNNs)\n\n### Long Short-Term Memory Networks (LSTMs)\n\n### Stacked Auto-Encoders\n\n### Deep Boltzmann Machine (DBM)\n\n### Deep Belief Networks (DBN)\n\n## Dimensionality Reduction Algorithms\n\n-   similar to clustering methods; this seeks and exploits the structure in the data - can be unsupervised or order to summarize or describe data using less information \\[1\\]\\\n-   can be used to visualize dimensional data or simplify data that can be used in a supervised learning method \\[1\\]\\\n-   can be used in classification and regression problems \\[1\\]\n\n### Principal Component Analysis (PCA)\n\n### Principal Component Regression (PCR)\n\n### Partial Least Squares Regression (PLSR)\n\n### Sammon Mapping\n\n### Multidimensional Scaling (MDS)\n\n### Projection Pursuit\n\n### Linear Discriminant Analysis (LDA)\n\n### Mixture Discriminant Analysis (MDA)\n\n### Quadratic Discriminant Analysis (QDA)\n\n### Flexible Discriminant Analysis (FDA)\n\n## Ensemble Algorithms\n\n-   models composed of several weaker models that are independently trained and whose predictions are combined to make an overall prediction \\[1\\]\\\n-   powerful and popular\n\n### Boosting\n\n### Bootstrapped Aggregation (Bagging)\n\n### AdaBoost\n\n### Weighted Average (Blending)\n\n### Stacked Generalization (stacking)\n\n### Gradient Boosting Machines (GBM)\n\n### Gradient Boosted Regression Trees (GBRT)\n\n### Random Forest\n\n## Other Machine Learning Algorithms\n\n### Feature selection algorithms\n\n### Algorithm accuracy evaluation\n\n### Performance measures\n\n### Optimization algorithms\n\n## Other Algorithms for Specialty Subfields\n\n### Computational intelligence (evolutionary algorithms)\n\n### Computer Vision (CV)\n\n### Natural Language Processing (NLP)\n\n### Recommender Systems\n\n### Reinforcement Learning\n\n### Graphical Models\n\n# Sources\n\n\\[1\\] https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "f7fe70cb5b26dec80b7879df482be0bc",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Walk-Through\"\nauthor: \"Saffron Evergreen\"\ndate: \"2023-08-24\"\ncategories: [code, visualizations, machine learning]\n---\n\n\n\nI will be following the process found on this [site](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/) by Jason Brownlee.   \n\n_An ode to template creation_    \n\nSo this author is using the iris flowers dataset, which I will use too. However, if time allows, I'll create a mirror project with another dataset. The researcher behind the iris flowers research was a eugenist [post source here; his name Edgar Anderson].\n\n\n# 1. Define Problem     \n\n## Characteristics for Data   \n\n1. Numeric columns   \n2. Classification problems (i.e., inputs of characteristics to determine the iris)     \n3. Few columns and 100-200 observations    \n4. All columns are in the same unit and scale; otherwise scaling and transformations would need to occur   \n\n# 2. Prepare Data       \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, fig.height=5, fig.width=7, message = F, warning = F)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(readr, tidyr, magrittr, knitr, tidyverse, janitor, broom, dplyr, caret, \n       lattice, # for caret\n       kernlab, # caret\n       randomForest) # caret\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"iris\") \ndataset <- iris  \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\n# 80% to train models, 20% validation dataset  \n\n# split the data into two sets\nvalidation_index <- caret::createDataPartition(\n  dataset$Species, \n  p = 0.80, \n  list = FALSE\n)\n\n# 20% used for validation  \nvalidation <- dataset[-validation_index, ]\n\n# 80% used for training and testing  \ndataset <- dataset[validation_index, ]\n```\n:::\n\n\n\n## Summarize the Dataset     \n\n### Dimensions    \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 120   5\n```\n:::\n:::\n\n\n### Types of Columns    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(dataset, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\" \n```\n:::\n:::\n\n\n### Data Peek    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n8          5.0         3.4          1.5         0.2  setosa\n```\n:::\n:::\n\n\n\n### Levels of the Species Column     \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using base\nlevels(dataset$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n```\n:::\n\n```{.r .cell-code}\n# 3+ categories in a column : multinomial \n# 2 categories in a column: binomial/binary\n```\n:::\n\n\n#### Breakdown of Species Column   \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using base\npercentage <- prop.table(table(dataset$Species)) * 100 \n\ncbind(freq = table(\n  dataset$Species), \n  percentage = percentage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           freq percentage\nsetosa       40   33.33333\nversicolor   40   33.33333\nvirginica    40   33.33333\n```\n:::\n:::\n\n\n### Summary of all Columns    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.100   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.575   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.300   Median :1.300  \n Mean   :5.853   Mean   :3.058   Mean   :3.768   Mean   :1.217  \n 3rd Qu.:6.425   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.700   Max.   :2.500  \n       Species  \n setosa    :40  \n versicolor:40  \n virginica :40  \n                \n                \n                \n```\n:::\n:::\n\n\n## Visualizations    \n\n### Univariate    \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- dataset[, 1:4]\ny <- dataset[,5]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2x2 layout for boxplots\npar(mfrow = c(1,4))\n\n# loop through each column of 'x' to create a boxplot\nfor(i in 1:4){\n  boxplot(x[ ,i], main = names(iris)[i])}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/boxplots-1.png){width=672}\n:::\n:::\n\n\n### Multivariate   \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visually assessing interactions between the variables, 3 colors to represent species types\n\n# caret::featurePlot(\n#  x = x, \n#  y = y, \n#  plot = \"ellipse\"\n#)\n\n### Error message:\n# Error in grid.Call.graphics(C_downviewport, name$name, strict) :\n# Viewport 'plot_01.panel.1.1.off.vp' was not founda\n\n# \"pairs\" without \"ellipse\" is most similar\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatterplot matrix-1.png){width=672}\n:::\n\n```{.r .cell-code}\n### another attempt with a different package \n\nlibrary(GGally)\nggpairs(dataset, columns = 1:4, \n        ggplot2::aes(color = y))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatterplot matrix-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"box\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/box and whisper plots-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# density plot for each category of species \n\nscales <- list(\n  x = list(\n    relation = \"free\"), \n  y = list(\n    relation = \"free\"\n))\n\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"density\", \n                   scales = scales)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/density plots-1.png){width=672}\n:::\n:::\n\n\n# 3. Evaluate Algorithms    \n\n## Test harness: 10-fold cross validation   \n_This method estimates accuracy_   \n* Splits data into 10 parts   \n* Trains 9 parts, tests 1 part   \n* Releases all combinations of train-test splits    \n* Repeat 3 times for each algorithm for more accurate estimate    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run algorithms using 10-fold cross validation   \n\ncontrol <- caret::trainControl(method = \"cv\", \n                               number = 10)  \n\nmetric <- \"Accuracy\"\n```\n:::\n\n\n_Formula for accuracy_   \n\n$$\n\\text{accuracy} = \\frac{\\text{correctly predicted instances}}{\\text{total number of instances}} ~\\cdot~ 100\n$$  \n\n## Build 5 different models for prediction    \n\n__Evaluate 5 different algorithms__   \n\n_simple linear method_   \n1. Linear Discriminant Analysis (LDA)   \n_nonlinear method_   \n2. Classification and Regression Trees (CART)    \n3. k-Nearest Neighbors (kNN)    \n_complex nonlinear method_   \n4. Support Vector Machines (SVM) with a linear kernel   \n5. Random Forest (RF)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### set seed before each run \n\n# simple linear\n### LDA  \nset.seed(7)\n\nfit.lda <- caret::train(Species~., \n                        data = dataset, \n                        method = \"lda\", \n                        metric = metric, \n                        trControl = control)\n\n\n# nonlinear\n### CART  \nset.seed(7) \n\nfit.cart <- caret::train(Species~., \n                        data = dataset, \n                        method = \"rpart\", \n                        metric = metric, \n                        trControl = control)\n\n### kNN  \nset.seed(7)   \n\nfit.knn <- caret::train(Species~., \n                        data = dataset, \n                        method = \"knn\", \n                        metric = metric, \n                        trControl = control)\n\n# complex nonlinear\n### SVM  \nset.seed(7)   \n\nfit.svm <- caret::train(Species~., \n                        data = dataset, \n                        method = \"svmRadial\", \n                        metric = metric, \n                        trControl = control)\n\n### RF  \nset.seed(7)  \n\nfit.rf <- caret::train(Species~., \n                        data = dataset, \n                        method = \"rf\", \n                        metric = metric, \n                        trControl = control)\n\n### Note from website: \n  # caret can configure and tune the configuration of each model but that isn't covered in the tutorial I'm going off of\n```\n:::\n\n\n## Select the best model     \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize accuracy of models  \nresults <- caret::resamples(\n  list(\n    lda = fit.lda, \n    cart = fit.cart, \n    knn = fit.knn, \n    svm = fit.svm, \n    rf = fit.rf\n  )\n)\n\nsummary(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsummary.resamples(object = results)\n\nModels: lda, cart, knn, svm, rf \nNumber of resamples: 10 \n\nAccuracy \n          Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's\nlda  0.9166667 0.9375000 1.0000000 0.9750000       1    1    0\ncart 0.8333333 0.9166667 1.0000000 0.9583333       1    1    0\nknn  0.8333333 0.9166667 1.0000000 0.9583333       1    1    0\nsvm  0.9166667 0.9166667 0.9583333 0.9583333       1    1    0\nrf   0.8333333 0.9166667 0.9583333 0.9416667       1    1    0\n\nKappa \n      Min. 1st Qu. Median   Mean 3rd Qu. Max. NA's\nlda  0.875 0.90625 1.0000 0.9625       1    1    0\ncart 0.750 0.87500 1.0000 0.9375       1    1    0\nknn  0.750 0.87500 1.0000 0.9375       1    1    0\nsvm  0.875 0.87500 0.9375 0.9375       1    1    0\nrf   0.750 0.87500 0.9375 0.9125       1    1    0\n```\n:::\n:::\n\n### Plot model evaluation results   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlattice::dotplot(results)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# the most accurate model is the LDA\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit.lda)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Discriminant Analysis \n\n120 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 108, 108, 108, 108, 108, 108, ... \nResampling results:\n\n  Accuracy  Kappa \n  0.975     0.9625\n```\n:::\n\n```{.r .cell-code}\n# standard deviation of \"accuracy\" and \"kappa\" were shown in the tutorial   \n\n### haven't figured it out yet... TBC...\n```\n:::\n\n\n# 4. Predictions + Conclusion    \n\n__Assessing the accuracy of the best fit model 'fit.lda' on the validation set__   \n* It's ideal to have a validation set in case the model is overfit   \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- stats::predict(fit.lda, validation)\n\nconfusionMatrix(predictions, validation$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         10          0         0\n  versicolor      0         10         0\n  virginica       0          0        10\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8843, 1)\n    No Information Rate : 0.3333     \n    P-Value [Acc > NIR] : 4.857e-15  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           1.0000\nSpecificity                 1.0000            1.0000           1.0000\nPos Pred Value              1.0000            1.0000           1.0000\nNeg Pred Value              1.0000            1.0000           1.0000\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3333           0.3333\nDetection Prevalence        0.3333            0.3333           0.3333\nBalanced Accuracy           1.0000            1.0000           1.0000\n```\n:::\n:::\n\n__Explanation__:    \nAccuracy is 100%. The validation dataset is small (20%) but is within our expected margin of error $97\\% ~\\pm~4\\%$. Concluding that we _might_ have an accurate and a reliably accurate model.    \n\n\n\n_End notes: That was fun as heck. Shout out to Jason_   ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "30e61c670910b707f95789b981c49e94",
  "result": {
    "markdown": "---\ntitle: \"Machine Learning Template\"\nauthor: \"Saffron Evergreen\"\ndate: \"2023-08-24\"\ncategories: [code, machine learning, templates]\n---   \n\n\n\nInspired by 'Machine Learning Walk-Through'; it reignited my undying love for template creation.   \n\nThe brains behind this template goes to [Jason Brownlee](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/).   \n\n\n\n# Set-up and Libraries   \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(eval = FALSE, fig.height=5, fig.width=7, message = F, warning = F)\n# eval = FALSE : show code, will not run code \n\n### change eval = FALSE to \n\n    ## echo = TRUE \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(readr, tidyr, magrittr, knitr, tidyverse, janitor, broom, dplyr, \n       GGally, # ggpairs\n       forcats, # factor manipulation\n       caret, \n  ### below are for caret ### \n       lattice, \n       kernlab, \n       ellipse, \n       randomForest)\n```\n:::\n\n\n# Load data  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# if in CRAN repository\ndata(\"[dataset here]\") \n\n# 'dataset' for ease\n```\n:::\n\n\n# 1. Clean and Wrangle Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456789)\n\n# []% to train models, []% validation dataset  \n\n# split the data into two sets\nvalidation_index <- caret::createDataPartition(\n  dataset$OUTCOMEVARIABLE, # Outcome variable/factored\n  p = 0.80, # percentage of dataset used for training\n  list = FALSE\n)\n\n# [20]% used for validation  \nvalidation <- dataset[-validation_index, ]\n\n# [80]% used for training and testing  \ndataset <- dataset[validation_index, ]\n```\n:::\n\n\n## Summary of dataset   \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(dataset) # make sure columns were retained \n\nhead(dataset)\n\nsummary(dataset)\n\nglimpse(dataset)\n```\n:::\n\n\n## Types of columns/variables   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(dataset, class)\n```\n:::\n\n\n## Transform variable types as needed   \n\n* As used in the 'iris' dataset, it is simpler to have explanatory variables be numeric and the outcome/predictor variable be a factor.   \n\n## Assess factored levels in outcome variable   \n\n**Using Base R**   \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using base\nlevels(dataset$OUTCOMEVARIABLE)\n# 3+ categories in a column : multinomial \n# 2 categories in a column: binomial/binary\n```\n:::\n\n\n\n**Using Forcats**   \n\n\n::: {.cell}\n\n:::\n\n\n\n# 2. Visualizations   \n\n## Univariate   \n\n* this will show an individual plot for each explanatory variable by the outcome variable in side-by-side format\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- dataset[, 1:4] # example 1:4 cols 1:4 are explanatory/numeric variables\ny <- dataset[,5] # example col 5 is the predictor/outcome/factored variable\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2x2 layout for boxplots\npar(mfrow = c(1,4)) # 1 row, 4 columns\n\n# loop through each column of 'x' to create a boxplot\nfor(i in 1:4){\n  boxplot(x[ ,i], main = names(RAWDATASET)[i])} # shows all combined species by the 4 explanatory variables\n```\n:::\n\n\n## Multivariate   \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visually assessing interactions between the variables, colored by factor levels\n\n caret::featurePlot(\n  x = x, \n  y = y, \n  plot = \"ellipse\"\n)\n\n# \"pairs\" without \"ellipse\" is similar\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"pairs\")\n\n### an alternative version using GGally\n\nGGally::ggpairs(dataset, \n                columns = 1:4, # select explanatory variables\n        ggplot2::aes(color = y)) # color by factor levels of outcome\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"box\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# density plot for each level in the outcome variable \n\nscales <- list(\n  x = list(\n    relation = \"free\"), \n  y = list(\n    relation = \"free\"\n))\n\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"density\", \n                   scales = scales)\n```\n:::\n\n\n\n# 3. Evaluate Algorithms for Accurate Estimates      \n\n## Test harness: 10-fold cross validation   \n_This method estimates accuracy_   \n* Splits data into 10 parts   \n* Trains 9 parts, tests 1 part   \n* Releases all combinations of train-test splits    \n* Repeat 3 times for each algorithm for more accurate estimate    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run algorithms using 10-fold cross validation   \n\ncontrol <- caret::trainControl(method = \"cv\", \n                               number = 10)  \n\nmetric <- \"Accuracy\"\n```\n:::\n\n\n_Formula for accuracy_   \n\n$$\n\\text{accuracy} = \\frac{\\text{correctly predicted instances}}{\\text{total number of instances}} ~\\cdot~ 100\n$$  \n\n\n## Build 5 different models for prediction    \n\n__Evaluate 5 different algorithms__   \n\n_simple linear method_   \n1. Linear Discriminant Analysis (LDA)   \n_nonlinear method_   \n2. Classification and Regression Trees (CART)    \n3. k-Nearest Neighbors (kNN)    \n_complex nonlinear method_   \n4. Support Vector Machines (SVM) with a linear kernel   \n5. Random Forest (RF)  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n### set seed before each run \n\n# simple linear\n### LDA  \nset.seed(7)\n\nfit.lda <- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"lda\", \n                        metric = metric, \n                        trControl = control)\n\n\n# nonlinear\n### CART  \nset.seed(7) \n\nfit.cart <- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"rpart\", \n                        metric = metric, \n                        trControl = control)\n\n### kNN  \nset.seed(7)   \n\nfit.knn <- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"knn\", \n                        metric = metric, \n                        trControl = control)\n\n# complex nonlinear\n### SVM  \nset.seed(7)   \n\nfit.svm <- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"svmRadial\", \n                        metric = metric, \n                        trControl = control)\n\n### RF  \nset.seed(7)  \n\nfit.rf <- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"rf\", \n                        metric = metric, \n                        trControl = control)\n\n### Note from website: \n  # caret can configure and tune the configuration of each model but that isn't done here (at least not yet)\n```\n:::\n\n\n## Select the best model     \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize accuracy of models  \nresults <- caret::resamples(\n  list(\n    lda = fit.lda, \n    cart = fit.cart, \n    knn = fit.knn, \n    svm = fit.svm, \n    rf = fit.rf\n  )\n)\n\nsummary(results)\n```\n:::\n\n\n### Plot model evaluation results   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlattice::dotplot(results)\n# visualize the accuracy of each model\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(CHOSENMODEL)\n# standard deviation of \"accuracy\" and \"kappa\" were shown in the tutorial   \n\n### haven't figured it out yet... TBC...\n```\n:::\n\n\n# 4. Predictions + Conclusion    \n\n__Assessing the accuracy of the best fit model on the validation set__   \n* It's ideal to have a validation set in case the model is over fit   \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- stats::predict(CHOSENMODEL, validation)\n\ncaret::confusionMatrix(predictions, validation$OUTCOMEVARIABLE)\n```\n:::\n\n\n__Explanation__:    \nAccuracy is []%. \n\nThe validation dataset is [20]% but [is within our expected margin $97\\% ~\\pm~4\\%$]. \n\nConcluding that we _might_ have an [accurate and a reliably accurate] model.    \n\n_Note to self_: I need to refresh my brain on determining the margin; I thought the standard was plus or minus 3% but I don't know if the technical reasons of why really matter right now. (They matter)   \n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
---
title: "Machine Learning Template"
author: "Saffron Evergreen"
date: "2023-08-24"
categories: [code, machine learning, templates]
---

Inspired by 'Machine Learning Walk-Through'; it reignited my undying love for template creation.

The brains behind this template goes to [Jason Brownlee](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/).

# Set-up and Libraries

```{r setup, include=TRUE}
knitr::opts_chunk$set(eval = FALSE, fig.height=5, fig.width=7, message = F, warning = F)
# eval = FALSE : show code, will not run code 

### change eval = FALSE to 

    ## echo = TRUE 
```

```{r packages}
library(pacman)
p_load(readr, tidyr, magrittr, knitr, tidyverse, janitor, broom, dplyr, 
       GGally, # ggpairs
       forcats, # factor manipulation
       caret, 
  ### below are for caret ### 
       lattice, 
       kernlab, 
       ellipse, 
       randomForest)
```

# Load data

```{r load data}
# if in CRAN repository
data("[dataset here]") 

# 'dataset' for ease
```

# 1. Clean and Wrangle Data

```{r split dataset}
set.seed(123456789)

# []% to train models, []% validation dataset  

# split the data into two sets
validation_index <- caret::createDataPartition(
  dataset$OUTCOMEVARIABLE, # Outcome variable/factored
  p = 0.80, # percentage of dataset used for training
  list = FALSE
)

# [20]% used for validation  
validation <- dataset[-validation_index, ]

# [80]% used for training and testing  
dataset <- dataset[validation_index, ]
```

## Summary of dataset

```{r dim}
dim(dataset) # make sure columns were retained 

head(dataset)

summary(dataset)

glimpse(dataset)
```

## Types of columns/variables

```{r}
sapply(dataset, class)
```

## Transform variable types as needed

-   As used in the 'iris' dataset, it is simpler to have explanatory variables be numeric and the outcome/predictor variable be a factor.

## Assess factored levels in outcome variable

**Using Base R**

```{r}
# using base
levels(dataset$OUTCOMEVARIABLE)
# 3+ categories in a column : multinomial 
# 2 categories in a column: binomial/binary
```

**Using Forcats**

```{r}

```

# 2. Visualizations

## Univariate

-   this will show an individual plot for each explanatory variable by the outcome variable in side-by-side format

```{r split x and y}
x <- dataset[, 1:4] # example 1:4 cols 1:4 are explanatory/numeric variables
y <- dataset[,5] # example col 5 is the predictor/outcome/factored variable
```

```{r boxplots}
# 2x2 layout for boxplots
par(mfrow = c(1,4)) # 1 row, 4 columns

# loop through each column of 'x' to create a boxplot
for(i in 1:4){
  boxplot(x[ ,i], main = names(RAWDATASET)[i])} # shows all combined species by the 4 explanatory variables
```

## Multivariate

```{r scatterplot matrix}
# visually assessing interactions between the variables, colored by factor levels

 caret::featurePlot(
  x = x, 
  y = y, 
  plot = "ellipse"
)

# "pairs" without "ellipse" is similar
caret::featurePlot(x = x, 
                   y = y, 
                   plot = "pairs")

### an alternative version using GGally

GGally::ggpairs(dataset, 
                columns = 1:4, # select explanatory variables
        ggplot2::aes(color = y)) # color by factor levels of outcome
```

```{r box and whisper plots}
caret::featurePlot(x = x, 
                   y = y, 
                   plot = "box")
```

```{r density plots}
# density plot for each level in the outcome variable 

scales <- list(
  x = list(
    relation = "free"), 
  y = list(
    relation = "free"
))

caret::featurePlot(x = x, 
                   y = y, 
                   plot = "density", 
                   scales = scales)
```

# 3. Evaluate Algorithms for Accurate Estimates

## Test harness: 10-fold cross validation

*This method estimates accuracy*\
\* Splits data into 10 parts\
\* Trains 9 parts, tests 1 part\
\* Releases all combinations of train-test splits\
\* Repeat 3 times for each algorithm for more accurate estimate

```{r test harness}
# run algorithms using 10-fold cross validation   

control <- caret::trainControl(method = "cv", 
                               number = 10)  

metric <- "Accuracy"
```

*Formula for accuracy*

$$
\text{accuracy} = \frac{\text{correctly predicted instances}}{\text{total number of instances}} ~\cdot~ 100
$$

## Build 5 different models for prediction

**Evaluate 5 different algorithms**

*simple linear method*\
1. Linear Discriminant Analysis (LDA)\
*nonlinear method*\
2. Classification and Regression Trees (CART)\
3. k-Nearest Neighbors (kNN)\
*complex nonlinear method*\
4. Support Vector Machines (SVM) with a linear kernel\
5. Random Forest (RF)

```{r build models}
### set seed before each run 

# simple linear
### LDA  
set.seed(7)

fit.lda <- caret::train(OUTCOMEVARIABLE~., 
                        data = dataset, 
                        method = "lda", 
                        metric = metric, 
                        trControl = control)


# nonlinear
### CART  
set.seed(7) 

fit.cart <- caret::train(OUTCOMEVARIABLE~., 
                        data = dataset, 
                        method = "rpart", 
                        metric = metric, 
                        trControl = control)

### kNN  
set.seed(7)   

fit.knn <- caret::train(OUTCOMEVARIABLE~., 
                        data = dataset, 
                        method = "knn", 
                        metric = metric, 
                        trControl = control)

# complex nonlinear
### SVM  
set.seed(7)   

fit.svm <- caret::train(OUTCOMEVARIABLE~., 
                        data = dataset, 
                        method = "svmRadial", 
                        metric = metric, 
                        trControl = control)

### RF  
set.seed(7)  

fit.rf <- caret::train(OUTCOMEVARIABLE~., 
                        data = dataset, 
                        method = "rf", 
                        metric = metric, 
                        trControl = control)

### Note from website: 
  # caret can configure and tune the configuration of each model but that isn't done here (at least not yet)
```

## Select the best model

```{r model accuracy}
# summarize accuracy of models  
results <- caret::resamples(
  list(
    lda = fit.lda, 
    cart = fit.cart, 
    knn = fit.knn, 
    svm = fit.svm, 
    rf = fit.rf
  )
)

summary(results)
```

### Plot model evaluation results

```{r}
lattice::dotplot(results)
# visualize the accuracy of each model
```

```{r}
print(CHOSENMODEL)
# standard deviation of "accuracy" and "kappa" were shown in the tutorial   

### haven't figured it out yet... TBC...
```

# 4. Predictions + Conclusion

**Assessing the accuracy of the best fit model on the validation set**\
\* It's ideal to have a validation set in case the model is over fit

```{r lda on validation}
predictions <- stats::predict(CHOSENMODEL, validation)

caret::confusionMatrix(predictions, validation$OUTCOMEVARIABLE)
```

**Explanation**:\
Accuracy is \[\]%.

The validation dataset is \[20\]% but \[is within our expected margin $97\% ~\pm~4\%$\].

Concluding that we *might* have an \[accurate and a reliably accurate\] model.

*Note to self*: I need to refresh my brain on determining the margin; I thought the standard was plus or minus 3% but I don't know if the technical reasons of why really matter right now. (They matter)

---
title: "Algorithms Template"
author: "Saffron Evergreen"
date: "2023-08-25"
categories: [code, machine learning, templates]
---

The brains behind this template goes to [Jason Brownlee](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/). Everything you find below is *most definitely not* my work; it is either Jason's brains, the encyclopedia of notes I've accumulated from various courses or ChatGPT. I'm keeping this as a rough outline for coding and deciphering which algorithms to use in the future, which as I pick away at these, I can add my own code and thoughts but until then... *nothing below is authentically mine and I take minimal credit.*

# Set-up and Libraries

```{r setup, include=TRUE}
knitr::opts_chunk$set(eval = FALSE, fig.height=5, fig.width=7, message = F, warning = F)
###
# code is unable to run given "eval = FALSE"
# input items as needed and change the r setup to "echo = TRUE" when trying this out elsewhere
```

```{r packages}
library(pacman)
p_load(readr, tidyr, magrittr, knitr, tidyverse, janitor, broom, dplyr, 
       GGally, # ggpairs
       forcats, # factor manipulation
       caret, 
  ### below are for caret ### 
       lattice, 
       kernlab, 
       ellipse, 
       randomForest)
```

# Categorizing Algorithms by Learning Style

Can be grouped by learning style or similarity \[1\].

Terms and definitions from \[1\].\
R Code from \[\*\].

## Supervised Learning

Example problems:\
\* Classification\
\* Regression

Example algorithms:\
\* Logistic regression\
\* Back Propagation Neural Network

## Unsupervised Learning

Example problems:\
\* Clustering\
\* Dimensionality reduction\
\* Association rule learning

Example algorithms:\
\* Apriori algorithm\
\* K-Means

## Semi-Supervised Learning

Example problems:\
\* Classification\
\* Regression

Example algorithms:\
\* flexible methods that make assumptions about how to model unlabeled data

# Categorizing Algorithms by Similarity

Terms and definitions from \[1\].\
Mix of my code and ChatGPT code \[\*\]

## Regression Algorithms

-   modeling the relationship between variables, iteratively refined using a measure of error in the predictions made by the model \[1\]

### Ordinary Least Squares Regression (OLSR)

-   predicting y based on x \[\*\]\
-   goal is to minimize the sum of squared residuals, which means finding the line that best fits the data by minimizing the vertical distance between the data points and the line \[\*\]\
-   broader concept than linear regression; estimates the coefficients (the intercept and slope) in a linear regression model... linear regression encompasses the entire process of modelling the relationship between variables \[\*\]

```{r OLSR}
### Generate example data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create a data frame from the data
data <- data.frame(x = x, y = y)


### Use existing data
# Fit the OLS regression model   

### linear relationship
model <- lm(y ~ x, data = data)  

### nonlinear relationships
model <- glm(y ~ x, 
             family = "", 
             data = "")

# Print model summary
summary(model)
```

### Linear Regression

-   predicting y based on x \[\*\]\
-   goal is to minimize the values of $\beta_0$ and $\beta_1$ that minimize the sum of squared residuals, which is the differences between the observed and predicted values \[\*\]

$$
y = \beta_0 ~+~ \beta_1x ~+~ \epsilon
$$

```{r}
### Generate example data where mean = 0 and sd = 1
set.seed(123)
x <- seq(1, 10, by = 0.1)
y <- 2 * x + 3 + rnorm(length(x), mean = 0, sd = 1)

# Create a data frame from the data
data <- data.frame(x = x, y = y)

### Use existing data
# Fit the linear regression model
model <- lm(y ~ x, data = data)

# Print model summary
summary(model)

# Make predictions using the model
new_data <- data.frame(x = c(11, 12, 13))
predictions <- predict(model, newdata = new_data)
```

### Logistic Regression

-   for binary/binomial outcome variable (0 or 1)\
-   predicts 0 or 1 for y based off of several x's

```{r}
### Generate example data
set.seed(123)
n <- 100
x <- rnorm(n)
y <- as.factor(ifelse(2 * x + rnorm(n) > 0, 1, 0))

# Create a data frame from the data
data <- data.frame(x = x, y = y)

### Use existing data
# Fit the logistic regression model
model <- glm(y ~ x, 
             data = data, 
             family = "binomial")

# Print model summary
summary(model)

# Make predictions using the model
new_data <- data.frame(x = c(0.5, 1.0, 1.5))

predicted_probs <- predict(model, 
                           newdata = new_data, 
                           type = "response") # p-values/probabilities

# Print predicted probabilities
cat("Predicted Probabilities:", predicted_probs, "\n")
```

### Stepwise Regression

-   a process where predictor variables are added or removed from a regression model based on statistical criteria \[\*\]\
-   can be sensitive to the order of predictor variables \[\*\]\
-   has some drawbacks, should look more into this...

```{r}
### Generate example data
set.seed(123)
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- 2 * x1 + 3 * x2 + rnorm(n)

# Create a data frame from the data
data <- data.frame(x1 = x1, x2 = x2, y = y)

### Use existing data 
# Fit the initial full model
full_model <- lm(y ~ ., data = data)

# Perform stepwise regression
stepwise_model <- step(full_model, 
                       direction = "both") # can add or remove predictor variables

# Print the summary of the stepwise model
summary(stepwise_model)
```

### Multivariate Adaptive Regression Splines (MARS)

-   a flexible and powerful technique for capturing complex relationships between variables \[\*\]\
-   can be useful for capturing nonlinear relationships in your data \[\*\]\
-   look more into intrepretation and validation...

```{r}
# Install and load the necessary package
install.packages("earth")
library(earth)

### Generate example data
set.seed(123)
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
y <- 2 * x1 + 3 * x2 + rnorm(n)

# Create a data frame from the data
data <- data.frame(x1 = x1, x2 = x2, y = y)

### Use existing data
# Fit the MARS model
mars_model <- earth(y ~ x1 + x2, 
                    data = data)

# Print the summary of the MARS model
summary(mars_model)

```

### Locally Estimated Scatterplot Smoothing (LOESS)

-   non-parametric tecnique used for fitting smooth curves to scatterplots \[\*\]\
-   the span parameter in the loess function controls the amount of smoothing; smaller span values result in more smoothing, larger span values result in less smoothing \[\*\]\
-   LOESS can be influenced by the span parameter and is more computationally intensive than some other regression techniques \[\*\]\
-   more ideal for smaller datasets and when assessing EDA and visualization

```{r}
### Generate example data
set.seed(123)
x <- seq(0, 2 * pi, length.out = 100)
y <- sin(x) + rnorm(100, mean = 0, sd = 0.2)

### Use existing data 
# Fit the LOESS model
loess_model <- loess(y ~ x) # default: span = 0.75

# Generate predicted values using the LOESS model
predicted_values <- predict(loess_model, 
                            data.frame(x = x))

# Plot the original data and the LOESS curve
plot(x, y, 
     main = "LOESS Smoothing")

lines(x, 
      predicted_values, 
      col = "red", 
      lwd = 2) 
```

## Instance-based Algorithms

-   instances like building up a database of example data and compare new data to the example database \[1\]
    -   uses a similarity measure in order to find the best match and make a prediction \[1\]\
    -   also called memory-based learning \[1\]\
    -   focuses on the representation of stored instances and similarity measures used between instances \[1\]

### k-Nearest Neighbor (kNN)

-   class labels are determined based on a linear relationship between the predictor variables (x1 and x2) \[\*\]\
-   using the 'class' package, you can use kNN regression using the 'knn.reg' function \[\*\]\
-   a simple method\
-   non-parametric\
-   can handle numerical and categorical data; can capture non-linear relationships \[\*\]\
-   can be effective for detecting outliers and anomalies; also sensitive to outliers and scaling \[\*\]\
-   not ideal for large datasets\
-   look more into finding the optimal k value...

**One method: rando data, not from a dataset \[\*\]**

```{r}
# Install and load the necessary package
install.packages("class")
library(class)

### Generate example data
set.seed(123)
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
class_labels <- factor(ifelse(2 * x1 + x2 + rnorm(n) > 0, "A", "B"))

# Create a data frame from the data
data <- data.frame(x1 = x1, x2 = x2, class = class_labels)

### Use existing data   
# Split data into training and test sets
train_indices <- sample(n, n * 0.7)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

# Fit the kNN classifier
k <- 3  # Set the number of neighbors
knn_model <- knn(train_data[, c("x1", "x2")], test_data[, c("x1", "x2")], train_data$class, k)

# Print the predicted class labels
cat("Predicted Class Labels:", knn_model, "\n")

# Calculate accuracy
accuracy <- sum(knn_model == test_data$class) / nrow(test_data)
cat("Accuracy:", accuracy, "\n")
```

**Another method: from a dataset \[\*\]**

```{r}
# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing

set.seed(123)  # for reproducibility

sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))

train_data <- data[sample_indices, ]

test_data <- data[-sample_indices, ]

# Define the number of neighbors (k) for KNN
k <- 3  # You can choose an appropriate value for your problem

# Train the KNN model
knn_model <- knn(train = 
                   train_data[, -target_column],  # Exclude target variable
                 test = 
                   test_data[, -target_column],    # Exclude target variable
                 cl = 
                   train_data$target_column,        # Target variable
                 k = k)

# Evaluate the model
# Here, you can use metrics like accuracy, precision, recall, etc., depending on your problem
# For simplicity, we'll use accuracy as an example

accuracy <- sum(knn_model == 
                  test_data$target_column) / 
                  length(test_data$target_column)
cat("Accuracy:", accuracy, "\n")
```

### Learning Vector Quantization (LVQ)

-   supervised ML algorithm used for classification\
-   purpose: classify input data into **predefined categories** or classes by adjusting a set of prototype vectors (reference points for each class)
    -   the algorithm learns to adjust them during training to make accurate predictions

```{r}
library(lvq)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing

set.seed(123)  # for reproducibility

sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))

train_data <- data[sample_indices, ]

test_data <- data[-sample_indices, ]

# Define the number of prototypes per class and other hyperparameters
num_prototypes <- 2  # Number of prototypes per class
learning_rate <- 0.1  # Learning rate
epochs <- 100  # Number of training epochs

# Extract the features (independent variables) and labels (target variable)
train_features <- train_data[, -target_column]  # Exclude target variable
train_labels <- train_data$target_column  # Target variable

# Train the LVQ model
lvq_model <- lvq.train(train_features, 
                       train_labels, 
                       num_prototypes = num_prototypes, 
                       learning.rate = learning_rate, 
                       epochs = epochs)

# Make predictions on the test data
test_features <- test_data[, -target_column]  # Exclude target variable
predictions <- lvq.predict(lvq_model, test_features)

# Evaluate the model
# You can use metrics like accuracy, precision, recall, etc., depending on your problem
# For simplicity, we'll use accuracy as an example
accuracy <- sum(predictions == test_data$target_column)/ 
                length(test_data$target_column)
cat("Accuracy:", accuracy, "\n")
```

### Self-Organizing Map (SOM)

-   unsupervised ML algorithm used for dimensionality reduction and visualization of high-dimensional data
    -   purpose: map complex, high-dimensional data onto a lower-dimensional grid in such a way that it preserves the topological properties and relationships of the original data\
    -   often used for clustering, visualization, and data exploration

```{r}
library(kohonen)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Normalize your data (if needed)
# SOMs are sensitive to data scaling, so it's often a good idea to normalize your data
# You can use functions like scale() or min-max normalization

# Create a SOM grid
# You need to specify the grid dimensions (e.g., grid rows and columns)
# and other parameters like the learning rate and neighborhood function type
grid_rows <- 5
grid_cols <- 5
som_grid <- somgrid(grid_rows, grid_cols, "hexagonal")

# Train the SOM
som_model <- som(data, grid = som_grid, rlen = 100, alpha = c(0.05, 0.01))

# Plot the SOM
# This step helps visualize the resulting SOM and cluster assignments
plot(som_model)

# You can also identify cluster assignments for your data points
cluster_assignments <- predict(som_model, newdata = data)

# Explore and analyze the results further based on your problem's objectives
```

### Locally Weighted Learning (LWL)

-   gives more weight to nearby data points and less weight to those farther away
    -   purpose: provide flexible and adaptive modeling, where the prediction for a new data point is based on the contributions of its neighboring data points

```{r}
library(locfit)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing

set.seed(123)  # for reproducibility

sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))

train_data <- data[sample_indices, ]

test_data <- data[-sample_indices, ]

# Define the bandwidth parameter for LWL
bandwidth <- 0.2  # You can choose an appropriate value for your problem

# Train the LWL model
lwl_model <- locfit::locfit(target_variable ~ predictors, 
                            data = train_data, 
                            alpha = bandwidth)

# Make predictions on the test data
predictions <- predict(lwl_model, 
                       newdata = data.frame(predictors = 
                                              test_data$predictors))

# Evaluate the model
# You can use appropriate evaluation metrics based on your problem
# For regression, you might use mean squared error, and for classification, you might use accuracy.
```

### Support Vector Machines (SVM)

-   supervised ML algorithm used for classification and regression
    -   purpose: to find a hyperplane (or decision boundary) that best separates different classes in a dataset
        -   goal is to maximize the margin between the data points of different classes while minimizing classification errors\
        -   useful for high-dimensional data; is effective in handling complex classification problems

```{r}
library(e1071)  # for SVM

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing

set.seed(123)  # for reproducibility

sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))

train_data <- data[sample_indices, ]

test_data <- data[-sample_indices, ]

# Define the SVM model
# Here, we'll use a linear kernel for simplicity
svm_model <- svm(target_column ~ ., 
                 data = train_data, 
                 kernel = "linear")

# Make predictions on the test data
predictions <- predict(svm_model, 
                       newdata = 
                         test_data[, -target_column])

# Evaluate the model
# Here, you can use metrics like accuracy, precision, recall, etc., depending on your problem
# For simplicity, we'll use accuracy as an example
accuracy <- sum(predictions == test_data$target_column) / length(test_data$target_column)
cat("Accuracy:", accuracy, "\n")
```

## Regularization Algorithms

-   an extension to another method (i.e., regression methods) \[1\]\
-   penalizes models based on complexity; favors simpler models that are more generalizable \[1\]

### Ridge Regression

-   used in linear regression to prevent overfitting and improve the models generalization by adding a "penalty term" to the regression equation
    -   purpose: to skrink the coefficients of the features towards zero while still maintaining all the geatures in the model
        -   helps reduce multicollinearity\
        -   leads to more stable and reliable predictions\
-   for datasets where features are highly correlated

```{r}
library(glmnet)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing

set.seed(123)  # for reproducibility

sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))

train_data <- data[sample_indices, ]

test_data <- data[-sample_indices, ]

# Separate the target variable and predictors
y_train <- train_data$target_column
X_train <- train_data[, -target_column]

# Fit a ridge regression model using glmnet
# You can specify the lambda (penalty) parameter to control the strength of regularization
# Smaller values of lambda result in stronger regularization
# cv.glmnet performs cross-validation to select an optimal lambda value
ridge_model <- cv.glmnet(x = as.matrix(X_train), 
                         y = y_train, 
                         alpha = 0, 
                         nfolds = 10)

# Print the optimal lambda value selected by cross-validation
best_lambda <- ridge_model$lambda.min
cat("Optimal Lambda:", best_lambda, "\n")

# Fit the final ridge regression model with the optimal lambda
final_ridge_model <- glmnet(x = as.matrix(X_train), 
                            y = y_train, 
                            alpha = 0, 
                            lambda = best_lambda)

# Make predictions on the test set
X_test <- test_data[, -target_column]
predictions <- predict(final_ridge_model, 
                       newx = as.matrix(X_test))

# Evaluate the model's performance
# You can use appropriate regression metrics like RMSE, R-squared, etc., depending on your problem
# For simplicity, we'll use Mean Squared Error (MSE) as an example
mse <- mean((predictions - test_data$target_column)^2)
cat("Mean Squared Error:", mse, "\n")
```

### Least Absolute Shrinkage and Selection Operator (LASSO)

-   used to prevent overfitting and perform feature selection by adding a penalty term to the linear regression cost function\
-   encourages sparse models by pushing some of the coefficient values to exactly 0, which effectively removes those features from the model
    -   it balances a trade-off between model complexity and goodness-of-fit

```{r}
library(glmnet)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into predictors (X) and target variable (y)
X <- data[, -target_column]  # Exclude the target variable
y <- data$target_column      # Target variable

# Standardize predictors (recommended for LASSO)
X <- scale(X)

# Set up the lambda values (penalty parameter)
# You can use cross-validation to determine the optimal lambda value
lambda_values <- 10^seq(10, -2, length = 100)

# Fit LASSO regression model
lasso_model <- glmnet(X, 
                      y, 
                      alpha = 1, 
                      lambda = lambda_values)

# Plot the LASSO coefficient paths (optional)
plot(lasso_model)

# Select the best lambda value using cross-validation (optional)
cv_model <- cv.glmnet(X, 
                      y, 
                      alpha = 1)
best_lambda <- cv_model$lambda.min
cat("Best Lambda:", best_lambda, "\n")

# Fit the LASSO model with the best lambda
lasso_model_best_lambda <- glmnet(X, 
                                  y, 
                                  alpha = 1, 
                                  lambda = best_lambda)

# Print the coefficients of the LASSO model
coef(lasso_model_best_lambda)
```

### Elastic Net

-   combines L1 (LASSO) and L2 (ridge regression) methods
    -   purpose: to overcome some of the limitations of each of those two methods by adding a regularization term that is a linear combination of L1 and L2 regularization penalties
-   there are two hyperparameters, alpha and lambda, that control the balance
    -   when alpha is 0, it's equal to ridge regression and when alpha is 1, it's equal to LASSO

```{r}
library(glmnet)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into predictors (X) and the target variable (Y)
X <- data[, -target_column]  # Exclude target variable
Y <- data$target_column

# Define the alpha and lambda values
alpha <- 0.5  # You can choose a value between 0 (Ridge) and 1 (Lasso)
lambda <- 0.01  # Regularization strength parameter

# Create an Elastic Net model
elastic_net_model <- glmnet(X, 
                            Y, 
                            alpha = alpha, 
                            lambda = lambda)

# Make predictions using the model
# You can replace 'new_data' with your test data if needed
predicted_values <- predict(elastic_net_model, 
                            new_data, 
                            s = lambda)

# Optionally, you can also plot the regularization path
plot(elastic_net_model)
```

### Least-Angle Regression (LARS)

-   used in linear regression\
-   purpose: to produce a sparse model by gradually adding predictors (X's) to the model while controlling the size of the coefficients
    -   ideal for high-dimensional data where you want to identify the most relevant features while avoiding overfitting

```{r}
library(lars)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Separate predictors (features) and the target variable
X <- data[, -target_column]  # Exclude the target variable
y <- data$target_column      # Target variable

# Fit the LARS model
lars_model <- lars(X, 
                   y, 
                   type = "lasso")  # LARS with L1 (Lasso) regularization

# Print the model summary
print(summary(lars_model))

# Get the selected features (variables)
selected_features <- names(X)[which(lars_model$active != 0)]
cat("Selected Features:", selected_features, "\n")
```

## Decision Tree Algorithms

-   typically for classification and regression problems \[1\]\
-   often fast, accurate and tres popular
-   construct a model of decisions made based on actual values in the data \[1\]\
-   create tree structures until a prediction decision is made for a given record \[1\]

### Classification and Regression Tree (CART)

-   used both classification and regression\
-   purpose: to create a tree-like model that can make predictions by recursively splitting the data into subsets based on the values of input features\
-   commonly used for predicting categories (classification) or predicting numeric values (regression) based on input features

```{r}
library(rpart)  # for CART algorithm

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing

set.seed(123)  # for reproducibility

sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))

train_data <- data[sample_indices, ]

test_data <- data[-sample_indices, ]

# Define the CART model
# For classification tasks, specify the target variable as a factor
# For regression tasks, specify the target variable as numeric
# Replace 'target_column' with the actual name of your target variable
cart_model <- rpart(target_column ~ ., data = train_data, method = "class")

# Make predictions using the CART model
# For classification tasks, replace 'new_data' with your test data and specify 'type = "class"'
# For regression tasks, replace 'new_data' with your test data and specify 'type = "vector"'
predictions <- predict(cart_model, newdata = test_data, type = "class")

# Evaluate the model
# Here, you can use metrics like accuracy (for classification) or RMSE (for regression), depending on your problem
# For simplicity, we'll use accuracy as an example for classification
accuracy <- sum(predictions == test_data$target_column) / length(test_data$target_column)
cat("Accuracy:", accuracy, "\n")
```

### Iterative Dichotomiser 3 (ID3)

-   used for building decision trees and data mining\
-   purpose: to classify or predict a target variable based on a set of input features
    -   recursively selects the best attribute to split the data into subsets that are as pure as possible in terms of the target variable\
-   an old algorithm, doesn't exist in R's core libraries\
-   can create an algorithm similar to ID3 using recursive functions in R

```{r}
library(dplyr)

# Define a recursive function to build the ID3 decision tree
id3 <- function(data, target_attr, attributes) {
  # Create a new node for the decision tree
  node <- list()
  
  # If all instances have the same class label, return that label as a leaf node
  if (length(unique(data[[target_attr]])) == 1) {
    node$leaf <- TRUE
    node$class <- unique(data[[target_attr]])[1]
    return(node)
  }
  
  # If there are no more attributes to split on, return the majority class as a leaf node
  if (length(attributes) == 0) {
    node$leaf <- TRUE
    node$class <- names(sort(table(data[[target_attr]]), decreasing = TRUE))[1]
    return(node)
  }
  
  # Select the best attribute to split on based on information gain or entropy
  best_attr <- select_best_attribute(data, target_attr, attributes)
  
  # Set the current node's attribute to the best attribute
  node$attribute <- best_attr
  
  # Create child nodes for each value of the best attribute
  node$children <- list()
  for (value in unique(data[[best_attr]])) {
    # Create a subset of the data where the best attribute has the specified value
    subset_data <- data %>%
      filter(data[[best_attr]] == value)
    
    # Recursively build the tree for the subset
    child <- id3(subset_data, target_attr, setdiff(attributes, best_attr))
    
    # Add the child node to the current node
    node$children[[as.character(value)]] <- child
  }
  
  return(node)
}

# Define a function to select the best attribute based on information gain or entropy
select_best_attribute <- function(data, target_attr, attributes) {
  # Implement your method for attribute selection here (e.g., using information gain or entropy)
  # This function should return the name of the best attribute to split on
  # You can replace this with your specific attribute selection logic
  # Example:
  # return(attributes[1])
}

# Example usage
# Replace 'your_dataset.csv' with the actual file path or URL to your dataset
# Replace 'target_attribute' with the name of your target attribute
data <- read.csv("your_dataset.csv")
target_attribute <- "target_attribute"

# Get a list of all attributes except the target attribute
all_attributes <- setdiff(names(data), target_attribute)

# Build the ID3 decision tree
decision_tree <- id3(data, target_attribute, all_attributes)

# Print the decision tree or use it for predictions
print(decision_tree)
```

### Iterative C4.5 and C5.0

-   iterative enhancements which are used in ML and data classification\
-   purpose: to improve the decision tree's accuracy and generalization by iteratively refining the tree through processes like pruning and re-splitting nodes

```{r}
library(C50)  # for C5.0 algorithm

# Load your dataset
data <- read.csv("your_dataset.csv")

# Split the dataset into training and testing sets
# You can use any method you prefer for data splitting
# Here's a simple random split into 80% training and 20% testing
set.seed(123)  # for reproducibility
sample_indices <- sample(nrow(data), size = 0.8 * nrow(data))
train_data <- data[sample_indices, ]
test_data <- data[-sample_indices, ]

# Train the C5.0 model
c50_model <- C5.0(train_data[, -target_column], train_data$target_column)

# Predict using the trained model
predictions <- predict(c50_model, test_data)

# Evaluate the model
# You can use various metrics like accuracy, precision, recall, etc., depending on your problem
# For simplicity, we'll use accuracy as an example
accuracy <- sum(predictions == test_data$target_column) / length(test_data$target_column)
cat("Accuracy:", accuracy, "\n")
```

### Chi-squared Automatic Interaction Detection (CHAID)

-   iterative decision tree for predictive modeling and classification\
-   purpose: create a decision tree that recursively splits the data into homogenous groups based on the most significant categorical predictor variables (X's)
    -   useful for analyzing categorical data\
    -   identifies relationships between categorical predictor variables and a categorical target variable by performing chi-squared tests for independence to find the most influential predictors

```{r}
library(partykit)

# Load your dataset
data <- read.csv("your_dataset.csv")

# Define your target variable and predictor variables
# Replace 'target_column' and 'predictor_columns' with the actual column names
target_column <- "target_variable"
predictor_columns <- c("predictor_1", "predictor_2", "predictor_3")

# Create a formula for the CHAID model
formula <- as.formula(paste(target_column, "~", paste(predictor_columns, collapse = "+")))

# Build the CHAID decision tree model
chaide_tree <- chaid(formula, data = data)

# Visualize the CHAID decision tree
plot(chaide_tree)

# Summary of the tree
print(chaide_tree)

# Predict using the CHAID decision tree model
# Replace 'new_data' with the data you want to make predictions on
new_data <- data.frame(predictor_1 = c(...), predictor_2 = c(...), predictor_3 = c(...))
predictions <- predict(chaide_tree, newdata = new_data)

# Evaluate the model and make further analyses as needed
```

### Decision Stump

### M5

### Conditional Decision Trees

## Bayesian Algorithms   

- conditional probability for two events (A|B)   

$$
P(A \mid B) = \frac{P(B \cap A)}{P(B)}
$$   

$$
{P(B \cap A)} = {P(B|A)}{P(A)}
$$   


- the formula above expresses that the probability that B occurs and causes A is equivalent to the probability of B and A occurring, divided by the probability of B occurring. 

-   explictly apply Bayes' Theorem for classification and regression probems \[1\]   
- 

### Naive Bayes

### Gaussian Naive Bayes

### Multinomial Naive Bayes

### Averaged One-Dependence Estimators (AODE)

### Bayesian Belief Network (BBN)

### Bayesian Network (BN)

## Clustering Algorithms

-   describes the class of problem and class of methods \[1\]\
-   organized by modeling approaches such as 'centroid-based' and 'hierarchal' \[1\]\
-   all methods use structures in the data to best organize the data into groups of maximum commonality \[1\]

### k-Means

### k-Medians

### Expectation Maximization (EM)

### Hierarchical Clustering

## Association Rule Learning Algorithms

-   extract rules that best explain observed relationshipns between variables in the dataset \[1\]\
-   can discover useful associations in multidimensional datasets \[1\]

### Apriori algorithm

### Eclat algorithm

## Artifical Neural Network Algorithms

-   inspired by the structure and/or function of biological neural networks \[1\]\
-   class of pattern matching used in regression and classification problems \[1\]\
-   subfield of hundreds of algorithms and variations for types of problems \[1\]

### Perceptron

### Multilayer Perceptrons (MLP)

### Back-Propagation

### Stochastic Gradient Descent

### Hopfield Network

### Radial Basis Function Network (RBFN)

## Deep Learning Algorithms

-   a modern update to 'Artificial Neural Networks' that exploit abundant cheap computation \[1\]\
-   much larger and more complex neural networks \[1\]\
-   use very large datasets of labelled analog data such as image, text, audio and video \[1\]

### Convolutional Neural Network (CNN)

### Recurrent Neural Networks (RNNs)

### Long Short-Term Memory Networks (LSTMs)

### Stacked Auto-Encoders

### Deep Boltzmann Machine (DBM)

### Deep Belief Networks (DBN)

## Dimensionality Reduction Algorithms

-   similar to clustering methods; this seeks and exploits the structure in the data - can be unsupervised or order to summarize or describe data using less information \[1\]\
-   can be used to visualize dimensional data or simplify data that can be used in a supervised learning method \[1\]\
-   can be used in classification and regression problems \[1\]

### Principal Component Analysis (PCA)

### Principal Component Regression (PCR)

### Partial Least Squares Regression (PLSR)

### Sammon Mapping

### Multidimensional Scaling (MDS)

### Projection Pursuit

### Linear Discriminant Analysis (LDA)

### Mixture Discriminant Analysis (MDA)

### Quadratic Discriminant Analysis (QDA)

### Flexible Discriminant Analysis (FDA)

## Ensemble Algorithms

-   models composed of several weaker models that are independently trained and whose predictions are combined to make an overall prediction \[1\]\
-   powerful and popular

### Boosting

### Bootstrapped Aggregation (Bagging)

### AdaBoost

### Weighted Average (Blending)

### Stacked Generalization (stacking)

### Gradient Boosting Machines (GBM)

### Gradient Boosted Regression Trees (GBRT)

### Random Forest

## Other Machine Learning Algorithms

### Feature selection algorithms

### Algorithm accuracy evaluation

### Performance measures

### Optimization algorithms

## Other Algorithms for Specialty Subfields

### Computational intelligence (evolutionary algorithms)

### Computer Vision (CV)

### Natural Language Processing (NLP)

### Recommender Systems

### Reinforcement Learning

### Graphical Models

# Sources

\[1\] https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/

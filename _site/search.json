[
  {
    "objectID": "posts/Sampling Analysis 23/index.html",
    "href": "posts/Sampling Analysis 23/index.html",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "",
    "text": "A lovely light-hearted survey and sampling analysis that felt like absolute hell but was an important learning curve."
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#re-coding-response-variable-injury",
    "href": "posts/Sampling Analysis 23/index.html#re-coding-response-variable-injury",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Re-Coding Response Variable: INJURY",
    "text": "Re-Coding Response Variable: INJURY"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#proportions-of-injuries-for-2009-and-2011",
    "href": "posts/Sampling Analysis 23/index.html#proportions-of-injuries-for-2009-and-2011",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Proportions of Injuries for 2009 and 2011",
    "text": "Proportions of Injuries for 2009 and 2011\n\n\n[1] 2099\n\n\n[1] 2607\n\n\n[1] 0.02373199\n\n\n[1] 0.02559018"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\n\n\n          mean     SE\nINJURY 0.02573 0.0026\n\n\n           2.5 %     97.5 %\nINJURY 0.0205571 0.03090286\n\n\n       variance     SE\nINJURY 0.025075 0.0025\n\n\n\n\n           mean     SE\nINJURY 0.026762 0.0029\n\n\n            2.5 %    97.5 %\nINJURY 0.02113361 0.0323912\n\n\n       variance     SE\nINJURY 0.026055 0.0027\n\n\n\nChi-Squared Test for Association\n\n\n      YEAR\nINJURY      2009      2011\n     0 45694.162 46572.321\n     1  1206.760  1280.661\n\n\n      YEAR\nINJURY  2009  2011\n     0 45694 46572\n     1  1207  1281\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~INJURY + YEAR, design = srs.des.merge, statistic = \"Chisq\")\nX-squared = 0.067992, df = 1, p-value = 0.7873"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#post-stratify-quarter-on-srs",
    "href": "posts/Sampling Analysis 23/index.html#post-stratify-quarter-on-srs",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Post-Stratify QUARTER on SRS",
    "text": "Post-Stratify QUARTER on SRS\n\n\n            2.5 %     97.5 %\nINJURY 0.02049483 0.03082097\n\n\n       variance     SE\nINJURY 0.025007 0.0025\n\n\n\n\n          mean     SE\nINJURY 0.02655 0.0029\n\n\n            2.5 %     97.5 %\nINJURY 0.02096167 0.03213899\n\n\n       variance     SE\nINJURY 0.025854 0.0027\n\n\n\nChi-Squared Test for Association\n\n\n      YEAR\nINJURY      2009      2011\n     0 86176.661 99170.185\n     1  2269.339  2704.815\n\n\n      YEAR\nINJURY  2009  2011\n     0 86177 99170\n     1  2269  2705\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~INJURY + YEAR, design = post_strat.des.merge, statistic = \"Chisq\")\nX-squared = 0.050773, df = 1, p-value = 0.8146"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#post-stratify-region-on-srs",
    "href": "posts/Sampling Analysis 23/index.html#post-stratify-region-on-srs",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Post-Stratify REGION on SRS",
    "text": "Post-Stratify REGION on SRS\n\n\n           mean     SE\nINJURY 0.025812 0.0027\n\n\n            2.5 %     97.5 %\nINJURY 0.02061612 0.03100836\n\n\n       variance     SE\nINJURY 0.025153 0.0025\n\n\n\n\n           mean     SE\nINJURY 0.026787 0.0029\n\n\n            2.5 %     97.5 %\nINJURY 0.02114993 0.03242353\n\n\n       variance     SE\nINJURY 0.026078 0.0027\n\n\n\nChi-Squared Test for Association\n\n\n      YEAR\nINJURY      2009      2011\n     0 86163.011 99146.102\n     1  2282.989  2728.898\n\n\n      YEAR\nINJURY  2009  2011\n     0 86163 99146\n     1  2283  2729\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~INJURY + YEAR, design = post_strat.des.merge, statistic = \"Chisq\")\nX-squared = 0.060097, df = 1, p-value = 0.7995"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#stratified-sampling-by-region",
    "href": "posts/Sampling Analysis 23/index.html#stratified-sampling-by-region",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Stratified Sampling by Region",
    "text": "Stratified Sampling by Region\n\nProportional Allocation\n\n\n\n    1     2     3     4 \n14458 18093 32308 23587 \n\n\n\n   1    2    3    4 \n 565  708 1264  922 \n\n\n[1] 3459\n\n\n           mean     SE\nINJURY 0.023418 0.0025\n\n\n            2.5 %     97.5 %\nINJURY 0.01847843 0.02835826\n\n\n       variance     SE\nINJURY 0.022877 0.0024\n\n\n\n\n\n    1     2     3     4 \n15835 20988 36672 28380 \n\n\n\n   1    2    3    4 \n 476  631 1103  854 \n\n\n[1] 3064\n\n\n           mean     SE\nINJURY 0.026437 0.0029\n\n\n           2.5 %     97.5 %\nINJURY 0.0208506 0.03202405\n\n\n       variance     SE\nINJURY 0.025747 0.0027\n\n\n\nChi-Squared Test of Association\n\n\n      YEAR\nINJURY      2009      2011\n     0 86374.741 99181.698\n     1  2071.259  2693.302\n\n\n      YEAR\nINJURY  2009  2011\n     0 86375 99182\n     1  2071  2693\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~INJURY + YEAR, design = prop.alloc.merge.des, statistic = \"Chisq\")\nX-squared = 0.60592, df = 1, p-value = 0.4175\n\n\n\n\n\nOptimal Allocation\n\n\n  REGION         x num_total    denom      pi_h\n1      1 0.1506692     14458 13451.54 0.1619425\n2      2 0.1621666     18093 13451.54 0.2181224\n3      3 0.1524347     32308 13451.54 0.3661187\n4      4 0.1447501     23587 13451.54 0.2538164\n\n\n\n   1    2    3    4 \n 561  755 1267  878 \n\n\n\n    1     2     3     4 \n14458 18093 32308 23587 \n\n\n           mean     SE\nINJURY 0.024233 0.0026\n\n\n            2.5 %   97.5 %\nINJURY 0.01921315 0.029253\n\n\n       variance     SE\nINJURY 0.023653 0.0024\n\n\n\n\n  REGION         x num_total    denom      pi_h\n1      1 0.1559617     15835 16084.89 0.1535387\n2      2 0.1629639     20988 16084.89 0.2126396\n3      3 0.1568058     36672 16084.89 0.3575020\n4      4 0.1566093     28380 16084.89 0.2763197\n\n\n\n   1    2    3    4 \n 459  641 1072  894 \n\n\n\n    1     2     3     4 \n15835 20988 36672 28380 \n\n\n           mean     SE\nINJURY 0.025067 0.0028\n\n\n            2.5 %     97.5 %\nINJURY 0.01961731 0.03051754\n\n\n       variance     SE\nINJURY 0.024447 0.0026\n\n\n\nChi-Squared Test of Association\n\n\n      YEAR\nINJURY      2009      2011\n     0 86302.682 99321.256\n     1  2143.318  2553.744\n\n\n      YEAR\nINJURY  2009  2011\n     0 86303 99321\n     1  2143  2554\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~INJURY + YEAR, design = opt.alloc.merge.des, statistic = \"Chisq\")\nX-squared = 0.046957, df = 1, p-value = 0.822"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#stratified-sampling-by-quarter",
    "href": "posts/Sampling Analysis 23/index.html#stratified-sampling-by-quarter",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Stratified Sampling by Quarter",
    "text": "Stratified Sampling by Quarter\n\nProportional Allocation\n\n\n\n    1     2     3     4 \n 9443 23093 22375 33535 \n\n\n\n   1    2    3    4 \n 369  903  875 1312 \n\n\n[1] 3459\n\n\n           mean     SE\nINJURY 0.024616 0.0026\n\n\n            2.5 %     97.5 %\nINJURY 0.01954614 0.02968686\n\n\n       variance     SE\nINJURY 0.024017 0.0025\n\n\n\n\n\n    1     2     3     4 \n25032 26944 25325 24574 \n\n\n\n  1   2   3   4 \n753 811 762 739 \n\n\n[1] 3065\n\n\n           mean     SE\nINJURY 0.027158 0.0029\n\n\n            2.5 %     97.5 %\nINJURY 0.02147682 0.03283926\n\n\n       variance     SE\nINJURY 0.026429 0.0027\n\n\n\nChi-Squared Test of Association\n\n\n      YEAR\nINJURY      2009      2011\n     0 86354.346 99310.055\n     1  2179.391  2772.358\n\n\n      YEAR\nINJURY  2009  2011\n     0 86354 99310\n     1  2179  2772\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  svychisq(~INJURY + YEAR, design = prop.alloc.merge.des, statistic = \"Chisq\")\nX-squared = 0.41427, df = 1, p-value = 0.5037"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---quarter",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---quarter",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Quarter",
    "text": "Simple Random Sample - Quarter\nAssessing proportion of injuries per year by quarter\n\n\n   QUARTER\n1 2.904529\n\n\n$total\n       as.numeric(QUARTER)\nINJURY          0.02540821\n\n$se\n       as.numeric(QUARTER)\nINJURY         0.002608762\n\n\n[1] 0.02540821 0.02029503 0.03052138\n\n\n  QUARTER\n1 2.48531\n\n\n$total\n       as.numeric(QUARTER)\nINJURY          0.02640864\n\n$se\n       as.numeric(QUARTER)\nINJURY         0.002838812\n\n\n[1] 0.02640864 0.02084457 0.03197271"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---age",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---age",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Age",
    "text": "Simple Random Sample - Age\nAssessing proportion of injuries per year by age\n\n\n$total\n              AGE\nINJURY 0.02571345\n\n$se\n               AGE\nINJURY 0.002650076\n\n\n[1] 0.02571345 0.02051930 0.03090760\n\n\n$total\n              AGE\nINJURY 0.02663461\n\n$se\n               AGE\nINJURY 0.002869556\n\n\n[1] 0.02663461 0.02101028 0.03225894"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---sex",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---sex",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Sex",
    "text": "Simple Random Sample - Sex\nAssessing proportion of injuries per year by sex\n\n\n$total\n              SEX\nINJURY 0.02569184\n\n$se\n              SEX\nINJURY 0.00264173\n\n\n[1] 0.02569184 0.02051404 0.03086963\n\n\n$total\n              SEX\nINJURY 0.02650775\n\n$se\n               SEX\nINJURY 0.002852794\n\n\n[1] 0.02650775 0.02091628 0.03209923"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---race",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---race",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Race",
    "text": "Simple Random Sample - Race\nAssessing proportion of injuries per year by race\n\n\n$total\n          RACENEW\nINJURY 0.02582102\n\n$se\n           RACENEW\nINJURY 0.002667604\n\n\n[1] 0.02582102 0.02059251 0.03104952\n\n\n$total\n          RACENEW\nINJURY 0.02666452\n\n$se\n          RACENEW\nINJURY 0.00287207\n\n\n[1] 0.02666452 0.02103526 0.03229378"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---abovebelow-poverty-line",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---abovebelow-poverty-line",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Above/Below Poverty Line",
    "text": "Simple Random Sample - Above/Below Poverty Line\nAssessing proportion of injuries per year by status of being above or below the poverty line\n\n\n$total\n           POORYN\nINJURY 0.02580386\n\n$se\n           POORYN\nINJURY 0.00270671\n\n\n[1] 0.02580386 0.02049871 0.03110901\n\n\n$total\n           POORYN\nINJURY 0.02663204\n\n$se\n            POORYN\nINJURY 0.002913605\n\n\n[1] 0.02663204 0.02092137 0.03234270"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---age-sex-race",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---age-sex-race",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Age + Sex + Race",
    "text": "Simple Random Sample - Age + Sex + Race\nAssessing proportion of injuries per year by age, sex and race\n\n\n$total\n         AGE       SEX    RACENEW\n1 0.02571345 0.6044988 0.02582102\n\n$se\n          AGE        SEX     RACENEW\n1 0.002650076 0.06215682 0.002667604\n\n\n$AGE\n[1] 0.02571345\n\n$SEX\n[1] 0.6044988\n\n$RACENEW\n[1] 0.02582102\n\n$AGE\n[1] 0.0205193\n\n$SEX\n[1] 0.4826714\n\n$RACENEW\n[1] 0.02059251\n\n$AGE\n[1] 0.0309076\n\n$SEX\n[1] 0.7263261\n\n$RACENEW\n[1] 0.03104952\n\n\n$total\n         AGE       SEX    RACENEW\n1 0.02627255 0.6230714 0.02633119\n\n$se\n          AGE        SEX     RACENEW\n1 0.002830548 0.06705564 0.002836167\n\n\n$AGE\n[1] 0.02627255\n\n$SEX\n[1] 0.6230714\n\n$RACENEW\n[1] 0.02633119\n\n$AGE\n[1] 0.02072467\n\n$SEX\n[1] 0.4916424\n\n$RACENEW\n[1] 0.0207723\n\n$AGE\n[1] 0.03182042\n\n$SEX\n[1] 0.7545005\n\n$RACENEW\n[1] 0.03189008"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---age",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---age",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Age",
    "text": "Main Sampling Design - Age\nPost-stratify by age\n\n\n$total\n              AGE\nINJURY 0.02741572\n\n$se\n               AGE\nINJURY 0.002892153\n\n\n[1] 0.02741572 0.02174710 0.03308434\n\n\n                AGE\nINJURY 6.580796e-09\n\n\n$total\n              AGE\nINJURY 0.02222087\n\n$se\n              AGE\nINJURY 0.00267332\n\n\n[1] 0.02222087 0.01698117 0.02746058\n\n\n                AGE\nINJURY 5.470782e-09\n\n\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x1 * n1, x2 * n2) out of c(n1, n2)\nX-squared = 1.6931, df = 1, p-value = 0.1932\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.002425969  0.012815669\nsample estimates:\n    prop 1     prop 2 \n0.02741572 0.02222087 \n\n\n[1] 1.301192"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---sex",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---sex",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Sex",
    "text": "Main Sampling Design - Sex\nPost-stratify by sex\n\n\n$total\n              SEX\nINJURY 0.02610033\n\n$se\n               SEX\nINJURY 0.002737903\n\n\n[1] 0.02610033 0.02073404 0.03146662\n\n\n                SEX\nINJURY 3.264921e-06\n\n\n$total\n              SEX\nINJURY 0.02612651\n\n$se\n               SEX\nINJURY 0.002740649\n\n\n[1] 0.02612651 0.02075483 0.03149818\n\n\n                SEX\nINJURY 3.264921e-06"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---race",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---race",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Race",
    "text": "Main Sampling Design - Race\nPost-stratify by race\n\n\n$total\n          RACENEW\nINJURY 0.02532012\n\n$se\n           RACENEW\nINJURY 0.002692587\n\n\n[1] 0.02532012 0.02004265 0.03059759\n\n\n           RACENEW\nINJURY 3.28981e-10\n\n\n$total\n          RACENEW\nINJURY 0.02564065\n\n$se\n           RACENEW\nINJURY 0.002726672\n\n\n[1] 0.02564065 0.02029637 0.03098493\n\n\n           RACENEW\nINJURY 3.28981e-10"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---abovebelow-poverty-line",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---abovebelow-poverty-line",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Above/Below Poverty Line",
    "text": "Main Sampling Design - Above/Below Poverty Line\n_Post-stratify by poverty status of being above or below the poverty line__\n\n\n$total\n           POORYN\nINJURY 0.02568653\n\n$se\n           POORYN\nINJURY 0.00287087\n\n\n[1] 0.02568653 0.02005962 0.03131343\n\n\n             POORYN\nINJURY 1.819397e-06\n\n\n$total\n           POORYN\nINJURY 0.02271839\n\n$se\n            POORYN\nINJURY 0.002917812\n\n\n[1] 0.02271839 0.01699948 0.02843730\n\n\n             POORYN\nINJURY 1.916214e-06\n\n\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x1 * n1, x2 * n2) out of c(n1, n2)\nX-squared = 0.030396, df = 1, p-value = 0.8616\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.008809279  0.006884459\nsample estimates:\n    prop 1     prop 2 \n0.02568653 0.02664894 \n\n\n[1] 0.1743445"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---age-sex-race",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---age-sex-race",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Age + Sex + Race",
    "text": "Main Sampling Design - Age + Sex + Race\nPost-stratify by age, sex and race\n\n\n$total\n         AGE       SEX    RACENEW\n1 0.02571345 0.6044988 0.02582102\n\n$se\n          AGE        SEX     RACENEW\n1 0.002650076 0.06215682 0.002667604\n\n\n$AGE\n[1] 0.02571345\n\n$SEX\n[1] 0.6044988\n\n$RACENEW\n[1] 0.02582102\n\n$AGE\n[1] 0.0205193\n\n$SEX\n[1] 0.4826714\n\n$RACENEW\n[1] 0.02059251\n\n$AGE\n[1] 0.0309076\n\n$SEX\n[1] 0.7263261\n\n$RACENEW\n[1] 0.03104952\n\n\n                AGE         SEX      RACENEW\nINJURY 5.525261e-09 3.03958e-06 3.229045e-10\n\n\n$total\n         AGE       SEX    RACENEW\n1 0.02627255 0.6230714 0.02633119\n\n$se\n          AGE        SEX     RACENEW\n1 0.002830548 0.06705564 0.002836167\n\n\n$AGE\n[1] 0.02627255\n\n$SEX\n[1] 0.6230714\n\n$RACENEW\n[1] 0.02633119\n\n$AGE\n[1] 0.02072467\n\n$SEX\n[1] 0.4916424\n\n$RACENEW\n[1] 0.0207723\n\n$AGE\n[1] 0.03182042\n\n$SEX\n[1] 0.7545005\n\n$RACENEW\n[1] 0.03189008\n\n\n                AGE          SEX      RACENEW\nINJURY 6.303432e-09 3.537583e-06 3.650019e-10"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---quarter-1",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---quarter-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Quarter",
    "text": "Simple Random Sample - Quarter\nModel based population estimates by quarter\n\n\n\nCall:\nsvyglm(formula = INJURY ~ as.numeric(QUARTER), design = srs.des.2009, \n    family = \"binomial\")\n\nSurvey design:\nsvydesign(id = ~1, data = srs.2009, fpc = ~rep(N.2009, n.2009))\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -3.84276    0.35445 -10.842   &lt;2e-16 ***\nas.numeric(QUARTER)  0.07015    0.11244   0.624    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1.000662)\n\nNumber of Fisher Scoring iterations: 6\n\n\n    link    SE\n1 -3.639 0.106\n\n\nas.numeric(QUARTER) \n           1.072666 \n\n\n    2.5 %    97.5 % \n0.8604348 1.3372463 \n\n\n\nCall:\nsvyglm(formula = INJURY ~ as.numeric(QUARTER), design = srs.des.2011, \n    family = \"binomial\")\n\nSurvey design:\nsvydesign(id = ~1, data = srs.2011, fpc = ~rep(N.2011, n.2011))\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -3.76764    0.26491 -14.222   &lt;2e-16 ***\nas.numeric(QUARTER)  0.06805    0.09283   0.733    0.464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 0.9998915)\n\nNumber of Fisher Scoring iterations: 6\n\n\n     link     SE\n1 -3.5985 0.1107\n\n\nas.numeric(QUARTER) \n           1.070422 \n\n\n    2.5 %    97.5 % \n0.8922825 1.2841250"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---age-1",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---age-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Age",
    "text": "Simple Random Sample - Age\nModel based population estimates by age\n\n\n\nCall:\nsvyglm(formula = INJURY ~ AGE, design = srs.des.2009, family = \"binomial\")\n\nSurvey design:\nsvydesign(id = ~1, data = srs.2009, fpc = ~rep(N.2009, n.2009))\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.676261   0.205173 -17.918   &lt;2e-16 ***\nAGE          0.001175   0.004860   0.242    0.809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1.000313)\n\nNumber of Fisher Scoring iterations: 6\n\n\n     link     SE\n1 -3.6344 0.1053\n\n\n     AGE \n1.001175 \n\n\n    2.5 %    97.5 % \n0.9916805 1.0107609 \n\n\n\nCall:\nsvyglm(formula = INJURY ~ AGE, design = srs.des.2011, family = \"binomial\")\n\nSurvey design:\nsvydesign(id = ~1, data = srs.2011, fpc = ~rep(N.2011, n.2011))\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.722900   0.214296 -17.373   &lt;2e-16 ***\nAGE          0.003479   0.004835   0.719    0.472    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1.000282)\n\nNumber of Fisher Scoring iterations: 6\n\n\n     link     SE\n1 -3.5972 0.1106\n\n\n     AGE \n1.003485 \n\n\n    2.5 %    97.5 % \n0.9940159 1.0130444"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---sex-1",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---sex-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Sex",
    "text": "Simple Random Sample - Sex\nModel based population estimates by sex\n\n\n   link     SE\n1 -3.64 0.1059\n\n\n      SEX \n0.7916548 \n\n\n    2.5 %    97.5 % \n0.5232267 1.1977930 \n\n\n     link     SE\n1 -3.6017 0.1112\n\n\n     SEX \n0.718863 \n\n\n    2.5 %    97.5 % \n0.4655596 1.1099848"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---race-1",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---race-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Race",
    "text": "Simple Random Sample - Race\nModel based population estimates by race\n\n\n     link     SE\n1 -3.6368 0.1056\n\n\n  RACENEW \n0.9992693 \n\n\n    2.5 %    97.5 % \n0.9970198 1.0015239 \n\n\nIndependent Sampling design\nsvydesign(id = ~1, data = srs.2011, fpc = ~rep(N.2011, n.2011))\n\nCall:  svyglm(formula = INJURY ~ RACENEW, design = srs.des.2011, family = \"binomial\")\n\nCoefficients:\n(Intercept)      RACENEW  \n  -3.797418     0.001288  \n\nDegrees of Freedom: 3063 Total (i.e. Null);  3062 Residual\nNull Deviance:      755.6 \nResidual Deviance: 753.8    AIC: 757.8\n\n\n     link     SE\n1 -3.6038 0.1113\n\n\n RACENEW \n1.001289 \n\n\n    2.5 %    97.5 % \n0.9994387 1.0031420"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#simple-random-sample---abovebelow-poverty",
    "href": "posts/Sampling Analysis 23/index.html#simple-random-sample---abovebelow-poverty",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Simple Random Sample - Above/Below Poverty",
    "text": "Simple Random Sample - Above/Below Poverty\nModel based population estimates by status of being above or below poverty line\n\n\n     link     SE\n1 -3.6408 0.1061\n\n\n   POORYN \n0.9539852 \n\n\n    2.5 %    97.5 % \n0.8696225 1.0465321 \n\n\nIndependent Sampling design\nsvydesign(id = ~1, data = srs.2011, fpc = ~rep(N.2011, n.2011))\n\nCall:  svyglm(formula = INJURY ~ POORYN, design = srs.des.2011, family = \"binomial\")\n\nCoefficients:\n(Intercept)       POORYN  \n  -3.606859     0.006189  \n\nDegrees of Freedom: 3063 Total (i.e. Null);  3062 Residual\nNull Deviance:      755.6 \nResidual Deviance: 755.6    AIC: 759.6\n\n\n     link     SE\n1 -3.5938 0.1103\n\n\n  POORYN \n1.006208 \n\n\n    2.5 %    97.5 % \n0.9272497 1.0918892"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---age-1",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---age-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Age",
    "text": "Main Sampling Design - Age\nModel based population estimates on a post-stratified design, by age\n\n\n\nCall:\nsvyglm(formula = INJURY ~ AGE, design = post_strat_des_2009)\n\nSurvey design:\nsvydesign(ids = ~NHISHID, fpc = ~Mi, strata = ~REGION, weights = ~post_strat_weight + \n    w2, data = post_merge_2009, nest = T)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0198644  0.0046520   4.270 2.09e-05 ***\nAGE         0.0001814  0.0001300   1.395    0.163    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.02531078)\n\nNumber of Fisher Scoring iterations: 2\n\n\n       2.5 %     97.5 %\n1 0.02087369 0.03178631\n\n\n       2.5 %     97.5 %\n1 0.01676609 0.02705727\n\n\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x1 * n1, x2 * n2) out of c(n1, n2)\nX-squared = 0.0013179, df = 1, p-value = 0.971\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.008329021  0.007465021\nsample estimates:\n  prop 1   prop 2 \n0.026330 0.026762 \n\n\n[1] 0.03630289\n\n\nStratified 1 - level Cluster Sampling design\nWith (2611) clusters.\npostStratify(design = mult.post.des.merge, strata = ~strata_var, \n    population = table)\n\nCall:  svyglm(formula = INJURY ~ as.factor(YEAR), design = mult.post.des.merge)\n\nCoefficients:\n        (Intercept)  as.factor(YEAR)2011  \n           0.025991            -0.004199  \n\nDegrees of Freedom: 6905 Total (i.e. Null);  2602 Residual\nNull Deviance:      160.1 \nResidual Deviance: 160  AIC: -6296\n\n\nStratified 1 - level Cluster Sampling design\nWith (2611) clusters.\npostStratify(design = mult.post.des.merge, strata = ~strata_var, \n    population = table)\n\nCall:  svyglm(formula = INJURY ~ as.factor(YEAR) + AGE, design = mult.post.des.merge)\n\nCoefficients:\n        (Intercept)  as.factor(YEAR)2011                  AGE  \n          0.0199882           -0.0044897            0.0001776  \n\nDegrees of Freedom: 6905 Total (i.e. Null);  2601 Residual\nNull Deviance:      160.1 \nResidual Deviance: 159.9    AIC: -6299\n\n\n\nCall:\nsvyglm(formula = INJURY ~ as.factor(YEAR) + AGE, design = mult.post.des.merge)\n\nSurvey design:\npostStratify(design = mult.post.des.merge, strata = ~strata_var, \n    population = table)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          1.999e-02  3.628e-03   5.510 3.94e-08 ***\nas.factor(YEAR)2011 -4.490e-03  3.768e-03  -1.192    0.234    \nAGE                  1.776e-04  8.372e-05   2.121    0.034 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.02316281)\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---sex-1",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---sex-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Sex",
    "text": "Main Sampling Design - Sex\nModel based population estimates on a post-stratified design, by sex\n\n\n      link     SE\n1 0.025964 0.0027\n\n\n       2.5 %     97.5 %\n1 0.02063257 0.03129533\n\n\n      link     SE\n1 0.021782 0.0026\n\n\n       2.5 %     97.5 %\n1 0.01667449 0.02688884"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---race-1",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---race-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Race",
    "text": "Main Sampling Design - Race\nModel based population estimates on a post-stratified design, by race\n\n\n      link     SE\n1 0.026059 0.0027\n\n\n       2.5 %     97.5 %\n1 0.02072449 0.03139273\n\n\n      link     SE\n1 0.021794 0.0026\n\n\n       2.5 %     97.5 %\n1 0.01668064 0.02690746"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#main-sampling-design---abovebelow-poverty",
    "href": "posts/Sampling Analysis 23/index.html#main-sampling-design---abovebelow-poverty",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "Main Sampling Design - Above/Below Poverty",
    "text": "Main Sampling Design - Above/Below Poverty\nModel based population estimates on a post-stratified design, by the status of being above or below the poverty line\n\n\n\nCall:\nsvyglm(formula = INJURY ~ POORYN, design = post_strat_des_2009)\n\nSurvey design:\nsvydesign(ids = ~NHISHID, fpc = ~Mi, strata = ~REGION, weights = ~post_strat_weight + \n    w2, data = post_merge_2009, nest = T)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.0267671  0.0035390   7.563 7.13e-14 ***\nPOORYN      -0.0003585  0.0009269  -0.387    0.699    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.02532512)\n\nNumber of Fisher Scoring iterations: 2\n\n\n      link     SE\n1 0.026004 0.0027\n\n\n      link     SE\n1 0.021613 0.0026\n\n\n[1] 6904"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#section",
    "href": "posts/Sampling Analysis 23/index.html#section",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "2009",
    "text": "2009"
  },
  {
    "objectID": "posts/Sampling Analysis 23/index.html#section-1",
    "href": "posts/Sampling Analysis 23/index.html#section-1",
    "title": "Survey and Sampling Analysis Project Spring 2023",
    "section": "2011",
    "text": "2011"
  },
  {
    "objectID": "posts/ML Template/index.html",
    "href": "posts/ML Template/index.html",
    "title": "Machine Learning Template",
    "section": "",
    "text": "Inspired by ‘Machine Learning Walk-Through’; it reignited my undying love for template creation.\nThe brains behind this template goes to Jason Brownlee."
  },
  {
    "objectID": "posts/ML Template/index.html#summary-of-the-training-dataset",
    "href": "posts/ML Template/index.html#summary-of-the-training-dataset",
    "title": "Machine Learning Template",
    "section": "Summary of (the training) dataset",
    "text": "Summary of (the training) dataset\n\ndim(dataset) # make sure columns were retained \n\nhead(dataset)\n\nsummary(dataset)\n\nglimpse(dataset)"
  },
  {
    "objectID": "posts/ML Template/index.html#types-of-columnsvariables",
    "href": "posts/ML Template/index.html#types-of-columnsvariables",
    "title": "Machine Learning Template",
    "section": "Types of columns/variables",
    "text": "Types of columns/variables\n\nsapply(dataset, class)"
  },
  {
    "objectID": "posts/ML Template/index.html#transform-variable-types-as-needed",
    "href": "posts/ML Template/index.html#transform-variable-types-as-needed",
    "title": "Machine Learning Template",
    "section": "Transform variable types as needed",
    "text": "Transform variable types as needed\n\nAs used in the ‘iris’ dataset, it is simpler to have explanatory variables be numeric and the outcome/predictor variable be a factor."
  },
  {
    "objectID": "posts/ML Template/index.html#assess-factored-levels-in-outcome-variable",
    "href": "posts/ML Template/index.html#assess-factored-levels-in-outcome-variable",
    "title": "Machine Learning Template",
    "section": "Assess factored levels in outcome variable",
    "text": "Assess factored levels in outcome variable\nUsing Base R\n\n# using base\nlevels(dataset$OUTCOMEVARIABLE)\n# 3+ categories in a column : multinomial \n# 2 categories in a column: binomial/binary\n\nUsing Forcats"
  },
  {
    "objectID": "posts/ML Template/index.html#univariate",
    "href": "posts/ML Template/index.html#univariate",
    "title": "Machine Learning Template",
    "section": "Univariate",
    "text": "Univariate\n\nthis will show an individual plot for each explanatory variable by the outcome variable in side-by-side format\n\n\nx &lt;- dataset[, 1:4] # example 1:4 cols 1:4 are explanatory/numeric variables\ny &lt;- dataset[,5] # example col 5 is the predictor/outcome/factored variable\n\n\n# 2x2 layout for boxplots\npar(mfrow = c(1,4)) # 1 row, 4 columns\n\n# loop through each column of 'x' to create a boxplot\nfor(i in 1:4){\n  boxplot(x[ ,i], main = names(RAWDATASET)[i])} # shows all combined species by the 4 explanatory variables"
  },
  {
    "objectID": "posts/ML Template/index.html#multivariate",
    "href": "posts/ML Template/index.html#multivariate",
    "title": "Machine Learning Template",
    "section": "Multivariate",
    "text": "Multivariate\n\n# visually assessing interactions between the variables, colored by factor levels\n\n caret::featurePlot(\n  x = x, \n  y = y, \n  plot = \"ellipse\"\n)\n\n# \"pairs\" without \"ellipse\" is similar\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"pairs\")\n\n### an alternative version using GGally\n\nGGally::ggpairs(dataset, \n                columns = 1:4, # select explanatory variables\n        ggplot2::aes(color = y)) # color by factor levels of outcome\n\n\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"box\")\n\n\n# density plot for each level in the outcome variable \n\nscales &lt;- list(\n  x = list(\n    relation = \"free\"), \n  y = list(\n    relation = \"free\"\n))\n\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"density\", \n                   scales = scales)"
  },
  {
    "objectID": "posts/ML Template/index.html#test-harness-10-fold-cross-validation",
    "href": "posts/ML Template/index.html#test-harness-10-fold-cross-validation",
    "title": "Machine Learning Template",
    "section": "Test harness: 10-fold cross validation",
    "text": "Test harness: 10-fold cross validation\nThis method estimates accuracy\n* Splits data into 10 parts\n* Trains 9 parts, tests 1 part\n* Releases all combinations of train-test splits\n* Repeat 3 times for each algorithm for more accurate estimate\n\n# run algorithms using 10-fold cross validation   \n\ncontrol &lt;- caret::trainControl(method = \"cv\", \n                               number = 10)  \n\nmetric &lt;- \"Accuracy\"\n\nFormula for accuracy\n\\[\n\\text{accuracy} = \\frac{\\text{correctly predicted instances}}{\\text{total number of instances}} ~\\cdot~ 100\n\\]"
  },
  {
    "objectID": "posts/ML Template/index.html#build-5-different-models-for-prediction",
    "href": "posts/ML Template/index.html#build-5-different-models-for-prediction",
    "title": "Machine Learning Template",
    "section": "Build 5 different models for prediction",
    "text": "Build 5 different models for prediction\nEvaluate 5 different algorithms\nsimple linear method\n1. Linear Discriminant Analysis (LDA)\nnonlinear method\n2. Classification and Regression Trees (CART)\n3. k-Nearest Neighbors (kNN)\ncomplex nonlinear method\n4. Support Vector Machines (SVM) with a linear kernel\n5. Random Forest (RF)\n\n### set seed before each run \n\n# simple linear\n### LDA  \nset.seed(7)\n\nfit.lda &lt;- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"lda\", \n                        metric = metric, \n                        trControl = control)\n\n\n# nonlinear\n### CART  \nset.seed(7) \n\nfit.cart &lt;- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"rpart\", \n                        metric = metric, \n                        trControl = control)\n\n### kNN  \nset.seed(7)   \n\nfit.knn &lt;- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"knn\", \n                        metric = metric, \n                        trControl = control)\n\n# complex nonlinear\n### SVM  \nset.seed(7)   \n\nfit.svm &lt;- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"svmRadial\", \n                        metric = metric, \n                        trControl = control)\n\n### RF  \nset.seed(7)  \n\nfit.rf &lt;- caret::train(OUTCOMEVARIABLE~., \n                        data = dataset, \n                        method = \"rf\", \n                        metric = metric, \n                        trControl = control)\n\n### Note from website: \n  # caret can configure and tune the configuration of each model but that isn't done here (at least not yet)"
  },
  {
    "objectID": "posts/ML Template/index.html#select-the-best-model",
    "href": "posts/ML Template/index.html#select-the-best-model",
    "title": "Machine Learning Template",
    "section": "Select the best model",
    "text": "Select the best model\n\n# summarize accuracy of models  \nresults &lt;- caret::resamples(\n  list(\n    lda = fit.lda, \n    cart = fit.cart, \n    knn = fit.knn, \n    svm = fit.svm, \n    rf = fit.rf\n  )\n)\n\nsummary(results)\n\n\nPlot model evaluation results\n\nlattice::dotplot(results)\n# visualize the accuracy of each model\n\n\nprint(CHOSENMODEL)\n# standard deviation of \"accuracy\" and \"kappa\" were shown in the tutorial   \n\n### haven't figured it out yet... TBC..."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html",
    "href": "posts/Logistic Regression Project 23/index.html",
    "title": "Logistic Regression Project Spring 2023",
    "section": "",
    "text": "This is nearly the same project as the “Logistic Regression Project Winter 2022” except my group and I had a better grasp of what to do."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#libraries-used",
    "href": "posts/Logistic Regression Project 23/index.html#libraries-used",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Libraries Used",
    "text": "Libraries Used\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(skimr)\nlibrary(tidyr)\nlibrary(Hmisc)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(forcats) # for categorical variables\nlibrary(janitor) # for tables\nlibrary(gt)\nlibrary(gtable)\nlibrary(tidyverse) # data mgmt and visual\nlibrary(gtsummary)\nlibrary(broom)\nlibrary(kableExtra)\nlibrary(haven)\nlibrary(lmtest)\nlibrary(mfp)\nlibrary(ResourceSelection)\nlibrary(epiDisplay)"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#cleaning-final-dataset-df_swan",
    "href": "posts/Logistic Regression Project 23/index.html#cleaning-final-dataset-df_swan",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Cleaning Final Dataset df_swan",
    "text": "Cleaning Final Dataset df_swan"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#removing-observations-with-missing-points",
    "href": "posts/Logistic Regression Project 23/index.html#removing-observations-with-missing-points",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Removing Observations with Missing Points",
    "text": "Removing Observations with Missing Points"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#set-reference-levels",
    "href": "posts/Logistic Regression Project 23/index.html#set-reference-levels",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Set reference levels",
    "text": "Set reference levels\n\nWhat is blocked out: I removed the coding process on how we recoded our reference levels for our covariates, but I decided to include a brief bullet point list to show what those reference levels are and why.\n\n* High Blood Pressure: No, given the majority of this sample does not have high blood pressure and we’re seeking a change in what is associated with high blood pressure.\n\nSleep Quality: Very good and fairly good categories collapsed into one reference category, since we’re assessing if poor sleep is associated with high blood pressure.\nRace: Caucasian/White Non-Hispanic due to white privilege, and systemic and institutional racism in the U.S.\nSmoking Status: No, given the majority of the sample are non-smokers.\nEducation: College Graduate and Post-Graduate given previous knowledge that at a generalized population-level, those with higher education often have better health outcomes, typically due to having more health literacy and higher income.\n\nMarital Status: Currently Married, since the majority of the population is married.\nIncome: $100,000 or more which is the highest income category, given the opportunity for better health care quality and access.\nTaking Blood Pressure Medication: Yes, given that people taking medication for high blood pressure will likely have lower blood pressure than those who are not taking any.\nQuality of Life: This was a tricky category and we ended up using the median value which was around a score of 8 out of 10. There should’ve been more research conducted to decide what our cut-off point should’ve been."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#summary-new-dataset",
    "href": "posts/Logistic Regression Project 23/index.html#summary-new-dataset",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Summary new dataset",
    "text": "Summary new dataset"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#frequency-distributions",
    "href": "posts/Logistic Regression Project 23/index.html#frequency-distributions",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Frequency Distributions",
    "text": "Frequency Distributions\n\nggpairs comparison"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#contigency-tables",
    "href": "posts/Logistic Regression Project 23/index.html#contigency-tables",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Contigency Tables",
    "text": "Contigency Tables\n\nNote: This code below is gifted from my professor at the time. I became more familiar with lapply() throughout her course and the use of contingency tables came as a life saver, so with that note, enjoy.\n\n\n#Contingency Tables np &gt; 5, all meet this standard, except for Hispanic, which will be excluded in further analyses due to issues within the SWAN dataset. \ndf_swan_cat &lt;- df_swan_minusID %&gt;% dplyr::select_if(., ~class(.) == \"factor\") \n\nlapply(df_swan_cat, function(x) table(df_swan_cat$HBP, x))\n\n$HBP\n         x\n          (1) No (2) Yes\n  (1) No    1202       0\n  (2) Yes      0     619\n\n$Sleep\n         x\n          (1) Very good (2) Fairly good (3) Fairly bad (4) Very bad\n  (1) No            319             651            195           37\n  (2) Yes           119             334            134           32\n\n$Race\n         x\n          (0) Caucasian/White Non-Hispanic (1) Black/African American\n  (1) No                               706                        193\n  (2) Yes                              244                        283\n         x\n          (2) Chinese/Chinese American (3) Japanese/Japanese American\n  (1) No                           148                            155\n  (2) Yes                           43                             49\n         x\n          (5) Hispanic\n  (1) No             0\n  (2) Yes            0\n\n$Smoking\n         x\n          (1) No (2) Yes\n  (1) No    1080     122\n  (2) Yes    533      86\n\n$Education\n         x\n          (1) Less than high school (2) High school graduate\n  (1) No                         32                      153\n  (2) Yes                        31                      124\n         x\n          (3) Some college/technical school (4) College graduate\n  (1) No                                340                  321\n  (2) Yes                               226                  105\n         x\n          (5) Post graduate education\n  (1) No                          356\n  (2) Yes                         133\n\n$Marital\n         x\n          (1) Single/never married (2) Currently married/living as married\n  (1) No                       125                                     825\n  (2) Yes                       82                                     352\n         x\n          (3) Separated (4) Widowed (5) Divorced\n  (1) No             34          47          171\n  (2) Yes            23          39          123\n\n$Income\n         x\n          (1) Less Than $19,999 (2) $20,000 to $49,999 (3) $50,000 to $99,999\n  (1) No                     74                    307                    258\n  (2) Yes                    85                    160                    146\n         x\n          (4) $100,000 or More\n  (1) No                   563\n  (2) Yes                  228\n\n$BPMed\n         x\n          (1) No (2) Yes\n  (1) No    1184      18\n  (2) Yes     49     570\n\n\nThere are sufficient cases in all cells."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-1-univariable-analysis",
    "href": "posts/Logistic Regression Project 23/index.html#step-1-univariable-analysis",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 1 Univariable Analysis",
    "text": "Step 1 Univariable Analysis\n\nNote: I chose to show this code below since it was life changing for me. This code was also at the sole hands of my professor and I take no credit for this except for including the relevant covariates. \n\n#summarize and pull coefficients for univariate analysis for factored variables\nslr_df_swan = df_swan %&gt;% dplyr::select(-Age, -BMI, -Caffeine, -Sleep, -HBP, -Race, -Education, -Marital, -QualLife, -Smoking, -HBP01, -SWANID, - Income, -BPMed)\n\nlapply(slr_df_swan, function(x)summary(glm(df_swan$HBP01 ~ x, family = \"binomial\"))$coefficients)\n\n$Sleep01\n              Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept) -0.7614039 0.05690722 -13.379743 7.942151e-41\nx1           0.3862442 0.12581464   3.069946 2.140972e-03\nx2           0.6162219 0.24802310   2.484534 1.297210e-02\n\n$Smoking01\n              Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept) -0.7061949 0.05293484 -13.340833 1.339535e-40\nx1           0.3565211 0.15042183   2.370142 1.778124e-02\n\n$Education01\n              Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept) -1.0454006 0.07535765 -13.872521 9.295206e-44\nx1           1.0136519 0.26303392   3.853693 1.163496e-04\nx2           0.8352442 0.14240520   5.865265 4.484153e-09\nx3           0.6369900 0.11421353   5.577185 2.444421e-08\n\n$Marital01\n              Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept) -0.8517522 0.06366338 -13.378997 8.022254e-41\nx1           0.4301577 0.15571817   2.762412 5.737604e-03\nx2           0.4608859 0.27738611   1.661532 9.660664e-02\nx3           0.6651663 0.22576680   2.946254 3.216485e-03\nx4           0.5222730 0.13427979   3.889439 1.004763e-04\n\n$Income01\n              Estimate Std. Error    z value     Pr(&gt;|z|)\n(Intercept) -0.9039340  0.0784994 -11.515170 1.106407e-30\nx1           1.0425202  0.1773143   5.879503 4.115007e-09\nx2           0.2522601  0.1251778   2.015214 4.388223e-02\nx3           0.3345810  0.1299517   2.574657 1.003395e-02\n\n$BPMed01\n             Estimate Std. Error   z value      Pr(&gt;|z|)\n(Intercept)  3.455265  0.2393946  14.43335  3.192144e-47\nx1          -6.640098  0.2802901 -23.69009 4.561404e-124\n\n$QualLife01\n              Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -0.5260931 0.07822327 -6.725531 1.749527e-11\nx0          -0.1967592 0.11654625 -1.688250 9.136318e-02\nx2          -0.2623643 0.12332795 -2.127371 3.338930e-02\n\n\nAll of our variables move on to the creation of a preliminary model."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-2-preliminary-variable-selection",
    "href": "posts/Logistic Regression Project 23/index.html#step-2-preliminary-variable-selection",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 2 Preliminary Variable Selection",
    "text": "Step 2 Preliminary Variable Selection\n\nInitial Model Comparisons\nBased on the p values, our initial model would include Sleep01 (clinically significant + meets 0.25 criteria), Race (clinically significant and meets 0.05 criteria), Marital01 (meets 0.05 criteria), BMI (meets 0.05 criteria), BPMed01, (meets 0.05 criteria) and Income01 (meets 0.25 criteria) because the model with all variables is less precise at predicting hypertension in this given sample.\nHO: the beta coefficients are equal to zero\nHA: one or more beta coefficients are unequal to zero\nFor our next step, Sleep01 (clinically significant), Race (p &lt; 0.05), Marital01 (p &lt;0.05), BMI (p &lt; 0.05), and BPMed01 (p &lt; 0.05) will be retained in the next iteration of the model. The model with income will not move forward.\nFail to reject the null hypothesis. With a p-value greater than 0.05, we choose the reduced model.\nAt this point, our model is \\[logit((\\pi(HBP01| \\text{Sleep01}, \\text{Race}, \\text{Marital01} + \\text{BPMed01} + BMI)) = \\\\ \\\\beta_0 + \\beta_1 \\text{Fairly Bad Sleep} + \\beta_2 \\text{Very Bad Sleep} + \\beta_3 \\text{Black/African-American} \\\\ + \\beta_4 \\text{Chinese/Chinese-American} + \\beta_5 \\text{Japanese/Japanese-American} + \\beta_6 \\text{Single/never married} + \\beta_7 \\text{Separated} \\\\ + \\beta_8 \\text{Widowed} + \\beta_9 \\text{Divorced} + \\beta_{10} \\text{BPMed01} + \\beta_{11} \\text{BMI}\\]"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-3-assessing-change-in-coefficients-in-reduced-model",
    "href": "posts/Logistic Regression Project 23/index.html#step-3-assessing-change-in-coefficients-in-reduced-model",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 3 Assessing Change in Coefficients in Reduced Model",
    "text": "Step 3 Assessing Change in Coefficients in Reduced Model\n\nAssessing Without ‘Income01’\nChecking for change greater than 20% in coefficients.\nThere is evidence that some confounding may be occurring due to the high percent change among two variables, therefore income should remain in the model because it is potentially a confounder."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-4-adding-excluded-variables-to-the-reduced-model",
    "href": "posts/Logistic Regression Project 23/index.html#step-4-adding-excluded-variables-to-the-reduced-model",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 4 Adding Excluded Variables to the Reduced Model",
    "text": "Step 4 Adding Excluded Variables to the Reduced Model\n\nWhat is blocked out: For this step we ran a likelihood ratio test with the initial model as the reduced model and the initial model with the included covariate that we were assessing as the full model.\n\n\nAssessing Age\n\n\nAssessing Smoking01\n\n\nAssessing Education01\n\n\nAssessing Caffeine\n\n\nAssessing QualLife01\nNone of the initially excluded variables should be added back into the model.\n\n\nCollapse Variable Kevels\n\nAssessing binary sleep quality with our preliminary main effects model\n\n\nAssessing binary race with our preliminary main effects model\n\n\nAssessing binary marital status with our preliminary main effects model"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#preliminary-main-effects-model",
    "href": "posts/Logistic Regression Project 23/index.html#preliminary-main-effects-model",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Preliminary Main Effects Model",
    "text": "Preliminary Main Effects Model"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-5-assessing-scale-for-continuous-variables",
    "href": "posts/Logistic Regression Project 23/index.html#step-5-assessing-scale-for-continuous-variables",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 5 Assessing Scale for Continuous Variables",
    "text": "Step 5 Assessing Scale for Continuous Variables\n\nSmoothed Scatter Plots\n\n\nFractional Polynomials\n\nThis was running prior but for some reason isn’t as of this point in time of me trying to render this page. I’m keeping it in just for the sake of showing we assessed fractional polynomials.\n\n\n#fracpoly_b = mfp(HBP01~ fp(BMI, df = 4) + Age + BPMed01 + Sleep01 + Income01 + Marital01,\n#               data=df_swan, family = \"binomial\", verbose = T)\n\n#fracpoly_b$fptable\n\n#linear model works for BMI"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#main-effects-model",
    "href": "posts/Logistic Regression Project 23/index.html#main-effects-model",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Main effects model",
    "text": "Main effects model\n\nModel\n\nmain.effects.model &lt;- initial.model\n\nAfter assessing the linearity of our continuous variables, the best performing model continues to be ‘initial.model’, renamed as ‘main.effects.model’"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-6-interactions",
    "href": "posts/Logistic Regression Project 23/index.html#step-6-interactions",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 6 Interactions",
    "text": "Step 6 Interactions\n\nNote: I have no idea how to fix this issue below. I have include and echo = FALSE but given the structure of the code (hint: it’s lapply and function but pulling stuff from the environment so it is kind of creating it’s own separate code chunk with its own set of laws I suppose).\n\n\n\n[[1]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                              Estimate Std. Error z value\n(Intercept)                                    1.56042    0.62781   2.485\nSleep011                                      -0.10963    0.48197  -0.227\nSleep012                                      -0.02100    1.00728  -0.021\nRace(1) Black/African American                 0.60335    0.37758   1.598\nRace(2) Chinese/Chinese American              -0.20639    0.53947  -0.383\nRace(3) Japanese/Japanese American             0.96150    0.42391   2.268\nBPMed011                                      -6.78456    0.31822 -21.320\nBMI                                            0.06239    0.02013   3.100\nIncome011                                     -0.13915    0.50819  -0.274\nIncome012                                     -0.12330    0.33666  -0.366\nIncome013                                      0.51445    0.32541   1.581\nMarital011                                    -0.96083    0.46253  -2.077\nMarital012                                    -0.29235    0.74636  -0.392\nMarital013                                    -0.66475    0.61208  -1.086\nMarital014                                    -0.18417    0.37680  -0.489\nSleep011:Race(1) Black/African American        1.37644    0.74615   1.845\nSleep012:Race(1) Black/African American        1.82494    1.21404   1.503\nSleep011:Race(2) Chinese/Chinese American      1.25486    1.16657   1.076\nSleep012:Race(2) Chinese/Chinese American    -11.85917  591.31736  -0.020\nSleep011:Race(3) Japanese/Japanese American    0.48137    1.06091   0.454\nSleep012:Race(3) Japanese/Japanese American   12.44606 1455.39799   0.009\n                                            Pr(&gt;|z|)    \n(Intercept)                                  0.01294 *  \nSleep011                                     0.82007    \nSleep012                                     0.98337    \nRace(1) Black/African American               0.11006    \nRace(2) Chinese/Chinese American             0.70204    \nRace(3) Japanese/Japanese American           0.02332 *  \nBPMed011                                     &lt; 2e-16 ***\nBMI                                          0.00194 ** \nIncome011                                    0.78422    \nIncome012                                    0.71418    \nIncome013                                    0.11390    \nMarital011                                   0.03777 *  \nMarital012                                   0.69528    \nMarital013                                   0.27746    \nMarital014                                   0.62501    \nSleep011:Race(1) Black/African American      0.06508 .  \nSleep012:Race(1) Black/African American      0.13279    \nSleep011:Race(2) Chinese/Chinese American    0.28207    \nSleep012:Race(2) Chinese/Chinese American    0.98400    \nSleep011:Race(3) Japanese/Japanese American  0.65002    \nSleep012:Race(3) Japanese/Japanese American  0.99318    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.5  on 1820  degrees of freedom\nResidual deviance:  525.8  on 1800  degrees of freedom\nAIC: 567.8\n\nNumber of Fisher Scoring iterations: 14\n\n\n[[2]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.283439   0.619927   2.070  0.03842 *  \nSleep011                            0.786703   0.765332   1.028  0.30399    \nSleep012                           -0.037535   1.085317  -0.035  0.97241    \nRace(1) Black/African American      1.011852   0.320778   3.154  0.00161 ** \nRace(2) Chinese/Chinese American    0.007866   0.473709   0.017  0.98675    \nRace(3) Japanese/Japanese American  1.107460   0.394037   2.811  0.00495 ** \nBPMed011                           -6.692071   0.344239 -19.440  &lt; 2e-16 ***\nBMI                                 0.064413   0.019874   3.241  0.00119 ** \nIncome011                          -0.072334   0.514299  -0.141  0.88815    \nIncome012                          -0.099978   0.332353  -0.301  0.76355    \nIncome013                           0.465729   0.323356   1.440  0.14978    \nMarital011                         -0.956631   0.468640  -2.041  0.04122 *  \nMarital012                         -0.252366   0.757010  -0.333  0.73885    \nMarital013                         -0.693576   0.610387  -1.136  0.25584    \nMarital014                         -0.179125   0.373172  -0.480  0.63122    \nSleep011:BPMed011                  -0.395966   0.855614  -0.463  0.64352    \nSleep012:BPMed011                   1.180241   1.213732   0.972  0.33085    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  531.03  on 1804  degrees of freedom\nAIC: 565.03\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[3]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.4309101  0.7418752   1.929  0.05376 .  \nSleep011                            0.0344342  1.1824329   0.029  0.97677    \nSleep012                            0.9843446  2.0459917   0.481  0.63044    \nRace(1) Black/African American      1.0313280  0.3205687   3.217  0.00129 ** \nRace(2) Chinese/Chinese American    0.0114880  0.4730464   0.024  0.98063    \nRace(3) Japanese/Japanese American  1.1035742  0.3936501   2.803  0.00506 ** \nBPMed011                           -6.7155294  0.3129457 -21.459  &lt; 2e-16 ***\nBMI                                 0.0595600  0.0249609   2.386  0.01703 *  \nIncome011                          -0.1029611  0.5192188  -0.198  0.84281    \nIncome012                          -0.1147497  0.3327172  -0.345  0.73018    \nIncome013                           0.4743448  0.3223023   1.472  0.14109    \nMarital011                         -0.9910955  0.4704926  -2.107  0.03516 *  \nMarital012                         -0.2706109  0.7497872  -0.361  0.71816    \nMarital013                         -0.6618042  0.6123452  -1.081  0.27980    \nMarital014                         -0.1819259  0.3727933  -0.488  0.62554    \nSleep011:BMI                        0.0154446  0.0400876   0.385  0.70004    \nSleep012:BMI                        0.0003948  0.0660636   0.006  0.99523    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  531.97  on 1804  degrees of freedom\nAIC: 565.97\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[4]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.14122    0.62595   1.823  0.06828 .  \nSleep011                            0.87988    0.49410   1.781  0.07495 .  \nSleep012                            1.54734    0.82887   1.867  0.06193 .  \nRace(1) Black/African American      1.02999    0.32125   3.206  0.00135 ** \nRace(2) Chinese/Chinese American    0.01377    0.47207   0.029  0.97672    \nRace(3) Japanese/Japanese American  1.13423    0.39309   2.885  0.00391 ** \nBPMed011                           -6.72042    0.31329 -21.451  &lt; 2e-16 ***\nBMI                                 0.06571    0.02018   3.256  0.00113 ** \nIncome011                           0.14579    0.63536   0.229  0.81852    \nIncome012                           0.17013    0.37766   0.450  0.65235    \nIncome013                           0.56681    0.37665   1.505  0.13236    \nMarital011                         -0.98891    0.47239  -2.093  0.03631 *  \nMarital012                         -0.27791    0.74356  -0.374  0.70858    \nMarital013                         -0.64259    0.62098  -1.035  0.30076    \nMarital014                         -0.13161    0.37549  -0.350  0.72597    \nSleep011:Income011                 -0.42898    1.10299  -0.389  0.69733    \nSleep012:Income011                 -1.66471    1.67488  -0.994  0.32026    \nSleep011:Income012                 -1.07495    0.84152  -1.277  0.20147    \nSleep012:Income012                 -1.71441    1.69364  -1.012  0.31141    \nSleep011:Income013                 -0.44914    0.82911  -0.542  0.58801    \nSleep012:Income013                 -0.22559    1.18940  -0.190  0.84957    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  528.84  on 1800  degrees of freedom\nAIC: 570.84\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[5]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients: (1 not defined because of singularities)\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                          1.35617    0.62068   2.185  0.02889 *  \nSleep011                             0.40093    0.41468   0.967  0.33363    \nSleep012                             1.36569    0.60866   2.244  0.02485 *  \nRace(1) Black/African American       1.03074    0.32136   3.207  0.00134 ** \nRace(2) Chinese/Chinese American     0.01987    0.47081   0.042  0.96634    \nRace(3) Japanese/Japanese American   1.10420    0.39440   2.800  0.00511 ** \nBPMed011                            -6.72736    0.31459 -21.384  &lt; 2e-16 ***\nBMI                                  0.06168    0.02010   3.069  0.00215 ** \nIncome011                           -0.01202    0.53155  -0.023  0.98195    \nIncome012                           -0.06614    0.33268  -0.199  0.84242    \nIncome013                            0.47009    0.32250   1.458  0.14494    \nMarital011                          -1.17576    0.54071  -2.174  0.02967 *  \nMarital012                          -0.66047    0.93076  -0.710  0.47795    \nMarital013                          -0.52086    0.69144  -0.753  0.45127    \nMarital014                           0.01153    0.42207   0.027  0.97821    \nSleep011:Marital011                  0.89502    1.04748   0.854  0.39286    \nSleep012:Marital011                 -0.35902    1.68035  -0.214  0.83081    \nSleep011:Marital012                  1.13744    1.45092   0.784  0.43307    \nSleep012:Marital012                       NA         NA      NA       NA    \nSleep011:Marital013                 -0.26598    1.71142  -0.155  0.87649    \nSleep012:Marital013                -13.54186  709.56262  -0.019  0.98477    \nSleep011:Marital014                 -0.43770    0.91037  -0.481  0.63067    \nSleep012:Marital014                 -2.22310    1.96595  -1.131  0.25814    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  528.29  on 1799  degrees of freedom\nAIC: 572.29\n\nNumber of Fisher Scoring iterations: 14\n\n\n[[6]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                             Estimate Std. Error z value\n(Intercept)                                   1.25868    0.62657   2.009\nSleep011                                      0.45799    0.32679   1.402\nSleep012                                      0.95832    0.53998   1.775\nRace(1) Black/African American                0.85423    0.53112   1.608\nRace(2) Chinese/Chinese American              1.04538    1.06584   0.981\nRace(3) Japanese/Japanese American           14.75406  616.10033   0.024\nBPMed011                                     -6.57544    0.39848 -16.501\nBMI                                           0.06246    0.01955   3.195\nIncome011                                    -0.05439    0.50587  -0.108\nIncome012                                    -0.11552    0.33194  -0.348\nIncome013                                     0.48715    0.32410   1.503\nMarital011                                   -0.98460    0.46451  -2.120\nMarital012                                   -0.30817    0.74049  -0.416\nMarital013                                   -0.67766    0.60189  -1.126\nMarital014                                   -0.17412    0.36773  -0.474\nRace(1) Black/African American:BPMed011       0.22512    0.62517   0.360\nRace(2) Chinese/Chinese American:BPMed011    -1.63565    1.30190  -1.256\nRace(3) Japanese/Japanese American:BPMed011 -13.78653  616.10047  -0.022\n                                            Pr(&gt;|z|)    \n(Intercept)                                   0.0446 *  \nSleep011                                      0.1611    \nSleep012                                      0.0759 .  \nRace(1) Black/African American                0.1078    \nRace(2) Chinese/Chinese American              0.3267    \nRace(3) Japanese/Japanese American            0.9809    \nBPMed011                                      &lt;2e-16 ***\nBMI                                           0.0014 ** \nIncome011                                     0.9144    \nIncome012                                     0.7278    \nIncome013                                     0.1328    \nMarital011                                    0.0340 *  \nMarital012                                    0.6773    \nMarital013                                    0.2602    \nMarital014                                    0.6359    \nRace(1) Black/African American:BPMed011       0.7188    \nRace(2) Chinese/Chinese American:BPMed011     0.2090    \nRace(3) Japanese/Japanese American:BPMed011   0.9821    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  528.04  on 1803  degrees of freedom\nAIC: 564.04\n\nNumber of Fisher Scoring iterations: 16\n\n\n[[7]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                             1.37585    0.85012   1.618  0.10557    \nSleep011                                0.44089    0.33045   1.334  0.18214    \nSleep012                                1.02068    0.54156   1.885  0.05947 .  \nRace(1) Black/African American          1.66642    1.27444   1.308  0.19102    \nRace(2) Chinese/Chinese American       -6.26118    2.40784  -2.600  0.00931 ** \nRace(3) Japanese/Japanese American      1.79959    2.38999   0.753  0.45147    \nBPMed011                               -6.77650    0.32005 -21.173  &lt; 2e-16 ***\nBMI                                     0.06259    0.02918   2.145  0.03195 *  \nIncome011                              -0.04964    0.52346  -0.095  0.92445    \nIncome012                              -0.09452    0.33537  -0.282  0.77807    \nIncome013                               0.49735    0.32564   1.527  0.12669    \nMarital011                             -0.96841    0.47975  -2.019  0.04353 *  \nMarital012                             -0.22154    0.73639  -0.301  0.76353    \nMarital013                             -0.68280    0.62815  -1.087  0.27704    \nMarital014                             -0.16290    0.37653  -0.433  0.66528    \nRace(1) Black/African American:BMI     -0.02091    0.04231  -0.494  0.62111    \nRace(2) Chinese/Chinese American:BMI    0.26223    0.09681   2.709  0.00675 ** \nRace(3) Japanese/Japanese American:BMI -0.02952    0.10127  -0.292  0.77065    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  522.74  on 1803  degrees of freedom\nAIC: 558.74\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[8]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                             Estimate Std. Error z value\n(Intercept)                                   1.30106    0.62418   2.084\nSleep011                                      0.50344    0.32978   1.527\nSleep012                                      0.98297    0.54702   1.797\nRace(1) Black/African American                1.16812    0.53399   2.188\nRace(2) Chinese/Chinese American              0.69325    0.64116   1.081\nRace(3) Japanese/Japanese American            0.45906    0.65119   0.705\nBPMed011                                     -6.77442    0.31947 -21.205\nBMI                                           0.06531    0.01992   3.278\nIncome011                                    -0.62625    0.83636  -0.749\nIncome012                                     0.07351    0.47983   0.153\nIncome013                                     0.40344    0.48997   0.823\nMarital011                                   -0.95487    0.46734  -2.043\nMarital012                                   -0.32005    0.75877  -0.422\nMarital013                                   -0.70153    0.60049  -1.168\nMarital014                                   -0.13334    0.37929  -0.352\nRace(1) Black/African American:Income011      0.64942    1.06662   0.609\nRace(2) Chinese/Chinese American:Income011    0.43473    2.20175   0.197\nRace(3) Japanese/Japanese American:Income011 -0.11530    3.25302  -0.035\nRace(1) Black/African American:Income012     -0.70536    0.79130  -0.891\nRace(2) Chinese/Chinese American:Income012   -1.62690    1.18703  -1.371\nRace(3) Japanese/Japanese American:Income012  1.02195    0.94502   1.081\nRace(1) Black/African American:Income013     -0.01481    0.78351  -0.019\nRace(2) Chinese/Chinese American:Income013   -1.38165    1.17284  -1.178\nRace(3) Japanese/Japanese American:Income013  1.19847    0.92406   1.297\n                                             Pr(&gt;|z|)    \n(Intercept)                                   0.03712 *  \nSleep011                                      0.12687    \nSleep012                                      0.07234 .  \nRace(1) Black/African American                0.02870 *  \nRace(2) Chinese/Chinese American              0.27959    \nRace(3) Japanese/Japanese American            0.48084    \nBPMed011                                      &lt; 2e-16 ***\nBMI                                           0.00105 ** \nIncome011                                     0.45399    \nIncome012                                     0.87824    \nIncome013                                     0.41028    \nMarital011                                    0.04103 *  \nMarital012                                    0.67317    \nMarital013                                    0.24271    \nMarital014                                    0.72518    \nRace(1) Black/African American:Income011      0.54261    \nRace(2) Chinese/Chinese American:Income011    0.84348    \nRace(3) Japanese/Japanese American:Income011  0.97173    \nRace(1) Black/African American:Income012      0.37272    \nRace(2) Chinese/Chinese American:Income012    0.17051    \nRace(3) Japanese/Japanese American:Income012  0.27952    \nRace(1) Black/African American:Income013      0.98492    \nRace(2) Chinese/Chinese American:Income013    0.23878    \nRace(3) Japanese/Japanese American:Income013  0.19464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  524.33  on 1797  degrees of freedom\nAIC: 572.33\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[9]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                              Estimate Std. Error z value\n(Intercept)                                    1.36437    0.61438   2.221\nSleep011                                       0.44482    0.33024   1.347\nSleep012                                       0.99189    0.54449   1.822\nRace(1) Black/African American                 0.94419    0.42391   2.227\nRace(2) Chinese/Chinese American               0.12708    0.53506   0.238\nRace(3) Japanese/Japanese American             0.98285    0.44387   2.214\nBPMed011                                      -6.76753    0.32034 -21.126\nBMI                                            0.06444    0.01981   3.253\nIncome011                                     -0.13782    0.53275  -0.259\nIncome012                                     -0.09609    0.33547  -0.286\nIncome013                                      0.49708    0.32392   1.535\nMarital011                                    -0.97043    0.69881  -1.389\nMarital012                                    -0.82210    1.64310  -0.500\nMarital013                                    -1.29979    0.96144  -1.352\nMarital014                                    -0.21428    0.58087  -0.369\nRace(1) Black/African American:Marital011      0.22114    0.96109   0.230\nRace(2) Chinese/Chinese American:Marital011   -1.44056    1.45729  -0.989\nRace(3) Japanese/Japanese American:Marital011  0.07479    1.98187   0.038\nRace(1) Black/African American:Marital012      0.77441    1.89299   0.409\nRace(2) Chinese/Chinese American:Marital012    0.69836    3.31944   0.210\nRace(3) Japanese/Japanese American:Marital012  0.47673    3.83073   0.124\nRace(1) Black/African American:Marital013      0.43212    1.30390   0.331\nRace(2) Chinese/Chinese American:Marital013    1.29374    2.26170   0.572\nRace(3) Japanese/Japanese American:Marital013  2.16745    1.50353   1.442\nRace(1) Black/African American:Marital014      0.12267    0.82102   0.149\nRace(2) Chinese/Chinese American:Marital014   -0.44326    1.62551  -0.273\nRace(3) Japanese/Japanese American:Marital014  0.20135    1.20515   0.167\n                                              Pr(&gt;|z|)    \n(Intercept)                                    0.02637 *  \nSleep011                                       0.17799    \nSleep012                                       0.06850 .  \nRace(1) Black/African American                 0.02593 *  \nRace(2) Chinese/Chinese American               0.81226    \nRace(3) Japanese/Japanese American             0.02681 *  \nBPMed011                                       &lt; 2e-16 ***\nBMI                                            0.00114 ** \nIncome011                                      0.79586    \nIncome012                                      0.77455    \nIncome013                                      0.12489    \nMarital011                                     0.16493    \nMarital012                                     0.61684    \nMarital013                                     0.17640    \nMarital014                                     0.71220    \nRace(1) Black/African American:Marital011      0.81802    \nRace(2) Chinese/Chinese American:Marital011    0.32290    \nRace(3) Japanese/Japanese American:Marital011  0.96990    \nRace(1) Black/African American:Marital012      0.68247    \nRace(2) Chinese/Chinese American:Marital012    0.83337    \nRace(3) Japanese/Japanese American:Marital012  0.90096    \nRace(1) Black/African American:Marital013      0.74034    \nRace(2) Chinese/Chinese American:Marital013    0.56731    \nRace(3) Japanese/Japanese American:Marital013  0.14942    \nRace(1) Black/African American:Marital014      0.88123    \nRace(2) Chinese/Chinese American:Marital014    0.78509    \nRace(3) Japanese/Japanese American:Marital014  0.86731    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  528.74  on 1794  degrees of freedom\nAIC: 582.74\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[10]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         2.64505    1.05770   2.501  0.01239 *  \nSleep011                            0.44389    0.32776   1.354  0.17563    \nSleep012                            0.95084    0.54138   1.756  0.07903 .  \nRace(1) Black/African American      0.99884    0.31618   3.159  0.00158 ** \nRace(2) Chinese/Chinese American   -0.01643    0.49578  -0.033  0.97357    \nRace(3) Japanese/Japanese American  1.15556    0.40064   2.884  0.00392 ** \nBPMed011                           -8.46216    1.19238  -7.097 1.28e-12 ***\nBMI                                 0.01637    0.03503   0.467  0.64020    \nIncome011                          -0.04911    0.50423  -0.097  0.92242    \nIncome012                          -0.10908    0.33386  -0.327  0.74387    \nIncome013                           0.48649    0.32368   1.503  0.13285    \nMarital011                         -0.95338    0.46085  -2.069  0.03857 *  \nMarital012                         -0.26923    0.75379  -0.357  0.72096    \nMarital013                         -0.68278    0.60243  -1.133  0.25705    \nMarital014                         -0.16046    0.37106  -0.432  0.66543    \nBPMed011:BMI                        0.06243    0.03976   1.570  0.11632    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  529.84  on 1805  degrees of freedom\nAIC: 561.84\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[11]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.38591    0.65908   2.103  0.03549 *  \nSleep011                            0.43284    0.32840   1.318  0.18750    \nSleep012                            0.91839    0.52841   1.738  0.08221 .  \nRace(1) Black/African American      1.03819    0.31911   3.253  0.00114 ** \nRace(2) Chinese/Chinese American    0.03094    0.48296   0.064  0.94892    \nRace(3) Japanese/Japanese American  1.12533    0.39829   2.825  0.00472 ** \nBPMed011                           -6.79080    0.46714 -14.537  &lt; 2e-16 ***\nBMI                                 0.06348    0.01958   3.242  0.00119 ** \nIncome011                          -0.90032    0.63782  -1.412  0.15808    \nIncome012                          -0.02766    0.61065  -0.045  0.96387    \nIncome013                           1.34348    1.07870   1.245  0.21296    \nMarital011                         -0.99234    0.46327  -2.142  0.03219 *  \nMarital012                         -0.35328    0.72832  -0.485  0.62763    \nMarital013                         -0.71560    0.59516  -1.202  0.22922    \nMarital014                         -0.20351    0.37315  -0.545  0.58549    \nBPMed011:Income011                  1.49964    0.79301   1.891  0.05862 .  \nBPMed011:Income012                 -0.12341    0.73188  -0.169  0.86610    \nBPMed011:Income013                 -0.95115    1.13915  -0.835  0.40373    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  526.64  on 1803  degrees of freedom\nAIC: 562.64\n\nNumber of Fisher Scoring iterations: 7\n\n\n[[12]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                          1.190253   0.630297   1.888  0.05897 .  \nSleep011                             0.476017   0.326834   1.456  0.14527    \nSleep012                             0.983765   0.539665   1.823  0.06832 .  \nRace(1) Black/African American       1.031145   0.322105   3.201  0.00137 ** \nRace(2) Chinese/Chinese American     0.009365   0.470194   0.020  0.98411    \nRace(3) Japanese/Japanese American   1.095500   0.392249   2.793  0.00522 ** \nBPMed011                            -6.586487   0.382266 -17.230  &lt; 2e-16 ***\nBMI                                  0.064660   0.019886   3.252  0.00115 ** \nIncome011                           -0.065775   0.531515  -0.124  0.90151    \nIncome012                           -0.110428   0.332671  -0.332  0.73993    \nIncome013                            0.479812   0.321167   1.494  0.13518    \nMarital011                          -0.925456   0.633058  -1.462  0.14377    \nMarital012                          12.703182 496.312880   0.026  0.97958    \nMarital013                          -0.056510   1.075610  -0.053  0.95810    \nMarital014                          -0.182469   0.685690  -0.266  0.79015    \nBPMed011:Marital011                 -0.064857   0.881572  -0.074  0.94135    \nBPMed011:Marital012                -13.480348 496.313962  -0.027  0.97833    \nBPMed011:Marital013                 -1.180383   1.489617  -0.792  0.42812    \nBPMed011:Marital014                  0.006055   0.788735   0.008  0.99387    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  529.94  on 1802  degrees of freedom\nAIC: 567.94\n\nNumber of Fisher Scoring iterations: 15\n\n\n[[13]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         0.315583   0.887546   0.356  0.72216    \nSleep011                            0.502386   0.331118   1.517  0.12921    \nSleep012                            0.972887   0.549682   1.770  0.07674 .  \nRace(1) Black/African American      1.024278   0.323054   3.171  0.00152 ** \nRace(2) Chinese/Chinese American    0.002979   0.464039   0.006  0.99488    \nRace(3) Japanese/Japanese American  1.125004   0.396413   2.838  0.00454 ** \nBPMed011                           -6.762386   0.317151 -21.322  &lt; 2e-16 ***\nBMI                                 0.102477   0.031199   3.285  0.00102 ** \nIncome011                           0.498585   1.818145   0.274  0.78391    \nIncome012                           0.983624   1.333666   0.738  0.46080    \nIncome013                           3.196710   1.376198   2.323  0.02019 *  \nMarital011                         -0.988340   0.472232  -2.093  0.03636 *  \nMarital012                         -0.270663   0.748400  -0.362  0.71761    \nMarital013                         -0.650182   0.611307  -1.064  0.28751    \nMarital014                         -0.203199   0.374672  -0.542  0.58758    \nBMI:Income011                      -0.024693   0.057232  -0.431  0.66613    \nBMI:Income012                      -0.041067   0.047155  -0.871  0.38381    \nBMI:Income013                      -0.100878   0.049944  -2.020  0.04340 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  527.84  on 1803  degrees of freedom\nAIC: 563.84\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[14]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.58211    0.78338   2.020  0.04342 *  \nSleep011                            0.45483    0.32711   1.390  0.16439    \nSleep012                            1.03466    0.54228   1.908  0.05639 .  \nRace(1) Black/African American      1.01710    0.32155   3.163  0.00156 ** \nRace(2) Chinese/Chinese American   -0.01606    0.47346  -0.034  0.97294    \nRace(3) Japanese/Japanese American  1.07766    0.39358   2.738  0.00618 ** \nBPMed011                           -6.72695    0.31484 -21.366  &lt; 2e-16 ***\nBMI                                 0.05421    0.02678   2.024  0.04294 *  \nIncome011                          -0.11705    0.52422  -0.223  0.82332    \nIncome012                          -0.09297    0.33308  -0.279  0.78015    \nIncome013                           0.48777    0.32365   1.507  0.13179    \nMarital011                         -2.25854    1.50172  -1.504  0.13259    \nMarital012                          0.56140    3.15687   0.178  0.85885    \nMarital013                          0.46822    2.72212   0.172  0.86343    \nMarital014                         -0.71324    1.43534  -0.497  0.61925    \nBMI:Marital011                      0.04221    0.04745   0.890  0.37370    \nBMI:Marital012                     -0.02920    0.11336  -0.258  0.79670    \nBMI:Marital013                     -0.03867    0.09290  -0.416  0.67718    \nBMI:Marital014                      0.01944    0.04941   0.393  0.69405    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  530.85  on 1802  degrees of freedom\nAIC: 568.85\n\nNumber of Fisher Scoring iterations: 6\n\n\n[[15]]\n\nCall:\nglm(formula = reformulate(c(vars, cb), \"HBP01\", env = .env), \n    family = binomial, data = df_swan)\n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                         1.469764   0.631645   2.327  0.01997 *  \nSleep011                            0.451884   0.331023   1.365  0.17222    \nSleep012                            1.025742   0.550423   1.864  0.06238 .  \nRace(1) Black/African American      1.010216   0.321705   3.140  0.00169 ** \nRace(2) Chinese/Chinese American    0.003417   0.479237   0.007  0.99431    \nRace(3) Japanese/Japanese American  1.113929   0.394628   2.823  0.00476 ** \nBPMed011                           -6.840611   0.330909 -20.672  &lt; 2e-16 ***\nBMI                                 0.061862   0.020035   3.088  0.00202 ** \nIncome011                           0.644566   0.952429   0.677  0.49856    \nIncome012                          -0.207287   0.421030  -0.492  0.62248    \nIncome013                           0.483421   0.378291   1.278  0.20128    \nMarital011                         -0.835400   0.981333  -0.851  0.39461    \nMarital012                         -0.770281   1.387083  -0.555  0.57867    \nMarital013                         -1.391565   0.958184  -1.452  0.14642    \nMarital014                          0.044020   0.632396   0.070  0.94451    \nIncome011:Marital011               -0.247593   1.534024  -0.161  0.87178    \nIncome012:Marital011               -0.449977   1.231241  -0.365  0.71476    \nIncome013:Marital011               -0.708678   1.404843  -0.504  0.61394    \nIncome011:Marital012               -0.921711   2.285121  -0.403  0.68669    \nIncome012:Marital012                0.776991   2.225427   0.349  0.72698    \nIncome013:Marital012                1.339378   1.776618   0.754  0.45091    \nIncome011:Marital013               -0.106891   1.796217  -0.060  0.95255    \nIncome012:Marital013                0.668799   1.541345   0.434  0.66436    \nIncome013:Marital013                1.856966   1.435691   1.293  0.19586    \nIncome011:Marital014               -1.929189   1.371690  -1.406  0.15960    \nIncome012:Marital014                0.339140   0.891680   0.380  0.70369    \nIncome013:Marital014               -0.552293   1.013895  -0.545  0.58594    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2334.47  on 1820  degrees of freedom\nResidual deviance:  525.28  on 1794  degrees of freedom\nAIC: 579.28\n\nNumber of Fisher Scoring iterations: 6\n\n\nThere are no significant interactions present between the levels of Sleep01 and other variables included in our main effects model at the alpha = 0.05 level."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#preliminary-final-model",
    "href": "posts/Logistic Regression Project 23/index.html#preliminary-final-model",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Preliminary Final Model",
    "text": "Preliminary Final Model\n\nModel\nAfter assessing the linearity of our continuous variables, the best performing model continues to be ‘main.effects.model’"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#step-7-addressing-issues-and-assessing-model-fit",
    "href": "posts/Logistic Regression Project 23/index.html#step-7-addressing-issues-and-assessing-model-fit",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Step 7 Addressing Issues and Assessing Model Fit",
    "text": "Step 7 Addressing Issues and Assessing Model Fit\n\nVIF\n\nlibrary(car)\ncar::vif(main.effects.model)\n\n              GVIF Df GVIF^(1/(2*Df))\nSleep01   1.099543  2        1.024007\nRace      1.432084  3        1.061683\nMarital01 1.374184  4        1.040532\nBMI       1.238386  1        1.112828\nBPMed01   1.208420  1        1.099282\nIncome01  1.269470  3        1.040568\n\n\nThere does not appear to be an issue with colinearity.\n\n\nPearson Goodness of Fit\nWe do not use Pearson Goodness of fit due to the continuous BMI Variable.\n\n\nHosmer Lemeshow Goodness of Fit\nCovariate pattern is greater than six due to the existence of a continuous variable in the model.\n\\(H_0:\\) the model fits the data well\n\\(H_A:\\) the model does not fit the data well\nConclusion: Fail to reject null hypothesis. Thus, the selected model for SWAN dataset fits the data relatively well.\n\n\nROC Curve and AUC\nTo quantify how well our model predicts a binary outcome.\n\n\n\n\n\n\n\n\n\nOur ROC is quite good, 0.973 is excellent, perhaps excessively so for a class project model using real-world data.\n\n\nAIC and BIC\nThe AIC and BIC are equivalent between the two models, 557.47/601.52, meaning they are comparable.\n\n\nVisual Logistic Diagnostics\n\nCharts\n\nsource(\"Logistic_Dx_Functions.R\") \n\nPlot the change in standardized deviance residuals against the estimated/predicted probabilities.\nPlot the change in coefficient estimates against the estimated/predicted probabilities.\n\n\nDiagnostic points\nThe AIC and BIC are very small when influential points are excluded, so it is likely the model is over fit when the influential points are removed.\nThe ROC and AUC are comparable to that of the preliminary final model."
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#estimated-logistic-regression-equation",
    "href": "posts/Logistic Regression Project 23/index.html#estimated-logistic-regression-equation",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Estimated Logistic Regression Equation",
    "text": "Estimated Logistic Regression Equation\nWith covariates included \\[logit((\\hat{\\pi}(HBP01| \\text{Sleep01}, \\text{Race}, \\text{Marital01} + \\text{BPMed01} + BMI + \\text{Income01})) = \\\\ 1.29180 + 0.47144 \\cdot \\text{Fairly Bad Sleep} + 0.97776 \\cdot \\text{Very Bad Sleep} + 1.02938 \\cdot \\text{Black/African-American} \\\\ + 0.02416 \\cdot \\text{Chinese/Chinese-American} + 1.10728 \\cdot \\text{Japanese/Japanese-American} - 0.98783 \\cdot \\text{Single/never married} \\\\ - 0.26615 \\cdot \\text{Separated} - 0.6754 \\cdot \\text{Widowed} - -0.18008 \\cdot \\text{Divorced} - 6.70745 \\cdot \\text{BPMed01} + 0.06436 \\cdot \\text{BMI} \\\\ +  - 0.08290 \\cdot \\text {Annual Income}&lt;19,999 - 0.10850 \\cdot \\text{Annual Income}20-49,999 + 0.47616 \\cdot \\text{Annual Income}50-99,999\\]"
  },
  {
    "objectID": "posts/Logistic Regression Project 23/index.html#odds-ratio-tables",
    "href": "posts/Logistic Regression Project 23/index.html#odds-ratio-tables",
    "title": "Logistic Regression Project Spring 2023",
    "section": "Odds Ratio Tables",
    "text": "Odds Ratio Tables\n\ntable2\n\n\n\n\n\n\nCrude OR\nAdjusted OR\nP-value (Wald's Test)\nP-value (LR Test\n\n\n\n\nSleep01..ref..0\n\n\n\n0.109\n\n\nX...1\n1.47 (1.15,1.88)\n1.6 (0.85,3.04)\n0.149\n\n\n\nX...2\n1.85 (1.14,3.01)\n2.66 (0.92,7.65)\n0.07\n\n\n\nX\n\n\n\n\n\n\nRace..ref...0..Caucasian.White.Non.Hispanic\n\n\n\n0.001\n\n\nX....1..Black.African.American\n4.24 (3.36,5.36)\n2.8 (1.49,5.25)\n0.001\n\n\n\nX....2..Chinese.Chinese.American\n0.84 (0.58,1.22)\n1.02 (0.41,2.58)\n0.959\n\n\n\nX....3..Japanese.Japanese.American\n0.91 (0.64,1.3)\n3.03 (1.4,6.53)\n0.005\n\n\n\nX.1\n\n\n\n\n\n\nMarital01..ref..0\n\n\n\n0.265\n\n\nX...1.1\n1.54 (1.13,2.09)\n0.37 (0.15,0.94)\n0.035\n\n\n\nX...2.1\n1.59 (0.92,2.73)\n0.77 (0.18,3.33)\n0.722\n\n\n\nX...3\n1.94 (1.25,3.03)\n0.51 (0.15,1.68)\n0.267\n\n\n\nX...4\n1.69 (1.3,2.19)\n0.84 (0.4,1.73)\n0.628\n\n\n\nX.2\n\n\n\n\n\n\nBMI..cont..var..\n1.13 (1.11,1.15)\n1.07 (1.03,1.11)\n0.001\n0.002\n\n\nX.3\n\n\n\n\n\n\nBPMed01..1.vs.0\n0 (0,0)\n0 (0,0)\n&lt; 0.001\n&lt; 0.001\n\n\nX.4\n\n\n\n\n\n\nIncome01..ref..0\n\n\n\n0.362\n\n\nX...1.2\n2.84 (2,4.02)\n0.92 (0.33,2.54)\n0.873\n\n\n\nX...2.2\n1.29 (1.01,1.64)\n0.9 (0.47,1.72)\n0.743\n\n\n\nX...3.1\n1.4 (1.08,1.8)\n1.61 (0.86,3.03)\n0.139\n\n\n\nX.5"
  },
  {
    "objectID": "posts/FOXP1 Analysis/index.html",
    "href": "posts/FOXP1 Analysis/index.html",
    "title": "FOXP1 Analysis",
    "section": "",
    "text": "Just messing around with interactive plots.\nlibrary(pacman)\np_load(readr, tidyr, magrittr, knitr, tidyverse, janitor, broom, ggplot2, forcats, epitools, dplyr, xfun, plotly)\nThe data used for this tinkering is found on the National Library of Medicine ClinVar database.\n# load data\n# imported from directory\n# Helpful note to self: when loading csv files from \"copy path\" delete 'C:' and change all \\ to /\nCVFP1 &lt;- read_csv(\"/Users/mckjo/OneDrive/Desktop/GitBlog/posts/FOXP1 Analysis/ShinyClinVar.csv\")\n\n# clean variable names\njanitor::clean_names(CVFP1)\n\n# A tibble: 144 × 14\n      x1 link         gene_symbol type  consequence clinical_significance review\n   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;                 &lt;chr&gt; \n 1     1 \"&lt;a href=\\\"… FOXP1       SNV   Stop gain   Pathogenic            Crite…\n 2     2 \"&lt;a href=\\\"… FOXP1       SNV   Missense    Benign/Likely benign  Crite…\n 3     3 \"&lt;a href=\\\"… FOXP1       SNV   Intronic    Benign                Crite…\n 4     5 \"&lt;a href=\\\"… FOXP1       SNV   Intronic    Benign                Crite…\n 5     6 \"&lt;a href=\\\"… FOXP1       SNV   Missense    Likely benign         Crite…\n 6     7 \"&lt;a href=\\\"… FOXP1       SNV   Intronic    Benign                Crite…\n 7     8 \"&lt;a href=\\\"… FOXP1       SNV   Missense    Benign/Likely benign  Crite…\n 8    11 \"&lt;a href=\\\"… FOXP1       SNV   Missense    Pathogenic            No as…\n 9    12 \"&lt;a href=\\\"… FOXP1       SNV   Missense    Likely benign         Crite…\n10    13 \"&lt;a href=\\\"… FOXP1       SNV   Synonymous  Uncertain/conflicting Crite…\n# ℹ 134 more rows\n# ℹ 7 more variables: phenotype_list &lt;chr&gt;, name &lt;chr&gt;, ref_aa &lt;chr&gt;,\n#   alt_aa &lt;chr&gt;, pos_aa &lt;dbl&gt;, cadd_phred &lt;chr&gt;, gnom_ad_binary_char &lt;chr&gt;\n\n# number of rows (144)\nnrow(CVFP1)\n\n[1] 144\n\n# convert character variables to factors\nCVFP1 &lt;- CVFP1 %&gt;%\n  mutate_if(is.character, as.factor)\n\n# summary of dataset \nsummary(CVFP1)\n\n      ...1       \n Min.   :  1.00  \n 1st Qu.: 91.75  \n Median :136.50  \n Mean   :130.18  \n 3rd Qu.:186.25  \n Max.   :243.00  \n                 \n                                                                                           Link    \n &lt;a href=\"https://www.ncbi.nlm.nih.gov/clinvar/variation/1013484\" target=\"_blank\"&gt;1013484&lt;/a&gt;:  1  \n &lt;a href=\"https://www.ncbi.nlm.nih.gov/clinvar/variation/1018781\" target=\"_blank\"&gt;1018781&lt;/a&gt;:  1  \n &lt;a href=\"https://www.ncbi.nlm.nih.gov/clinvar/variation/1030051\" target=\"_blank\"&gt;1030051&lt;/a&gt;:  1  \n &lt;a href=\"https://www.ncbi.nlm.nih.gov/clinvar/variation/1030052\" target=\"_blank\"&gt;1030052&lt;/a&gt;:  1  \n &lt;a href=\"https://www.ncbi.nlm.nih.gov/clinvar/variation/1065605\" target=\"_blank\"&gt;1065605&lt;/a&gt;:  1  \n &lt;a href=\"https://www.ncbi.nlm.nih.gov/clinvar/variation/1098603\" target=\"_blank\"&gt;1098603&lt;/a&gt;:  1  \n (Other)                                                                                     :138  \n GeneSymbol   Type         consequence                   ClinicalSignificance\n FOXP1:144   SNV:144   3-UTR     : 1   Benign                      : 8       \n                       Intronic  :21   Benign/Likely benign        : 5       \n                       Missense  :71   Likely benign               :31       \n                       Splice-D/A:13   Likely pathogenic           :20       \n                       Stop gain :17   Pathogenic                  :33       \n                       Synonymous:21   Pathogenic/Likely pathogenic: 2       \n                                       Uncertain/conflicting       :45       \n                                                  review   \n Criteria provided/ conflicting interpretations      :  6  \n Criteria provided/ multiple submitters/ no conflicts: 16  \n Criteria provided/ single submitter                 :113  \n No assertion criteria provided                      :  9  \n                                                           \n                                                           \n                                                           \n                                                                                        PhenotypeList\n Not provided/ not specified                                                                   :68   \n Mental retardation with language impairment and with or without autistic features             :28   \n History of neurodevelopmental disorder                                                        :10   \n Intellectual disability                                                                       : 6   \n Inborn genetic diseases                                                                       : 5   \n Not provided/ not specified|History of neurodevelopmental disorder|Not provided/ not specified: 5   \n (Other)                                                                                       :22   \n                                            Name         ref_aa       alt_aa  \n NM_001244808.2(FOXP1):c.869+1G&gt;A             :  1   Arg    :16   Ter    :17  \n NM_001244810.1(FOXP1):c.1348+2T&gt;C            :  1   Ala    :14   Thr    :13  \n NM_001349338.3(FOXP1):c.1096A&gt;G (p.Met366Val):  1   Gln    : 9   Ala    : 9  \n NM_001349338.3(FOXP1):c.119A&gt;T (p.Glu40Val)  :  1   Thr    : 9   Ser    : 9  \n NM_001349338.3(FOXP1):c.1313G&gt;A (p.Arg438Gln):  1   Tyr    : 9   Gly    : 7  \n NM_001349338.3(FOXP1):c.1319C&gt;G (p.Ser440Ter):  1   (Other):52   (Other):54  \n (Other)                                      :138   NA's   :35   NA's   :35  \n     pos_aa         CADD_phred  gnomAD_binary_char\n Min.   :  3.0   0.070793:  1   No :83            \n 1st Qu.:192.0   0.204885:  1   Yes:61            \n Median :443.0   0.287364:  1                     \n Mean   :363.8   0.323585:  1                     \n 3rd Qu.:514.0   0.325591:  1                     \n Max.   :674.0   0.352630:  1                     \n NA's   :35      (Other) :138                     \n\n# removing the first two colummns   \nCVFP1 &lt;- CVFP1[, -c(1, 2)] \n\n# double checking the two columns were removed\nhead(CVFP1)\n\n# A tibble: 6 × 12\n  GeneSymbol Type  consequence ClinicalSignificance review   PhenotypeList Name \n  &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;fct&gt;    &lt;fct&gt;         &lt;fct&gt;\n1 FOXP1      SNV   Stop gain   Pathogenic           Criteri… Mental retar… NM_0…\n2 FOXP1      SNV   Missense    Benign/Likely benign Criteri… Aortic valve… NM_0…\n3 FOXP1      SNV   Intronic    Benign               Criteri… Not provided… NM_0…\n4 FOXP1      SNV   Intronic    Benign               Criteri… Not provided… NM_0…\n5 FOXP1      SNV   Missense    Likely benign        Criteri… Not provided… NM_0…\n6 FOXP1      SNV   Intronic    Benign               Criteri… Not provided… NM_0…\n# ℹ 5 more variables: ref_aa &lt;fct&gt;, alt_aa &lt;fct&gt;, pos_aa &lt;dbl&gt;,\n#   CADD_phred &lt;fct&gt;, gnomAD_binary_char &lt;fct&gt;\n\n# list of column names \n(column_names &lt;- colnames(CVFP1))\n\n [1] \"GeneSymbol\"           \"Type\"                 \"consequence\"         \n [4] \"ClinicalSignificance\" \"review\"               \"PhenotypeList\"       \n [7] \"Name\"                 \"ref_aa\"               \"alt_aa\"              \n[10] \"pos_aa\"               \"CADD_phred\"           \"gnomAD_binary_char\""
  },
  {
    "objectID": "posts/FOXP1 Analysis/index.html#static-data-visualizations",
    "href": "posts/FOXP1 Analysis/index.html#static-data-visualizations",
    "title": "FOXP1 Analysis",
    "section": "Static Data Visualizations",
    "text": "Static Data Visualizations\n\nBar Plots\n\nggplot(CVFP1, aes(x = ClinicalSignificance)) + geom_bar(color = \"black\", fill = \"lightgray\") + \n  labs(title = \"Clinical Significance of Genetic Variant from FOXP1\", \n       subtitle = \"n = 144\",\n       x = \"Level of Significance\", \n       y = \"Number of Individuals\") + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 25, hjust = 1))"
  },
  {
    "objectID": "posts/FOXP1 Analysis/index.html#interactive-data-visualizations",
    "href": "posts/FOXP1 Analysis/index.html#interactive-data-visualizations",
    "title": "FOXP1 Analysis",
    "section": "Interactive Data Visualizations",
    "text": "Interactive Data Visualizations\n\nBar Plots\nNote: As of August 3rd, 2023, I am unable to render using ‘plotly’. The code chunk below has came to a halt – I’ve troubleshooted by restarting, clearing the console and environment, checking for package conflicts, making sure everything is updated, removed my previous code with layout() and am still having this error:\n\nchecked again on August 21st\n\nQuitting from lines 68-138 [plotly] (index.qmd)\nError in add_html_caption():\n! unused argument (xfun::grep_sub(“1* &lt;[^&gt;]+aria-labelledby[ ]* =[ ] *  ”([^ “]+) ”.*$“,” \\1”, x))\nBacktrace:\n1. global .main()\n2. execute(…)\n3. rmarkdown::render(…)\n4. knitr::knit(knit_input, knit_output, envir = envir, quiet = quiet)\n5. knitr:::process_file(text, output)\n…\n14. sew(res, options)\n15. knitr:::sew.list(x, options, …)\n16. base::lapply(x, sew, options, …)\n17. FUN(X[[i]], …)\n18. knitr:::sew.knit_asis(x, options, …)\nExecution halted\n\n# Clinical Significance Bar Plot\n\n(df.bplot &lt;- CVFP1 %&gt;% dplyr::group_by(ClinicalSignificance) %&gt;% dplyr::count())\n\n# A tibble: 7 × 2\n# Groups:   ClinicalSignificance [7]\n  ClinicalSignificance             n\n  &lt;fct&gt;                        &lt;int&gt;\n1 Benign                           8\n2 Benign/Likely benign             5\n3 Likely benign                   31\n4 Likely pathogenic               20\n5 Pathogenic                      33\n6 Pathogenic/Likely pathogenic     2\n7 Uncertain/conflicting           45\n\ny.cs &lt;- c(8, 5, 31, 20, 33, 2, 45)\n\ntext.clin.sig &lt;- c(\"8\", \"5\", \"31\", \"20\", \"33\", \"2\", \"45\")\n\n## showing the counts of cases based on Clinical Significance \n# b.plot &lt;- plotly::plot_ly(data = df.bplot, \n#                          x = ~ClinicalSignificance, \n#                          y = ~y.cs, \n#                          type = \"bar\", \n#                          text = text.clin.sig, \n#                          marker = list(color = \"maroon\")) \n\n# b.plot\n\n## showing the percentage of case based on Clinical Significance \n\n### Benign  \nben &lt;- paste(round(8/144*100, digits = 2), \"%\", sep = \"\")\n\n### Benign/Likely Benign  \nben.lben &lt;- paste(round(5/144*100, digits = 2), \"%\", sep = \"\")\n\n### Likely Benign  \nlben &lt;- paste(round(31/144*100, digits = 2), \"%\", sep = \"\")\n\n### Likely Pathogenic  \nlpat &lt;- paste(round(20/144*100, digits = 2), \"%\", sep = \"\")\n\n### Pathogenic   \npat &lt;- paste(round(33/144*100, digits = 2), \"%\", sep = \"\")\n\n### Pathogenic/Likely Pathogenic  \npat.lpat &lt;- paste(round(2/144*100, digits = 2), \"%\", sep = \"\")\n\n### Uncertain/Conflicting \nunc.con &lt;- paste(round(45/144*100, digits = 2), \"%\", sep = \"\")\n\n\ntext.per &lt;- c(ben, ben.lben, lben, lpat, pat, pat.lpat, unc.con)\n\n# barplot with percentages \n# b.per.plot &lt;- plotly::plot_ly(data = df.bplot, \n#                          x = ~ClinicalSignificance, \n#                          y = ~y.cs, \n#                          type = \"bar\", \n#                          text = text.per, \n#                          marker = list(color = \"skyblue\")) \n\n# b.per.plot\n\n# Consequence Bar Plot \n\n(df.c.bplot &lt;- CVFP1 %&gt;% dplyr::group_by(consequence) %&gt;% dplyr::count())\n\n# A tibble: 6 × 2\n# Groups:   consequence [6]\n  consequence     n\n  &lt;fct&gt;       &lt;int&gt;\n1 3-UTR           1\n2 Intronic       21\n3 Missense       71\n4 Splice-D/A     13\n5 Stop gain      17\n6 Synonymous     21\n\n# showing the counts \ny.con &lt;- c(1, 21, 71, 13, 17, 21)\n\n# c.b.plot &lt;- plotly::plot_ly(data = df.c.bplot,\n#                          x = ~consequence,\n#                          y = ~y.con, \n#                          type = \"bar\", \n#                          marker = list(color = \"lavender\", \n#                                        line = list(color = \"darkgrey\", \n#                                                    width = 1.5))) \n\n# c.b.plot"
  },
  {
    "objectID": "posts/FOXP1 Analysis/index.html#interactive-plots-using-highcharter",
    "href": "posts/FOXP1 Analysis/index.html#interactive-plots-using-highcharter",
    "title": "FOXP1 Analysis",
    "section": "Interactive Plots Using ‘highcharter’",
    "text": "Interactive Plots Using ‘highcharter’\nNote: As of August 5th, plotly is still not running correctly so I’m going to try out this package highcharter\nMy ideas and inspiration came from this (site)[https://www.datanovia.com/en/lessons/highchart-interactive-bar-plot-in-r/]\n\nAugust 21st: same code warning with “highcharter” as it is with “plotly” - I figure at this point it’s good to show my flops as well as my successes, so enjoy this little flop\n\n\n# install.packages(\"highcharter\")   \nlibrary(highcharter) \n\n# setting highcharter options \noptions(highcharter.theme = hc_theme_smpl(tooltip = list(valueDecimals = 2)))\n\n\n# basic &lt;- df.c.bplot %&gt;% \n#  highcharter::hchart(\"column\", \n#         hcaes(\n#           x = consequence, \n#           y = y.con\n#         ), \n#         color = \"lavender\", \n#         borderColor = \"black\", \n#         pointWidth = 90)\n# basic"
  },
  {
    "objectID": "posts/FOXP1 Analysis/index.html#interactive-plots-using-gganimate",
    "href": "posts/FOXP1 Analysis/index.html#interactive-plots-using-gganimate",
    "title": "FOXP1 Analysis",
    "section": "Interactive Plots Using gganimate",
    "text": "Interactive Plots Using gganimate\nMy ideas and inspiration came from this (site)[https://towardsdatascience.com/create-animated-bar-charts-using-r-31d09e5841da]\n\n# install.packages(\"gganimate\")  \nlibrary(gganimate)\n\n\ncvfp1_formatted &lt;- CVFP1 %&gt;%\n  group_by(consequence) %&gt;%\n  # The * 1 makes it possible to have non-integer ranks while sliding\n  mutate(C.S.Pathogenic = (ClinicalSignificance==\"Pathogenic\")) \n\n\nstaticplot &lt;- ggplot(CVFP1, aes(x = ClinicalSignificance)) + geom_bar(color = \"black\", fill = \"lightgray\") + \n  labs(title = \"Clinical Significance of Genetic Variant from FOXP1\", \n       subtitle = \"n = 144\",\n       x = \"Level of Significance\", \n       y = \"Number of Individuals\") + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 25, hjust = 1))\n\n\nCoding a Plot.gif\n\naniplot &lt;- staticplot + \n  transition_states(ClinicalSignificance)\n\n### Note to self: \n\n# do not print/view 'aniplot' at this stage\n    # every static plot gets its own .png file (101 pngs here)\n\n# install.packages(\"gifski\")\nlibrary(gifski)\n\nanimate(aniplot, 200, fps = 20, width = 1200, height = 1000, \n        renderer = gifski_renderer(\"gganim.gif\"))"
  },
  {
    "objectID": "posts/FOXP1 Analysis/index.html#footnotes",
    "href": "posts/FOXP1 Analysis/index.html#footnotes",
    "title": "FOXP1 Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n^&lt;↩︎"
  },
  {
    "objectID": "posts/Associations/index.html",
    "href": "posts/Associations/index.html",
    "title": "Associations",
    "section": "",
    "text": "I mess around with correlations and associations here (eventually).\nI like to seek out random associations, correlations, assumed causations just for fun (e.g., is daily juice drinking associated with athlete’s foot outbreaks?). So this is just one of those that I am messing around with.\nThe data I’m using is from the SWAN study; I’m using the baseline dataset and visit 10 dataset.\n\nlibrary(pacman)\np_load(tidyr, magrittr, knitr, tidyverse, janitor, broom, infer, glue, ggplot2, ggfortify, forcats, epiR, epiDisplay, epitools, dplyr, DescTools, describedata, datawizard, corrplot, caret, car, nlme)\n\n\n# visit 1  \nload(\"C:/Users/mckjo/OneDrive/Desktop/GitBlog/posts/Associations/ICPSR_29221-V3/ICPSR_29221/DS0001/29221-0001-Data.rda\")\nfirstvisit &lt;- da29221.0001\n\n# visit 10\nload(\"C:/Users/mckjo/OneDrive/Desktop/GitBlog/posts/Associations/ICPSR_32961-V2 (2)/ICPSR_32961/DS0001/32961-0001-Data.rda\")\ntenthvisit &lt;- da32961.0001\n\n\nnames(firstvisit) %&gt;% head(n = 5)\n\n[1] \"SWANID\"   \"VISIT\"    \"INTDAY1\"  \"AGE1\"     \"LANGINT1\"\n\nnames(tenthvisit) %&gt;% head(n = 5)\n\n[1] \"SWANID\"    \"VISIT\"     \"AGE10\"     \"INTDAY10\"  \"LANGINT10\"\n\n\nBased off of what I read here under “Scope of Project”, the SWANID’s are linked across studies, so I will assume at this point in time that the SWANID’s from Visit 1 and Visit 10 are dependent."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collections",
    "section": "",
    "text": "Machine Learning Template\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\ntemplates\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nSaffron Evergreen\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithms Template\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\ntemplates\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nSaffron Evergreen\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Walk-Through\n\n\n\n\n\n\ncode\n\n\nvisualizations\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nAug 24, 2023\n\n\nSaffron Evergreen\n\n\n\n\n\n\n\n\n\n\n\n\nFOXP1 Analysis\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\nvisualizations\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nSaffron\n\n\n\n\n\n\n\n\n\n\n\n\nAssociations\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nSaffron\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Project Spring 2023\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nSaffron\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Project Winter 2022\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nSaffron\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey and Sampling Analysis Project Spring 2023\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nSaffron\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Post\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nSaffron\n\n\n\n\n\n\n\n\n\n\n\n\nPostpartum Research Proposal\n\n\n\n\n\n\nthesis\n\n\npostpartum\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nSaffron\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is under construction but is intended as a way to express what I’ve learned throughout my MPH Biostatistics program, as a student and individual who geeks out in their free time."
  },
  {
    "objectID": "posts/Algorithms Template/index.html",
    "href": "posts/Algorithms Template/index.html",
    "title": "Algorithms Template",
    "section": "",
    "text": "The brains behind this template goes to Jason Brownlee. Everything you find below is most definitely not my work; it is either Jason’s brains, the encyclopedia of notes I’ve accumulated from various courses or ChatGPT. I’m keeping this as a rough outline for coding and deciphering which algorithms to use in the future, which as I pick away at these, I can add my own code and thoughts but until then… nothing below is authentically mine and I take minimal credit."
  },
  {
    "objectID": "posts/Algorithms Template/index.html#supervised-learning",
    "href": "posts/Algorithms Template/index.html#supervised-learning",
    "title": "Algorithms Template",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nExample problems:\n* Classification\n* Regression\nExample algorithms:\n* Logistic regression\n* Back Propagation Neural Network"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#unsupervised-learning",
    "href": "posts/Algorithms Template/index.html#unsupervised-learning",
    "title": "Algorithms Template",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nExample problems:\n* Clustering\n* Dimensionality reduction\n* Association rule learning\nExample algorithms:\n* Apriori algorithm\n* K-Means"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#semi-supervised-learning",
    "href": "posts/Algorithms Template/index.html#semi-supervised-learning",
    "title": "Algorithms Template",
    "section": "Semi-Supervised Learning",
    "text": "Semi-Supervised Learning\nExample problems:\n* Classification\n* Regression\nExample algorithms:\n* flexible methods that make assumptions about how to model unlabeled data"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#regression-algorithms",
    "href": "posts/Algorithms Template/index.html#regression-algorithms",
    "title": "Algorithms Template",
    "section": "Regression Algorithms",
    "text": "Regression Algorithms\n\nmodeling the relationship between variables, iteratively refined using a measure of error in the predictions made by the model [1]\n\n\nOrdinary Least Squares Regression (OLSR)\n\npredicting y based on x [*]\n\ngoal is to minimize the sum of squared residuals, which means finding the line that best fits the data by minimizing the vertical distance between the data points and the line [*]\n\nbroader concept than linear regression; estimates the coefficients (the intercept and slope) in a linear regression model… linear regression encompasses the entire process of modelling the relationship between variables [*]\n\n\n### Generate example data\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\n\n# Create a data frame from the data\ndata &lt;- data.frame(x = x, y = y)\n\n\n### Use existing data\n# Fit the OLS regression model   \n\n### linear relationship\nmodel &lt;- lm(y ~ x, data = data)  \n\n### nonlinear relationships\nmodel &lt;- glm(y ~ x, \n             family = \"\", \n             data = \"\")\n\n# Print model summary\nsummary(model)\n\n\n\nLinear Regression\n\npredicting y based on x [*]\n\ngoal is to minimize the values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the sum of squared residuals, which is the differences between the observed and predicted values [*]\n\n\\[\ny = \\beta_0 ~+~ \\beta_1x ~+~ \\epsilon\n\\]\n\n### Generate example data where mean = 0 and sd = 1\nset.seed(123)\nx &lt;- seq(1, 10, by = 0.1)\ny &lt;- 2 * x + 3 + rnorm(length(x), mean = 0, sd = 1)\n\n# Create a data frame from the data\ndata &lt;- data.frame(x = x, y = y)\n\n### Use existing data\n# Fit the linear regression model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print model summary\nsummary(model)\n\n# Make predictions using the model\nnew_data &lt;- data.frame(x = c(11, 12, 13))\npredictions &lt;- predict(model, newdata = new_data)\n\n\n\nLogistic Regression\n\nfor binary/binomial outcome variable (0 or 1)\n\npredicts 0 or 1 for y based off of several x’s\n\n\n### Generate example data\nset.seed(123)\nn &lt;- 100\nx &lt;- rnorm(n)\ny &lt;- as.factor(ifelse(2 * x + rnorm(n) &gt; 0, 1, 0))\n\n# Create a data frame from the data\ndata &lt;- data.frame(x = x, y = y)\n\n### Use existing data\n# Fit the logistic regression model\nmodel &lt;- glm(y ~ x, \n             data = data, \n             family = \"binomial\")\n\n# Print model summary\nsummary(model)\n\n# Make predictions using the model\nnew_data &lt;- data.frame(x = c(0.5, 1.0, 1.5))\n\npredicted_probs &lt;- predict(model, \n                           newdata = new_data, \n                           type = \"response\") # p-values/probabilities\n\n# Print predicted probabilities\ncat(\"Predicted Probabilities:\", predicted_probs, \"\\n\")\n\n\n\nStepwise Regression\n\na process where predictor variables are added or removed from a regression model based on statistical criteria [*]\n\ncan be sensitive to the order of predictor variables [*]\n\nhas some drawbacks, should look more into this…\n\n\n### Generate example data\nset.seed(123)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\ny &lt;- 2 * x1 + 3 * x2 + rnorm(n)\n\n# Create a data frame from the data\ndata &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n### Use existing data \n# Fit the initial full model\nfull_model &lt;- lm(y ~ ., data = data)\n\n# Perform stepwise regression\nstepwise_model &lt;- step(full_model, \n                       direction = \"both\") # can add or remove predictor variables\n\n# Print the summary of the stepwise model\nsummary(stepwise_model)\n\n\n\nMultivariate Adaptive Regression Splines (MARS)\n\na flexible and powerful technique for capturing complex relationships between variables [*]\n\ncan be useful for capturing nonlinear relationships in your data [*]\n\nlook more into intrepretation and validation…\n\n\n# Install and load the necessary package\ninstall.packages(\"earth\")\nlibrary(earth)\n\n### Generate example data\nset.seed(123)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\ny &lt;- 2 * x1 + 3 * x2 + rnorm(n)\n\n# Create a data frame from the data\ndata &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n### Use existing data\n# Fit the MARS model\nmars_model &lt;- earth(y ~ x1 + x2, \n                    data = data)\n\n# Print the summary of the MARS model\nsummary(mars_model)\n\n\n\nLocally Estimated Scatterplot Smoothing (LOESS)\n\nnon-parametric tecnique used for fitting smooth curves to scatterplots [*]\n\nthe span parameter in the loess function controls the amount of smoothing; smaller span values result in more smoothing, larger span values result in less smoothing [*]\n\nLOESS can be influenced by the span parameter and is more computationally intensive than some other regression techniques [*]\n\nmore ideal for smaller datasets and when assessing EDA and visualization\n\n\n### Generate example data\nset.seed(123)\nx &lt;- seq(0, 2 * pi, length.out = 100)\ny &lt;- sin(x) + rnorm(100, mean = 0, sd = 0.2)\n\n### Use existing data \n# Fit the LOESS model\nloess_model &lt;- loess(y ~ x) # default: span = 0.75\n\n# Generate predicted values using the LOESS model\npredicted_values &lt;- predict(loess_model, \n                            data.frame(x = x))\n\n# Plot the original data and the LOESS curve\nplot(x, y, \n     main = \"LOESS Smoothing\")\n\nlines(x, \n      predicted_values, \n      col = \"red\", \n      lwd = 2)"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#instance-based-algorithms",
    "href": "posts/Algorithms Template/index.html#instance-based-algorithms",
    "title": "Algorithms Template",
    "section": "Instance-based Algorithms",
    "text": "Instance-based Algorithms\n\ninstances like building up a database of example data and compare new data to the example database [1]\n\nuses a similarity measure in order to find the best match and make a prediction [1]\n\nalso called memory-based learning [1]\n\nfocuses on the representation of stored instances and similarity measures used between instances [1]\n\n\n\nk-Nearest Neighbor (kNN)\n\nclass labels are determined based on a linear relationship between the predictor variables (x1 and x2) [*]\n\nusing the ‘class’ package, you can use kNN regression using the ‘knn.reg’ function [*]\n\na simple method\n\nnon-parametric\n\ncan handle numerical and categorical data; can capture non-linear relationships [*]\n\ncan be effective for detecting outliers and anomalies; also sensitive to outliers and scaling [*]\n\nnot ideal for large datasets\n\nlook more into finding the optimal k value…\n\nOne method: rando data, not from a dataset [*]\n\n# Install and load the necessary package\ninstall.packages(\"class\")\nlibrary(class)\n\n### Generate example data\nset.seed(123)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nclass_labels &lt;- factor(ifelse(2 * x1 + x2 + rnorm(n) &gt; 0, \"A\", \"B\"))\n\n# Create a data frame from the data\ndata &lt;- data.frame(x1 = x1, x2 = x2, class = class_labels)\n\n### Use existing data   \n# Split data into training and test sets\ntrain_indices &lt;- sample(n, n * 0.7)\ntrain_data &lt;- data[train_indices, ]\ntest_data &lt;- data[-train_indices, ]\n\n# Fit the kNN classifier\nk &lt;- 3  # Set the number of neighbors\nknn_model &lt;- knn(train_data[, c(\"x1\", \"x2\")], test_data[, c(\"x1\", \"x2\")], train_data$class, k)\n\n# Print the predicted class labels\ncat(\"Predicted Class Labels:\", knn_model, \"\\n\")\n\n# Calculate accuracy\naccuracy &lt;- sum(knn_model == test_data$class) / nrow(test_data)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\nAnother method: from a dataset [*]\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data &lt;- data[sample_indices, ]\n\ntest_data &lt;- data[-sample_indices, ]\n\n# Define the number of neighbors (k) for KNN\nk &lt;- 3  # You can choose an appropriate value for your problem\n\n# Train the KNN model\nknn_model &lt;- knn(train = \n                   train_data[, -target_column],  # Exclude target variable\n                 test = \n                   test_data[, -target_column],    # Exclude target variable\n                 cl = \n                   train_data$target_column,        # Target variable\n                 k = k)\n\n# Evaluate the model\n# Here, you can use metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\n\naccuracy &lt;- sum(knn_model == \n                  test_data$target_column) / \n                  length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\n\n\nLearning Vector Quantization (LVQ)\n\nsupervised ML algorithm used for classification\n\npurpose: classify input data into predefined categories or classes by adjusting a set of prototype vectors (reference points for each class)\n\nthe algorithm learns to adjust them during training to make accurate predictions\n\n\n\nlibrary(lvq)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data &lt;- data[sample_indices, ]\n\ntest_data &lt;- data[-sample_indices, ]\n\n# Define the number of prototypes per class and other hyperparameters\nnum_prototypes &lt;- 2  # Number of prototypes per class\nlearning_rate &lt;- 0.1  # Learning rate\nepochs &lt;- 100  # Number of training epochs\n\n# Extract the features (independent variables) and labels (target variable)\ntrain_features &lt;- train_data[, -target_column]  # Exclude target variable\ntrain_labels &lt;- train_data$target_column  # Target variable\n\n# Train the LVQ model\nlvq_model &lt;- lvq.train(train_features, \n                       train_labels, \n                       num_prototypes = num_prototypes, \n                       learning.rate = learning_rate, \n                       epochs = epochs)\n\n# Make predictions on the test data\ntest_features &lt;- test_data[, -target_column]  # Exclude target variable\npredictions &lt;- lvq.predict(lvq_model, test_features)\n\n# Evaluate the model\n# You can use metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\naccuracy &lt;- sum(predictions == test_data$target_column)/ \n                length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\n\n\nSelf-Organizing Map (SOM)\n\nunsupervised ML algorithm used for dimensionality reduction and visualization of high-dimensional data\n\npurpose: map complex, high-dimensional data onto a lower-dimensional grid in such a way that it preserves the topological properties and relationships of the original data\n\noften used for clustering, visualization, and data exploration\n\n\n\nlibrary(kohonen)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Normalize your data (if needed)\n# SOMs are sensitive to data scaling, so it's often a good idea to normalize your data\n# You can use functions like scale() or min-max normalization\n\n# Create a SOM grid\n# You need to specify the grid dimensions (e.g., grid rows and columns)\n# and other parameters like the learning rate and neighborhood function type\ngrid_rows &lt;- 5\ngrid_cols &lt;- 5\nsom_grid &lt;- somgrid(grid_rows, grid_cols, \"hexagonal\")\n\n# Train the SOM\nsom_model &lt;- som(data, grid = som_grid, rlen = 100, alpha = c(0.05, 0.01))\n\n# Plot the SOM\n# This step helps visualize the resulting SOM and cluster assignments\nplot(som_model)\n\n# You can also identify cluster assignments for your data points\ncluster_assignments &lt;- predict(som_model, newdata = data)\n\n# Explore and analyze the results further based on your problem's objectives\n\n\n\nLocally Weighted Learning (LWL)\n\ngives more weight to nearby data points and less weight to those farther away\n\npurpose: provide flexible and adaptive modeling, where the prediction for a new data point is based on the contributions of its neighboring data points\n\n\n\nlibrary(locfit)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data &lt;- data[sample_indices, ]\n\ntest_data &lt;- data[-sample_indices, ]\n\n# Define the bandwidth parameter for LWL\nbandwidth &lt;- 0.2  # You can choose an appropriate value for your problem\n\n# Train the LWL model\nlwl_model &lt;- locfit::locfit(target_variable ~ predictors, \n                            data = train_data, \n                            alpha = bandwidth)\n\n# Make predictions on the test data\npredictions &lt;- predict(lwl_model, \n                       newdata = data.frame(predictors = \n                                              test_data$predictors))\n\n# Evaluate the model\n# You can use appropriate evaluation metrics based on your problem\n# For regression, you might use mean squared error, and for classification, you might use accuracy.\n\n\n\nSupport Vector Machines (SVM)\n\nsupervised ML algorithm used for classification and regression\n\npurpose: to find a hyperplane (or decision boundary) that best separates different classes in a dataset\n\ngoal is to maximize the margin between the data points of different classes while minimizing classification errors\n\nuseful for high-dimensional data; is effective in handling complex classification problems\n\n\n\n\nlibrary(e1071)  # for SVM\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data &lt;- data[sample_indices, ]\n\ntest_data &lt;- data[-sample_indices, ]\n\n# Define the SVM model\n# Here, we'll use a linear kernel for simplicity\nsvm_model &lt;- svm(target_column ~ ., \n                 data = train_data, \n                 kernel = \"linear\")\n\n# Make predictions on the test data\npredictions &lt;- predict(svm_model, \n                       newdata = \n                         test_data[, -target_column])\n\n# Evaluate the model\n# Here, you can use metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\naccuracy &lt;- sum(predictions == test_data$target_column) / length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#regularization-algorithms",
    "href": "posts/Algorithms Template/index.html#regularization-algorithms",
    "title": "Algorithms Template",
    "section": "Regularization Algorithms",
    "text": "Regularization Algorithms\n\nan extension to another method (i.e., regression methods) [1]\n\npenalizes models based on complexity; favors simpler models that are more generalizable [1]\n\n\nRidge Regression\n\nused in linear regression to prevent overfitting and improve the models generalization by adding a “penalty term” to the regression equation\n\npurpose: to skrink the coefficients of the features towards zero while still maintaining all the geatures in the model\n\nhelps reduce multicollinearity\n\nleads to more stable and reliable predictions\n\n\n\nfor datasets where features are highly correlated\n\n\nlibrary(glmnet)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data &lt;- data[sample_indices, ]\n\ntest_data &lt;- data[-sample_indices, ]\n\n# Separate the target variable and predictors\ny_train &lt;- train_data$target_column\nX_train &lt;- train_data[, -target_column]\n\n# Fit a ridge regression model using glmnet\n# You can specify the lambda (penalty) parameter to control the strength of regularization\n# Smaller values of lambda result in stronger regularization\n# cv.glmnet performs cross-validation to select an optimal lambda value\nridge_model &lt;- cv.glmnet(x = as.matrix(X_train), \n                         y = y_train, \n                         alpha = 0, \n                         nfolds = 10)\n\n# Print the optimal lambda value selected by cross-validation\nbest_lambda &lt;- ridge_model$lambda.min\ncat(\"Optimal Lambda:\", best_lambda, \"\\n\")\n\n# Fit the final ridge regression model with the optimal lambda\nfinal_ridge_model &lt;- glmnet(x = as.matrix(X_train), \n                            y = y_train, \n                            alpha = 0, \n                            lambda = best_lambda)\n\n# Make predictions on the test set\nX_test &lt;- test_data[, -target_column]\npredictions &lt;- predict(final_ridge_model, \n                       newx = as.matrix(X_test))\n\n# Evaluate the model's performance\n# You can use appropriate regression metrics like RMSE, R-squared, etc., depending on your problem\n# For simplicity, we'll use Mean Squared Error (MSE) as an example\nmse &lt;- mean((predictions - test_data$target_column)^2)\ncat(\"Mean Squared Error:\", mse, \"\\n\")\n\n\n\nLeast Absolute Shrinkage and Selection Operator (LASSO)\n\nused to prevent overfitting and perform feature selection by adding a penalty term to the linear regression cost function\n\nencourages sparse models by pushing some of the coefficient values to exactly 0, which effectively removes those features from the model\n\nit balances a trade-off between model complexity and goodness-of-fit\n\n\n\nlibrary(glmnet)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into predictors (X) and target variable (y)\nX &lt;- data[, -target_column]  # Exclude the target variable\ny &lt;- data$target_column      # Target variable\n\n# Standardize predictors (recommended for LASSO)\nX &lt;- scale(X)\n\n# Set up the lambda values (penalty parameter)\n# You can use cross-validation to determine the optimal lambda value\nlambda_values &lt;- 10^seq(10, -2, length = 100)\n\n# Fit LASSO regression model\nlasso_model &lt;- glmnet(X, \n                      y, \n                      alpha = 1, \n                      lambda = lambda_values)\n\n# Plot the LASSO coefficient paths (optional)\nplot(lasso_model)\n\n# Select the best lambda value using cross-validation (optional)\ncv_model &lt;- cv.glmnet(X, \n                      y, \n                      alpha = 1)\nbest_lambda &lt;- cv_model$lambda.min\ncat(\"Best Lambda:\", best_lambda, \"\\n\")\n\n# Fit the LASSO model with the best lambda\nlasso_model_best_lambda &lt;- glmnet(X, \n                                  y, \n                                  alpha = 1, \n                                  lambda = best_lambda)\n\n# Print the coefficients of the LASSO model\ncoef(lasso_model_best_lambda)\n\n\n\nElastic Net\n\ncombines L1 (LASSO) and L2 (ridge regression) methods\n\npurpose: to overcome some of the limitations of each of those two methods by adding a regularization term that is a linear combination of L1 and L2 regularization penalties\n\nthere are two hyperparameters, alpha and lambda, that control the balance\n\nwhen alpha is 0, it’s equal to ridge regression and when alpha is 1, it’s equal to LASSO\n\n\n\nlibrary(glmnet)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into predictors (X) and the target variable (Y)\nX &lt;- data[, -target_column]  # Exclude target variable\nY &lt;- data$target_column\n\n# Define the alpha and lambda values\nalpha &lt;- 0.5  # You can choose a value between 0 (Ridge) and 1 (Lasso)\nlambda &lt;- 0.01  # Regularization strength parameter\n\n# Create an Elastic Net model\nelastic_net_model &lt;- glmnet(X, \n                            Y, \n                            alpha = alpha, \n                            lambda = lambda)\n\n# Make predictions using the model\n# You can replace 'new_data' with your test data if needed\npredicted_values &lt;- predict(elastic_net_model, \n                            new_data, \n                            s = lambda)\n\n# Optionally, you can also plot the regularization path\nplot(elastic_net_model)\n\n\n\nLeast-Angle Regression (LARS)\n\nused in linear regression\n\npurpose: to produce a sparse model by gradually adding predictors (X’s) to the model while controlling the size of the coefficients\n\nideal for high-dimensional data where you want to identify the most relevant features while avoiding overfitting\n\n\n\nlibrary(lars)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Separate predictors (features) and the target variable\nX &lt;- data[, -target_column]  # Exclude the target variable\ny &lt;- data$target_column      # Target variable\n\n# Fit the LARS model\nlars_model &lt;- lars(X, \n                   y, \n                   type = \"lasso\")  # LARS with L1 (Lasso) regularization\n\n# Print the model summary\nprint(summary(lars_model))\n\n# Get the selected features (variables)\nselected_features &lt;- names(X)[which(lars_model$active != 0)]\ncat(\"Selected Features:\", selected_features, \"\\n\")"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#decision-tree-algorithms",
    "href": "posts/Algorithms Template/index.html#decision-tree-algorithms",
    "title": "Algorithms Template",
    "section": "Decision Tree Algorithms",
    "text": "Decision Tree Algorithms\n\ntypically for classification and regression problems [1]\n\noften fast, accurate and tres popular\nconstruct a model of decisions made based on actual values in the data [1]\n\ncreate tree structures until a prediction decision is made for a given record [1]\n\n\nClassification and Regression Tree (CART)\n\nused both classification and regression\n\npurpose: to create a tree-like model that can make predictions by recursively splitting the data into subsets based on the values of input features\n\ncommonly used for predicting categories (classification) or predicting numeric values (regression) based on input features\n\n\nlibrary(rpart)  # for CART algorithm\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\n\nset.seed(123)  # for reproducibility\n\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\n\ntrain_data &lt;- data[sample_indices, ]\n\ntest_data &lt;- data[-sample_indices, ]\n\n# Define the CART model\n# For classification tasks, specify the target variable as a factor\n# For regression tasks, specify the target variable as numeric\n# Replace 'target_column' with the actual name of your target variable\ncart_model &lt;- rpart(target_column ~ ., data = train_data, method = \"class\")\n\n# Make predictions using the CART model\n# For classification tasks, replace 'new_data' with your test data and specify 'type = \"class\"'\n# For regression tasks, replace 'new_data' with your test data and specify 'type = \"vector\"'\npredictions &lt;- predict(cart_model, newdata = test_data, type = \"class\")\n\n# Evaluate the model\n# Here, you can use metrics like accuracy (for classification) or RMSE (for regression), depending on your problem\n# For simplicity, we'll use accuracy as an example for classification\naccuracy &lt;- sum(predictions == test_data$target_column) / length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\n\n\nIterative Dichotomiser 3 (ID3)\n\nused for building decision trees and data mining\n\npurpose: to classify or predict a target variable based on a set of input features\n\nrecursively selects the best attribute to split the data into subsets that are as pure as possible in terms of the target variable\n\n\nan old algorithm, doesn’t exist in R’s core libraries\n\ncan create an algorithm similar to ID3 using recursive functions in R\n\n\nlibrary(dplyr)\n\n# Define a recursive function to build the ID3 decision tree\nid3 &lt;- function(data, target_attr, attributes) {\n  # Create a new node for the decision tree\n  node &lt;- list()\n  \n  # If all instances have the same class label, return that label as a leaf node\n  if (length(unique(data[[target_attr]])) == 1) {\n    node$leaf &lt;- TRUE\n    node$class &lt;- unique(data[[target_attr]])[1]\n    return(node)\n  }\n  \n  # If there are no more attributes to split on, return the majority class as a leaf node\n  if (length(attributes) == 0) {\n    node$leaf &lt;- TRUE\n    node$class &lt;- names(sort(table(data[[target_attr]]), decreasing = TRUE))[1]\n    return(node)\n  }\n  \n  # Select the best attribute to split on based on information gain or entropy\n  best_attr &lt;- select_best_attribute(data, target_attr, attributes)\n  \n  # Set the current node's attribute to the best attribute\n  node$attribute &lt;- best_attr\n  \n  # Create child nodes for each value of the best attribute\n  node$children &lt;- list()\n  for (value in unique(data[[best_attr]])) {\n    # Create a subset of the data where the best attribute has the specified value\n    subset_data &lt;- data %&gt;%\n      filter(data[[best_attr]] == value)\n    \n    # Recursively build the tree for the subset\n    child &lt;- id3(subset_data, target_attr, setdiff(attributes, best_attr))\n    \n    # Add the child node to the current node\n    node$children[[as.character(value)]] &lt;- child\n  }\n  \n  return(node)\n}\n\n# Define a function to select the best attribute based on information gain or entropy\nselect_best_attribute &lt;- function(data, target_attr, attributes) {\n  # Implement your method for attribute selection here (e.g., using information gain or entropy)\n  # This function should return the name of the best attribute to split on\n  # You can replace this with your specific attribute selection logic\n  # Example:\n  # return(attributes[1])\n}\n\n# Example usage\n# Replace 'your_dataset.csv' with the actual file path or URL to your dataset\n# Replace 'target_attribute' with the name of your target attribute\ndata &lt;- read.csv(\"your_dataset.csv\")\ntarget_attribute &lt;- \"target_attribute\"\n\n# Get a list of all attributes except the target attribute\nall_attributes &lt;- setdiff(names(data), target_attribute)\n\n# Build the ID3 decision tree\ndecision_tree &lt;- id3(data, target_attribute, all_attributes)\n\n# Print the decision tree or use it for predictions\nprint(decision_tree)\n\n\n\nIterative C4.5 and C5.0\n\niterative enhancements which are used in ML and data classification\n\npurpose: to improve the decision tree’s accuracy and generalization by iteratively refining the tree through processes like pruning and re-splitting nodes\n\n\nlibrary(C50)  # for C5.0 algorithm\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Split the dataset into training and testing sets\n# You can use any method you prefer for data splitting\n# Here's a simple random split into 80% training and 20% testing\nset.seed(123)  # for reproducibility\nsample_indices &lt;- sample(nrow(data), size = 0.8 * nrow(data))\ntrain_data &lt;- data[sample_indices, ]\ntest_data &lt;- data[-sample_indices, ]\n\n# Train the C5.0 model\nc50_model &lt;- C5.0(train_data[, -target_column], train_data$target_column)\n\n# Predict using the trained model\npredictions &lt;- predict(c50_model, test_data)\n\n# Evaluate the model\n# You can use various metrics like accuracy, precision, recall, etc., depending on your problem\n# For simplicity, we'll use accuracy as an example\naccuracy &lt;- sum(predictions == test_data$target_column) / length(test_data$target_column)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\n\n\nChi-squared Automatic Interaction Detection (CHAID)\n\niterative decision tree for predictive modeling and classification\n\npurpose: create a decision tree that recursively splits the data into homogenous groups based on the most significant categorical predictor variables (X’s)\n\nuseful for analyzing categorical data\n\nidentifies relationships between categorical predictor variables and a categorical target variable by performing chi-squared tests for independence to find the most influential predictors\n\n\n\nlibrary(partykit)\n\n# Load your dataset\ndata &lt;- read.csv(\"your_dataset.csv\")\n\n# Define your target variable and predictor variables\n# Replace 'target_column' and 'predictor_columns' with the actual column names\ntarget_column &lt;- \"target_variable\"\npredictor_columns &lt;- c(\"predictor_1\", \"predictor_2\", \"predictor_3\")\n\n# Create a formula for the CHAID model\nformula &lt;- as.formula(paste(target_column, \"~\", paste(predictor_columns, collapse = \"+\")))\n\n# Build the CHAID decision tree model\nchaide_tree &lt;- chaid(formula, data = data)\n\n# Visualize the CHAID decision tree\nplot(chaide_tree)\n\n# Summary of the tree\nprint(chaide_tree)\n\n# Predict using the CHAID decision tree model\n# Replace 'new_data' with the data you want to make predictions on\nnew_data &lt;- data.frame(predictor_1 = c(...), predictor_2 = c(...), predictor_3 = c(...))\npredictions &lt;- predict(chaide_tree, newdata = new_data)\n\n# Evaluate the model and make further analyses as needed\n\n\n\nDecision Stump\n\n\nM5\n\n\nConditional Decision Trees"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#bayesian-algorithms",
    "href": "posts/Algorithms Template/index.html#bayesian-algorithms",
    "title": "Algorithms Template",
    "section": "Bayesian Algorithms",
    "text": "Bayesian Algorithms\n\nconditional probability for two events (A|B)\n\n\\[\nP(A \\mid B) = \\frac{P(B \\cap A)}{P(B)}\n\\]\n\\[\n{P(B \\cap A)} = {P(B|A)}{P(A)}\n\\]\n\nthe formula above expresses that the probability that B occurs and causes A is equivalent to the probability of B and A occurring, divided by the probability of B occurring.\nexplictly apply Bayes’ Theorem for classification and regression probems [1]\n\n\n\n\nNaive Bayes\n\n\nGaussian Naive Bayes\n\n\nMultinomial Naive Bayes\n\n\nAveraged One-Dependence Estimators (AODE)\n\n\nBayesian Belief Network (BBN)\n\n\nBayesian Network (BN)"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#clustering-algorithms",
    "href": "posts/Algorithms Template/index.html#clustering-algorithms",
    "title": "Algorithms Template",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\n\ndescribes the class of problem and class of methods [1]\n\norganized by modeling approaches such as ‘centroid-based’ and ‘hierarchal’ [1]\n\nall methods use structures in the data to best organize the data into groups of maximum commonality [1]\n\n\nk-Means\n\n\nk-Medians\n\n\nExpectation Maximization (EM)\n\n\nHierarchical Clustering"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#association-rule-learning-algorithms",
    "href": "posts/Algorithms Template/index.html#association-rule-learning-algorithms",
    "title": "Algorithms Template",
    "section": "Association Rule Learning Algorithms",
    "text": "Association Rule Learning Algorithms\n\nextract rules that best explain observed relationshipns between variables in the dataset [1]\n\ncan discover useful associations in multidimensional datasets [1]\n\n\nApriori algorithm\n\n\nEclat algorithm"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#artifical-neural-network-algorithms",
    "href": "posts/Algorithms Template/index.html#artifical-neural-network-algorithms",
    "title": "Algorithms Template",
    "section": "Artifical Neural Network Algorithms",
    "text": "Artifical Neural Network Algorithms\n\ninspired by the structure and/or function of biological neural networks [1]\n\nclass of pattern matching used in regression and classification problems [1]\n\nsubfield of hundreds of algorithms and variations for types of problems [1]\n\n\nPerceptron\n\n\nMultilayer Perceptrons (MLP)\n\n\nBack-Propagation\n\n\nStochastic Gradient Descent\n\n\nHopfield Network\n\n\nRadial Basis Function Network (RBFN)"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#deep-learning-algorithms",
    "href": "posts/Algorithms Template/index.html#deep-learning-algorithms",
    "title": "Algorithms Template",
    "section": "Deep Learning Algorithms",
    "text": "Deep Learning Algorithms\n\na modern update to ‘Artificial Neural Networks’ that exploit abundant cheap computation [1]\n\nmuch larger and more complex neural networks [1]\n\nuse very large datasets of labelled analog data such as image, text, audio and video [1]\n\n\nConvolutional Neural Network (CNN)\n\n\nRecurrent Neural Networks (RNNs)\n\n\nLong Short-Term Memory Networks (LSTMs)\n\n\nStacked Auto-Encoders\n\n\nDeep Boltzmann Machine (DBM)\n\n\nDeep Belief Networks (DBN)"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#dimensionality-reduction-algorithms",
    "href": "posts/Algorithms Template/index.html#dimensionality-reduction-algorithms",
    "title": "Algorithms Template",
    "section": "Dimensionality Reduction Algorithms",
    "text": "Dimensionality Reduction Algorithms\n\nsimilar to clustering methods; this seeks and exploits the structure in the data - can be unsupervised or order to summarize or describe data using less information [1]\n\ncan be used to visualize dimensional data or simplify data that can be used in a supervised learning method [1]\n\ncan be used in classification and regression problems [1]\n\n\nPrincipal Component Analysis (PCA)\n\n\nPrincipal Component Regression (PCR)\n\n\nPartial Least Squares Regression (PLSR)\n\n\nSammon Mapping\n\n\nMultidimensional Scaling (MDS)\n\n\nProjection Pursuit\n\n\nLinear Discriminant Analysis (LDA)\n\n\nMixture Discriminant Analysis (MDA)\n\n\nQuadratic Discriminant Analysis (QDA)\n\n\nFlexible Discriminant Analysis (FDA)"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#ensemble-algorithms",
    "href": "posts/Algorithms Template/index.html#ensemble-algorithms",
    "title": "Algorithms Template",
    "section": "Ensemble Algorithms",
    "text": "Ensemble Algorithms\n\nmodels composed of several weaker models that are independently trained and whose predictions are combined to make an overall prediction [1]\n\npowerful and popular\n\n\nBoosting\n\n\nBootstrapped Aggregation (Bagging)\n\n\nAdaBoost\n\n\nWeighted Average (Blending)\n\n\nStacked Generalization (stacking)\n\n\nGradient Boosting Machines (GBM)\n\n\nGradient Boosted Regression Trees (GBRT)\n\n\nRandom Forest"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#other-machine-learning-algorithms",
    "href": "posts/Algorithms Template/index.html#other-machine-learning-algorithms",
    "title": "Algorithms Template",
    "section": "Other Machine Learning Algorithms",
    "text": "Other Machine Learning Algorithms\n\nFeature selection algorithms\n\n\nAlgorithm accuracy evaluation\n\n\nPerformance measures\n\n\nOptimization algorithms"
  },
  {
    "objectID": "posts/Algorithms Template/index.html#other-algorithms-for-specialty-subfields",
    "href": "posts/Algorithms Template/index.html#other-algorithms-for-specialty-subfields",
    "title": "Algorithms Template",
    "section": "Other Algorithms for Specialty Subfields",
    "text": "Other Algorithms for Specialty Subfields\n\nComputational intelligence (evolutionary algorithms)\n\n\nComputer Vision (CV)\n\n\nNatural Language Processing (NLP)\n\n\nRecommender Systems\n\n\nReinforcement Learning\n\n\nGraphical Models"
  },
  {
    "objectID": "posts/First Blog Post/index.html",
    "href": "posts/First Blog Post/index.html",
    "title": "First Post",
    "section": "",
    "text": "This is my first attempt at posting a blog using Quarto!\nI’ve built two blogs in the past using the Blogdown-Hugo-GitHub-Netlify pipeline but those are on a laptop which has now passed over and the URL’s are nowhere to be found. So my goal here is to lay out all the fun I’ve had and the major learning curves I’ve pushed through these last few years.\nI have way too many interests and passions related and unrelated to data science so my goal is to touch on many different topics that I’ve covered in my classes, as well as what I’ve worked on outside of graduate courses.\nResume Link\nA new tool that I’ve been lightly tinkering with is typst.app which is what I’ve been using to build my resume and cover letters. Here is the link for my resume.\n\n# This is for me to edit my resume \n# https://typst.app/project/wfidmzdG33SwaQ2Adu5pNG  \n\n# This is for me to edit my cover letter \n# https://typst.app/project/w14mwPCsdNobGkuX_i_knB \n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html",
    "href": "posts/Logistic Regression Project 22/index.html",
    "title": "Logistic Regression Project Winter 2022",
    "section": "",
    "text": "A fun little group project assessing CRP values and stress."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#packages",
    "href": "posts/Logistic Regression Project 22/index.html#packages",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(ggplot2)\nlibrary(lattice)\nlibrary(psych)\nlibrary(describedata) \nlibrary(ggfortify)\nlibrary(plotly)\nlibrary(GGally) \nlibrary(dplyr)\nlibrary(rsq)\nlibrary(gtsummary)\nlibrary(tinytex)\nlibrary(car)  # vif(), car::Anova()\nlibrary(cowplot)\nlibrary(moderndive)\nlibrary(forcats)\nlibrary(leaps)\nlibrary(ggridges)"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#loading-data",
    "href": "posts/Logistic Regression Project 22/index.html#loading-data",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Loading Data",
    "text": "Loading Data\n\n# baseline data\nload(\"C:/Users/mckjo/OneDrive/Desktop/GitBlog/posts/Logistic Regression Project 22/ICPSR_04368/ICPSR_04368/DS0001/04368-0001-Data.rda\")\n\n# cross-sectional data \nload(\"C:/Users/mckjo/OneDrive/Desktop/GitBlog/posts/Logistic Regression Project 22/ICPSR_28762/ICPSR_28762/DS0001/28762-0001-Data.rda\")"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#merging-datasets-renaming-variables",
    "href": "posts/Logistic Regression Project 22/index.html#merging-datasets-renaming-variables",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Merging Datasets, Renaming Variables",
    "text": "Merging Datasets, Renaming Variables"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#primary-predictor-as-binary",
    "href": "posts/Logistic Regression Project 22/index.html#primary-predictor-as-binary",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Primary Predictor as Binary",
    "text": "Primary Predictor as Binary"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#baseline-model",
    "href": "posts/Logistic Regression Project 22/index.html#baseline-model",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Baseline Model",
    "text": "Baseline Model\nRegression Formula\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\textrm{Financial Strain} + \\beta_2 \\textrm{Race/Ethnicity} + \\beta_3  \\textrm{Education}+ \\beta_4 \\textrm{Marital Status} + \\\\ \\beta_5 \\textrm{Smoking Status} + \\beta_6 \\textrm{Age} + \\beta_7 \\textrm{BMI} + \\beta_8 \\textrm{SBP} + \\beta_9 \\textrm{Physical Activity} + \\\\ \\beta_{10} \\textrm{Difficulty Sleeping} + \\beta_{11} \\textrm{Health Rank} + \\epsilon\\]\nBaseline Hypothesis\n\\[H_0: \\beta_{1}=\\beta_{2}=.....\\beta_{11}=0 \\quad \\text{vs.} \\quad H_A: \\text{At least one of } \\beta_{j} \\neq 0\\]\n\nWhat is blocked out: We had ran a model using lm() function and created a clean table assessing the coefficient estimates."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#assessing-outliers-cooks-distance",
    "href": "posts/Logistic Regression Project 22/index.html#assessing-outliers-cooks-distance",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Assessing Outliers: Cook’s Distance",
    "text": "Assessing Outliers: Cook’s Distance\n\nWhat is blocked out: we performed a visual assessment to show the influential outliers using cook’s distance with the cut-off point of \\(4 \\div 2928\\) along the slope and removed the rows which had those outliers.\n\n\nBaseline Model: Reassessment\n\nWhat is blocked out: we then assessed linearity of our baseline model with the influential points removed. The assumptions for linearity were most definitely still not met at this point.\nAssumptions for linearity:\n* Linearity: nope\n* Independence: yep\n* Homoscedasticity: definitely not\n* Normality: not quite"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#descriptive-statistics-table",
    "href": "posts/Logistic Regression Project 22/index.html#descriptive-statistics-table",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Descriptive Statistics Table",
    "text": "Descriptive Statistics Table\n\nWhat is blocked out: a bunch of code converting the covariates to factored variables.\n\n\nTable 1\n\nNote: I was emotionally attached to this table since this was one of the learning curves I endured combining various gt libraries and functions so I’m including it since this was my baby at the time.\n\n\n# list of all variables for easy input \n\n# Financial_Strain, Race, Education, Marital_Status, Smoke, Age, BMI, SBP, Physical_Activity, HealthRank, Difficulty_Sleeping\n\nswan_fct %&gt;% tbl_summary(\n  by = Financial_Strain, # stratifying by this binary variable (primary predictor)\n  label = list(                       # row name appearance\n    Financial_Strain ~ \"Financial Strain\",\n    Race ~ \"Race/Ethnicity\",\n    Marital_Status ~ \"Marital Status\", \n    Smoke ~ \"Smoking Status\", \n    Age ~ \"Age\",\n    BMI ~ \"Body Mass Index (BMI)\",\n    SBP ~ \"Systolic Blood Pressure (SBP)\",\n    Physical_Activity ~ \"Physical Activity\", \n    HealthRank ~ \"Health Status\", \n    Difficulty_Sleeping ~ \"Difficulty Sleeping\"\n  ),\n  include = c(     # which variables from dataset to include in table \n    Financial_Strain, Age, Race, Education, Marital_Status, SBP, Smoke, BMI, Physical_Activity, HealthRank, Difficulty_Sleeping)) %&gt;% \n  #add_overall() %&gt;% ## this didn't run for us\n  modify_header(label ~ \"Variable\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"Median (IQR); Frequency (%)\") %&gt;%\n  modify_caption(\"Table 1.\") %&gt;%\n bold_labels() %&gt;% \n## to use gt() functions, add `as_gt()` then include gt() functions\n as_gt() %&gt;% \n  tab_header(\n    title = \"Participants' characteristics, stratified by Financial Strain\",\n    subtitle = \"Financial Strain: How hard is it to afford basic living expenses?\") %&gt;% \n  tab_options(heading.title.font.size = \"small\",\n              heading.title.font.weight = \"bold\",\n              heading.subtitle.font.size = \"large\",\n              heading.subtitle.font.weight = \"80\",\n              heading.align = \"right\") %&gt;% \n  opt_table_outline(style = \"solid\", width = px(5)) %&gt;% \n  opt_stylize(style = 6, color = \"gray\")\n\n\n\n\n\n\nTable 1.\n\n\nParticipants' characteristics, stratified by Financial Strain\n\n\nFinancial Strain: How hard is it to afford basic living expenses?\n\n\nVariable\nSomewhat/Very hard, N = 1,9671\nNot hard, N = 1,3101\n\n\n\n\nAge\n46.00 (44.00, 48.00)\n45.00 (43.00, 48.00)\n\n\nRace/Ethnicity\n\n\n\n\n    Black/African American\n497 (25%)\n431 (33%)\n\n\n    Chinese/Chinese American\n180 (9.2%)\n70 (5.3%)\n\n\n    Japanese/Japanese American\n196 (10.0%)\n84 (6.4%)\n\n\n    Caucasian/White Non-Hispanic\n1,049 (53%)\n495 (38%)\n\n\n    Hispanic\n45 (2.3%)\n230 (18%)\n\n\nEducation\n\n\n\n\n    Less than high school\n45 (2.3%)\n191 (15%)\n\n\n    High school graduate\n278 (14%)\n300 (23%)\n\n\n    Some college/technical school\n594 (30%)\n454 (35%)\n\n\n    College graduate\n448 (23%)\n211 (16%)\n\n\n    Post graduate education\n594 (30%)\n143 (11%)\n\n\n    Unknown\n8\n11\n\n\nMarital Status\n\n\n\n\n    Single, never married\n259 (13%)\n178 (14%)\n\n\n    Currently married/living as married\n1,399 (72%)\n742 (58%)\n\n\n    Separated or divorced\n265 (14%)\n327 (25%)\n\n\n    Widowed\n24 (1.2%)\n41 (3.2%)\n\n\n    Unknown\n20\n22\n\n\nSystolic Blood Pressure (SBP)\n112 (104, 123)\n118 (108, 128)\n\n\n    Unknown\n7\n12\n\n\nSmoking Status\n\n\n\n\n    Never smoked\n1,152 (60%)\n713 (56%)\n\n\n    Former smoker\n525 (27%)\n268 (21%)\n\n\n    Current smoker\n259 (13%)\n298 (23%)\n\n\n    Unknown\n31\n31\n\n\nBody Mass Index (BMI)\n25 (22, 29)\n27 (23, 32)\n\n\n    Unknown\n46\n94\n\n\nPhysical Activity\n\n\n\n\n    Much less than other women your age\n60 (3.1%)\n87 (7.0%)\n\n\n    Somewhat less than other women your age\n263 (14%)\n228 (18%)\n\n\n    About the same as other women your age\n742 (39%)\n525 (42%)\n\n\n    Somewhat more than other women your age\n566 (30%)\n259 (21%)\n\n\n    Much more than other women your age\n283 (15%)\n141 (11%)\n\n\n    Unknown\n53\n70\n\n\nHealth Status\n\n\n\n\n    Excellent\n517 (27%)\n175 (14%)\n\n\n    Very good\n790 (41%)\n382 (30%)\n\n\n    Good\n481 (25%)\n464 (36%)\n\n\n    Fair\n143 (7.3%)\n223 (17%)\n\n\n    Poor\n15 (0.8%)\n46 (3.6%)\n\n\n    Unknown\n21\n20\n\n\nDifficulty Sleeping\n\n\n\n\n    None\n1,194 (77%)\n664 (73%)\n\n\n    Yes\n362 (23%)\n245 (27%)\n\n\n    Unknown\n411\n401\n\n\n\n1 Median (IQR); Frequency (%)"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#distribution-of-crp",
    "href": "posts/Logistic Regression Project 22/index.html#distribution-of-crp",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Distribution of CRP",
    "text": "Distribution of CRP"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#boxplots",
    "href": "posts/Logistic Regression Project 22/index.html#boxplots",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Boxplots",
    "text": "Boxplots"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#density-plots",
    "href": "posts/Logistic Regression Project 22/index.html#density-plots",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Density plots",
    "text": "Density plots\n\nCheck for multicollinearity\n\n\n# any VIF value &gt; 5 suggests multicollinearity\ncar::vif(baseline_model)\n\n   Financial_Strain                Race           Education      Marital_Status \n           1.180696            1.088712            1.195049            1.028614 \n              Smoke                 Age                 BMI                 SBP \n           1.028287            1.034975            1.292750            1.210437 \n  Physical_Activity          HealthRank Difficulty_Sleeping \n           1.259768            1.392629            1.070196"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#forward-elimination-procedure",
    "href": "posts/Logistic Regression Project 22/index.html#forward-elimination-procedure",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Forward Elimination Procedure",
    "text": "Forward Elimination Procedure\n\nNote: The code below (with the exception of the covariates added) is solely a gift from my previous professor. \n\n# another method for determining parsimonious model\nmodel &lt;- regsubsets(CRP ~ \n              Financial_Strain + \n              Race + Education + \n              Marital_Status + \n              Smoke + Age + BMI + \n              SBP + \n              Physical_Activity + \n              HealthRank + \n              Difficulty_Sleeping, \n                         data = SWANish, \n                         nvmax = 13,\n                    method=\"forward\")\n\np.val &lt;- 0; i &lt;- 1; vars_last &lt;- NULL\n\n#----------------------------\n# do not edit code within the \"while\" loop\"\nwhile(p.val &lt;= 0.1){\n  vars_current &lt;- names(coef(model,i))[-1]\n  var_add &lt;- vars_current[!vars_current %in% vars_last]\n  coef &lt;- summary(lm(as.formula(paste(\"CRP ~ \", paste(vars_current, collapse =\" + \"), sep = \"\")),  \n                  data = SWANish))$coefficients\n  \n  p.val &lt;- coef[var_add,\"Pr(&gt;|t|)\"]\n  vars_last &lt;- vars_current\n  i=i + 1\n  print(var_add) # print the variable being added \n  print(coef) #print the coefficients and p-value of new model\n}\n\n[1] \"BMI\"\n              Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) -7.2246767 0.37353368 -19.34143  9.166705e-79\nBMI          0.4031809 0.01333142  30.24291 3.287630e-176\n[1] \"Smoke\"\n              Estimate Std. Error    t value      Pr(&gt;|t|)\n(Intercept) -8.0330103  0.4167640 -19.274721  3.489860e-78\nSmoke        0.5474543  0.1189348   4.602979  4.336138e-06\nBMI          0.4012350  0.0135023  29.716055 1.620836e-170\n[1] \"SBP\"\n                Estimate  Std. Error    t value      Pr(&gt;|t|)\n(Intercept) -10.12333941 0.656060665 -15.430493  9.249083e-52\nSmoke         0.51976089 0.119381335   4.353787  1.382613e-05\nBMI           0.38227247 0.014353787  26.632168 1.610528e-140\nSBP           0.02271891 0.005562449   4.084335  4.535400e-05\n[1] \"HealthRank\"\n                Estimate  Std. Error    t value      Pr(&gt;|t|)\n(Intercept) -10.32713860 0.658203408 -15.689889  2.178707e-53\nSmoke         0.50692563 0.119274332   4.250082  2.201800e-05\nBMI           0.37152877 0.014769088  25.155837 6.517520e-127\nSBP           0.02122697 0.005576864   3.806256  1.438950e-04\nHealthRank    0.29529144 0.095400031   3.095297  1.984225e-03\n[1] \"Race\"\n               Estimate Std. Error    t value      Pr(&gt;|t|)\n(Intercept) -9.37704108 0.75369495 -12.441427  1.092894e-34\nRace        -0.17371276 0.06733664  -2.579766  9.933716e-03\nSmoke        0.51630714 0.11921799   4.330782  1.534328e-05\nBMI          0.36772807 0.01482861  24.798552 1.079844e-123\nSBP          0.01856742 0.00566621   3.276868  1.061567e-03\nHealthRank   0.27865504 0.09552851   2.916983  3.560496e-03\n[1] \"Physical_Activity\"\n                     Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept)       -8.54195765 0.905491077 -9.433508  7.766347e-21\nRace              -0.19359742 0.068284184 -2.835172  4.611254e-03\nSmoke              0.54688434 0.120612497  4.534226  6.011602e-06\nBMI                0.36264845 0.015437444 23.491482 5.093549e-112\nSBP                0.01687031 0.005731595  2.943388  3.271718e-03\nPhysical_Activity -0.10669600 0.099170608 -1.075883  2.820674e-01\nHealthRank         0.21552263 0.102363350  2.105467  3.533494e-02"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#summary-of-the-parsimonious-model",
    "href": "posts/Logistic Regression Project 22/index.html#summary-of-the-parsimonious-model",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Summary of the parsimonious model",
    "text": "Summary of the parsimonious model\nThe parsimonious model did not include Financial Strain, but since that is the primary predictor for this regression analysis project, we decided to include it back into the model.\nModel table\n\nWhat is blocked out: a table of the coefficient estimates from the parsimonious model using the forward elimination procedure.\n\nRegression table\n\nCheck again for multicollinearity\n\n\n# any VIF value &gt; 5 suggests multicollinearity\ncar::vif(parsimonious_model)\n\n Financial_Strain              Race             Smoke               BMI \n         1.092213          1.083085          1.017671          1.288503 \n              SBP Physical_Activity        HealthRank \n         1.178972          1.250491          1.290492 \n\n\nThis was done prior to building the parsimonious model, but the multicollinearity test was ran again just to ensure that after removing a couple of variables, the linearity between independent variables didn’t change. All vif values are less than 5, suggesting that there is no collinearity between predictor variables in our parsimonious model."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#concluding-model---no-interactions-just-additions",
    "href": "posts/Logistic Regression Project 22/index.html#concluding-model---no-interactions-just-additions",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Concluding Model - No interactions, just additions",
    "text": "Concluding Model - No interactions, just additions\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\cdot \\textrm{Financial Strain} + \\beta_2 \\cdot \\textrm{Race/Ethnicity} + \\\\ \\beta_3 \\cdot \\textrm{Education} + \\beta_4 \\cdot \\textrm{Smoking Status} + \\beta_5 \\cdot \\textrm{BMI} + \\beta_6 \\cdot \\textrm{SBP} +\\\\ \\beta_7 \\cdot \\textrm{Health Rank} + \\epsilon\\]"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#parsimonious-model-conclusion-after-testing-for-interactions",
    "href": "posts/Logistic Regression Project 22/index.html#parsimonious-model-conclusion-after-testing-for-interactions",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Parsimonious Model Conclusion: After Testing for Interactions",
    "text": "Parsimonious Model Conclusion: After Testing for Interactions\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\cdot \\textrm{Financial Strain} + \\beta_2 \\cdot \\textrm{Race/Ethnicity} + \\\\ \\beta_3 \\cdot \\textrm{Education} + \\beta_4 \\cdot \\textrm{Smoking Status} + \\beta_5 \\cdot \\textrm{BMI} + \\beta_6 \\cdot \\textrm{SBP} +\\\\ \\beta_7 \\cdot \\textrm{Health Rank} + \\epsilon\\]"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#testing-normality",
    "href": "posts/Logistic Regression Project 22/index.html#testing-normality",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Testing Normality",
    "text": "Testing Normality\nBelow, we are taking the residuals of the parsimonious model that we established above and looked to see if this model meets the normality assumptions.\nVisualization: Assessing Normality\nTraditional Method: Assessing Normality, Shapiro Wilkes Test\nNote: residuals are normal but there is not solid linearity (as seen in the Normal Q-Q plot) so more needs to be explored\n\nTransforming CRP\n\nNote: I decided to show the transformation of the outcome variable since that is ideally something we should have checked earlier on. I also just like how satisfying it is to see the before and after effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy log-transforming CRP, the regression model has more normality than prior to the transformation. The Residuals vs Leverage plot indicates there’s more to look into to see if further transformations could be done. There are two continuous variables that could be possibly be transformed, which will be assessed below.\n\n\nAssessing Continuous Variables\nThere are two continuous variables in this dataset: BMI and SBP\nBMI Variable\nSBP Variable\nAfter performing log-transformations on the BMI and SBP, the density plots have shown that these variables appear more like a normal distribution."
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html",
    "href": "posts/Machine Learning Walk-Through/index.html",
    "title": "Machine Learning Walk-Through",
    "section": "",
    "text": "I will be following the process found on this site by Jason Brownlee.\nAn ode to template creation\nSo this author is using the iris flowers dataset, which I will use too. However, if time allows, I’ll create a mirror project with another dataset. The researcher behind the iris flowers research was a eugenist [post source here; his name Edgar Anderson]."
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html#characteristics-for-data",
    "href": "posts/Machine Learning Walk-Through/index.html#characteristics-for-data",
    "title": "Machine Learning Walk-Through",
    "section": "Characteristics for Data",
    "text": "Characteristics for Data\n\nNumeric columns\n\nClassification problems (i.e., inputs of characteristics to determine the iris)\n\nFew columns and 100-200 observations\n\nAll columns are in the same unit and scale; otherwise scaling and transformations would need to occur"
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html#summarize-the-dataset",
    "href": "posts/Machine Learning Walk-Through/index.html#summarize-the-dataset",
    "title": "Machine Learning Walk-Through",
    "section": "Summarize the Dataset",
    "text": "Summarize the Dataset\n\nDimensions\n\ndim(dataset)\n\n[1] 120   5\n\n\n\n\nTypes of Columns\n\nsapply(dataset, class)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\" \n\n\n\n\nData Peek\n\nhead(dataset)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n8          5.0         3.4          1.5         0.2  setosa\n\n\n\n\nLevels of the Species Column\n\n# using base\nlevels(dataset$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n# 3+ categories in a column : multinomial \n# 2 categories in a column: binomial/binary\n\n\nBreakdown of Species Column\n\n# using base\npercentage &lt;- prop.table(table(dataset$Species)) * 100 \n\ncbind(freq = table(\n  dataset$Species), \n  percentage = percentage)\n\n           freq percentage\nsetosa       40   33.33333\nversicolor   40   33.33333\nvirginica    40   33.33333\n\n\n\n\n\nSummary of all Columns\n\nsummary(dataset)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.100   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.575   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.300   Median :1.300  \n Mean   :5.853   Mean   :3.058   Mean   :3.768   Mean   :1.217  \n 3rd Qu.:6.425   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.700   Max.   :2.500  \n       Species  \n setosa    :40  \n versicolor:40  \n virginica :40"
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html#visualizations",
    "href": "posts/Machine Learning Walk-Through/index.html#visualizations",
    "title": "Machine Learning Walk-Through",
    "section": "Visualizations",
    "text": "Visualizations\n\nUnivariate\n\nx &lt;- dataset[, 1:4]\ny &lt;- dataset[,5]\n\n\n# 2x2 layout for boxplots\npar(mfrow = c(1,4))\n\n# loop through each column of 'x' to create a boxplot\nfor(i in 1:4){\n  boxplot(x[ ,i], main = names(iris)[i])}\n\n\n\n\n\n\n\n\n\n\nMultivariate\n\n# visually assessing interactions between the variables, 3 colors to represent species types\n\n# caret::featurePlot(\n#  x = x, \n#  y = y, \n#  plot = \"ellipse\"\n#)\n\n### Error message:\n# Error in grid.Call.graphics(C_downviewport, name$name, strict) :\n# Viewport 'plot_01.panel.1.1.off.vp' was not founda\n\n# \"pairs\" without \"ellipse\" is most similar\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"pairs\")\n\n\n\n\n\n\n\n### another attempt with a different package \n\nlibrary(GGally)\nggpairs(dataset, columns = 1:4, \n        ggplot2::aes(color = y))\n\n\n\n\n\n\n\n\n\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"box\")\n\n\n\n\n\n\n\n\n\n# density plot for each category of species \n\nscales &lt;- list(\n  x = list(\n    relation = \"free\"), \n  y = list(\n    relation = \"free\"\n))\n\ncaret::featurePlot(x = x, \n                   y = y, \n                   plot = \"density\", \n                   scales = scales)"
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html#test-harness-10-fold-cross-validation",
    "href": "posts/Machine Learning Walk-Through/index.html#test-harness-10-fold-cross-validation",
    "title": "Machine Learning Walk-Through",
    "section": "Test harness: 10-fold cross validation",
    "text": "Test harness: 10-fold cross validation\nThis method estimates accuracy\n* Splits data into 10 parts\n* Trains 9 parts, tests 1 part\n* Releases all combinations of train-test splits\n* Repeat 3 times for each algorithm for more accurate estimate\n\n# run algorithms using 10-fold cross validation   \n\ncontrol &lt;- caret::trainControl(method = \"cv\", \n                               number = 10)  \n\nmetric &lt;- \"Accuracy\"\n\nFormula for accuracy\n\\[\n\\text{accuracy} = \\frac{\\text{correctly predicted instances}}{\\text{total number of instances}} ~\\cdot~ 100\n\\]"
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html#build-5-different-models-for-prediction",
    "href": "posts/Machine Learning Walk-Through/index.html#build-5-different-models-for-prediction",
    "title": "Machine Learning Walk-Through",
    "section": "Build 5 different models for prediction",
    "text": "Build 5 different models for prediction\nEvaluate 5 different algorithms\nsimple linear method\n1. Linear Discriminant Analysis (LDA)\nnonlinear method\n2. Classification and Regression Trees (CART)\n3. k-Nearest Neighbors (kNN)\ncomplex nonlinear method\n4. Support Vector Machines (SVM) with a linear kernel\n5. Random Forest (RF)\n\n### set seed before each run \n\n# simple linear\n### LDA  \nset.seed(7)\n\nfit.lda &lt;- caret::train(Species~., \n                        data = dataset, \n                        method = \"lda\", \n                        metric = metric, \n                        trControl = control)\n\n\n# nonlinear\n### CART  \nset.seed(7) \n\nfit.cart &lt;- caret::train(Species~., \n                        data = dataset, \n                        method = \"rpart\", \n                        metric = metric, \n                        trControl = control)\n\n### kNN  \nset.seed(7)   \n\nfit.knn &lt;- caret::train(Species~., \n                        data = dataset, \n                        method = \"knn\", \n                        metric = metric, \n                        trControl = control)\n\n# complex nonlinear\n### SVM  \nset.seed(7)   \n\nfit.svm &lt;- caret::train(Species~., \n                        data = dataset, \n                        method = \"svmRadial\", \n                        metric = metric, \n                        trControl = control)\n\n### RF  \nset.seed(7)  \n\nfit.rf &lt;- caret::train(Species~., \n                        data = dataset, \n                        method = \"rf\", \n                        metric = metric, \n                        trControl = control)\n\n### Note from website: \n  # caret can configure and tune the configuration of each model but that isn't covered in the tutorial I'm going off of"
  },
  {
    "objectID": "posts/Machine Learning Walk-Through/index.html#select-the-best-model",
    "href": "posts/Machine Learning Walk-Through/index.html#select-the-best-model",
    "title": "Machine Learning Walk-Through",
    "section": "Select the best model",
    "text": "Select the best model\n\n# summarize accuracy of models  \nresults &lt;- caret::resamples(\n  list(\n    lda = fit.lda, \n    cart = fit.cart, \n    knn = fit.knn, \n    svm = fit.svm, \n    rf = fit.rf\n  )\n)\n\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: lda, cart, knn, svm, rf \nNumber of resamples: 10 \n\nAccuracy \n          Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's\nlda  0.9166667 0.9375000 1.0000000 0.9750000       1    1    0\ncart 0.8333333 0.9166667 1.0000000 0.9583333       1    1    0\nknn  0.8333333 0.9166667 1.0000000 0.9583333       1    1    0\nsvm  0.9166667 0.9166667 0.9583333 0.9583333       1    1    0\nrf   0.8333333 0.9166667 0.9583333 0.9416667       1    1    0\n\nKappa \n      Min. 1st Qu. Median   Mean 3rd Qu. Max. NA's\nlda  0.875 0.90625 1.0000 0.9625       1    1    0\ncart 0.750 0.87500 1.0000 0.9375       1    1    0\nknn  0.750 0.87500 1.0000 0.9375       1    1    0\nsvm  0.875 0.87500 0.9375 0.9375       1    1    0\nrf   0.750 0.87500 0.9375 0.9125       1    1    0\n\n\n\nPlot model evaluation results\n\nlattice::dotplot(results)\n\n\n\n\n\n\n\n# the most accurate model is the LDA\n\n\nprint(fit.lda)\n\nLinear Discriminant Analysis \n\n120 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 108, 108, 108, 108, 108, 108, ... \nResampling results:\n\n  Accuracy  Kappa \n  0.975     0.9625\n\n# standard deviation of \"accuracy\" and \"kappa\" were shown in the tutorial   \n\n### haven't figured it out yet... TBC..."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Research Proposal Ideas/index.html",
    "href": "posts/Research Proposal Ideas/index.html",
    "title": "Postpartum Research Proposal",
    "section": "",
    "text": "From the time of this post, I have an internship and 3 comp exams before I will be qualified to graduate. The internship is in two parts; the integrative project and practice experience. The purpose of this is to showcase a portfolio of work that I have done throughout the program and will give a presentation at the end of the given term.\nTo succeed in this integrative project I need to demonstrate the sixth foundational competency for MPH students,\n\nDiscuss the means by which structural bias, social inequities and racism undermine health and create challenges to achieving health equity at organizational, communitym and systemic levels.\n\nBe able to discuss factors (esp racism) that impact health equity at multiple levels for a particular health problem. Students should be able to discuss health disparities and differences among groups, as well as the ways in which organizations, systems, and structures operate that may have inequitable influences on certain groups.\nGiven the above, I have wanted my thesis to be around the industrial prison complex, health disparities, efficient and quality care, and the surmounting cost of everything involved. I also really like exploring ‘taboo’ topics and pushing myself to grow to get comfortable with discussing these topics. One that is often on my mind is perinatal and postpartum mental health with an emphasis on psychosis, extreme personality and behavior changes, and the scale of people (mostly children) affected in some form from postpartum behavioral changes (e.g., homicide). I am also wanting to explore the current ways in which perinatal and postpartum mental health is evaluated and possible program initiatives to raise awareness for pregnant people.\nRegarding biostatistics and its applications towards this research, I am wanting to build and test association and prediction models using population-based, longitudinal data to assess the cost of imprisoning mental illness, predict the amount of lives lost, and possibly how the current screenings predict postpartum depression or related outcome."
  }
]
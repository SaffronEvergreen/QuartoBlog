[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collections",
    "section": "",
    "text": "Logistic Regression Project Winter 2022\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nSaffron\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nSaffron\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nSaffron\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is under construction but is intended as a way to express what I’ve learned throughout my MPH Biostatistics program, as a student and individual who geeks out in their free time."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I’m going to keep this as my trial and error post –\nThe content below is what was provided in the default Quarto Blog stuff\n__\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/First Blog Post/index.html",
    "href": "posts/First Blog Post/index.html",
    "title": "First Post",
    "section": "",
    "text": "This is my first attempt at posting a blog using Quarto!\nI’ve built two blogs in the past using the Blogdown-Hugo-GitHub-Netlify pipeline but those are on a laptop which has now passed over and the URL’s are nowhere to be found. So my goal here is to lay out all the fun I’ve had and the major learning curves I’ve pushed through these last few years.\nI have way too many interests and passions related and unrelated to data science so my goal is to touch on many different topics that I’ve covered in my classes, as well as what I’ve worked on outside of graduate courses.\nResume Link\nA new tool that I’ve been lightly tinkering with is typst.app which is what I’ve been using to build my resume and cover letters. Here is the link for my resume.\n\n# This is for me to edit my resume \n# https://typst.app/project/wfidmzdG33SwaQ2Adu5pNG  \n# This is for me to edit my cover letter \n# https://typst.app/project/w14mwPCsdNobGkuX_i_knB"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html",
    "href": "posts/Logistic Regression Project 22/index.html",
    "title": "Logistic Regression Project Winter 2022",
    "section": "",
    "text": "As I was rendering all of my group’s hard work, I remembered that uploading projects and assignments in full completion for all to see on the internet is against academic policy. For those looking to collaborate or hire me as part of your team, I will send a copy of the completed project upon request. To abide by the rules, I will show just a bit of the highlights and my thoughts along the way.\nThis project was done in collaboration with 3 other peers and I am forever grateful for their commitment and perseverance, especially during this final project. Please know that what I have posted below is not all my brain, it is a collaboration piece, as is most projects.\nThe datasets used in this project were sourced from the (SWAN database) https://www.icpsr.umich.edu/web/ICPSR/series/00253."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#baseline-model",
    "href": "posts/Logistic Regression Project 22/index.html#baseline-model",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Baseline Model",
    "text": "Baseline Model\nRegression Formula\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\textrm{Financial Strain} + \\beta_2 \\textrm{Race/Ethnicity} + \\beta_3  \\textrm{Education}+ \\beta_4 \\textrm{Marital Status} + \\\\ \\beta_5 \\textrm{Smoking Status} + \\beta_6 \\textrm{Age} + \\beta_7 \\textrm{BMI} + \\beta_8 \\textrm{SBP} + \\beta_9 \\textrm{Physical Activity} + \\\\ \\beta_{10} \\textrm{Difficulty Sleeping} + \\beta_{11} \\textrm{Health Rank} + \\epsilon\\]\nBaseline Hypothesis\n\\[H_0: \\beta_{1}=\\beta_{2}=.....\\beta_{11}=0 \\quad \\text{vs.} \\quad H_A: \\text{At least one of } \\beta_{j} \\neq 0\\]\n\nWhat is blocked out: We had ran a model using lm() function and created a clean table assessing the coefficient estimates."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#finding-outliers-using-cooks-distance",
    "href": "posts/Logistic Regression Project 22/index.html#finding-outliers-using-cooks-distance",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Finding outliers using Cook’s distance",
    "text": "Finding outliers using Cook’s distance\n\n# finding outliers \ncooks_points <- cooks.distance(baseline_model)\n\n(cooks_distance_plot <- plot(baseline_model, which = 4))\n\n\n\n\nNULL\n\n# determining outliers, regardless of influence\nas.numeric(names(cooks_points)[(cooks_points > (4/2928))]) \n\n  [1]    4   25   68   77   85   88  151  171  214  316  328  329  340  354  436\n [16]  477  482  502  503  524  535  571  576  609  657  660  672  686  727  741\n [31]  771  787  824  868  892  948  957  979  983 1020 1024 1031 1077 1113 1122\n [46] 1124 1171 1209 1262 1264 1303 1335 1341 1366 1408 1431 1449 1464 1488 1491\n [61] 1506 1550 1551 1568 1581 1592 1650 1658 1751 1775 1786 1849 1852 1859 1876\n [76] 1899 1915 1916 1922 1985 1993 2002 2040 2070 2183 2194 2317 2318 2346 2375\n [91] 2405 2428 2460 2466 2489 2514 2627 2635 2643 2727 2749 2759 2772 2796 2846\n[106] 2861 2907 2908 2913 2919 2979 2993 3043 3046 3089 3094 3108 3143 3153 3162\n[121] 3164 3212 3236 3280 3297 3300 3303\n\n  # a lot of outliers without strong influence over linearity of the model\n\n# removing influential outliers \nSWANish <-SWANish[!(row.names(SWANish) %in% c(\"3303\",\"1124\",\"1568\",\"2635\")),]\n\n\nBaseline model sans influential outliers\n\n# new model without outliers\nbaseline_model <- lm(CRP ~ Financial_Strain + Race + Education + Marital_Status + Smoke + Age + BMI + SBP + Physical_Activity + HealthRank + Difficulty_Sleeping,\n            data = SWANish)\ntidy(baseline_model) %>%  gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-8.38185665\n1.803018788\n-4.6487905\n3.487436e-06\n    Financial_Strain1\n-0.15414031\n0.206328939\n-0.7470610\n4.550870e-01\n    Race\n-0.17415986\n0.068705294\n-2.5348827\n1.130043e-02\n    Education\n-0.11889584\n0.084955353\n-1.3995097\n1.617667e-01\n    Marital_Status\n-0.17061525\n0.147686943\n-1.1552494\n2.480831e-01\n    Smoke\n0.51069935\n0.122185276\n4.1797127\n3.004750e-05\n    Age\n0.01637096\n0.034962824\n0.4682391\n6.396486e-01\n    BMI\n0.36074630\n0.015511072\n23.2573412\n6.879442e-110\n    SBP\n0.01714012\n0.005815805\n2.9471630\n3.232510e-03\n    Physical_Activity\n-0.14135195\n0.099561118\n-1.4197506\n1.557874e-01\n    HealthRank\n0.16197453\n0.108698472\n1.4901270\n1.362992e-01\n    Difficulty_Sleeping\n0.08024985\n0.082842611\n0.9687025\n3.327741e-01\n  \n  \n  \n\n\n\nregression_points <- augment(baseline_model)\n\n# baseline sans outliers \nautoplot(baseline_model)\n\n\n\n# making sure that all influential values have been removed\n  # all values below threshold   \nplot(baseline_model, which = 4)"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#transforming-variables-numeric-to-factored",
    "href": "posts/Logistic Regression Project 22/index.html#transforming-variables-numeric-to-factored",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Transforming variables: numeric to factored",
    "text": "Transforming variables: numeric to factored\nThese data transformations are for creating Table 1. It became too complicated to keep the missing values and as numeric variables. We didn’t want to significantly affect our primary dataset SWANish, so this dataset created below, swan_fct, is used as a substitute for Table 1.\n\n# remove rows with NA and create dataset for recoding variables into factors  \n\n# it's convenient to have extra datasets like this, so when edits are made to factors or variable names, it is easy to re-run without overlapping edits\n\nswan_fct <- SWANish\n\nswan_fct$Financial_Strain <- swan_fct$Financial_Strain %>% \n  factor(levels = c(\"0\", \"1\"), # how many levels set\n    labels = c(\"Somewhat/Very hard\", \n                   \"Not hard\"), \n        ordered = TRUE) # showing \"hierarchy\" primarily for creating tables and having the same structure for each\n\nswan_fct$Race <- swan_fct$Race %>% \n  factor(levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"),\n         labels = c(\"Black/African American\", \n                    \"Chinese/Chinese American\", \n                    \"Japanese/Japanese American\", \n                    \"Caucasian/White Non-Hispanic\", \n                    \"Hispanic\"), \n         ordered = TRUE)\n\nswan_fct$Education <- swan_fct$Education %>% \n  factor(levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"),\n         labels = c(\"Less than high school\", \n                    \"High school graduate\", \n                    \"Some college/technical school\", \n                    \"College graduate\", \n                    \"Post graduate education\"), \n         ordered = TRUE)\n\nswan_fct$Marital_Status <- swan_fct$Marital_Status %>% \n  factor(levels = c(\"1\", \"2\", \"3\", \"4\"), \n         labels = c(\"Single, never married\", \n                    \"Currently married/living as married\", \n                    \"Separated or divorced\", \n                    \"Widowed\"), \n         ordered = TRUE)\n\nswan_fct$Physical_Activity <- swan_fct$Physical_Activity %>% \n  factor(levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"),\n         labels = c(\"Much less than other women your age\", \n                    \"Somewhat less than other women your age\", \n                    \"About the same as other women your age\", \n                    \"Somewhat more than other women your age\", \n                    \"Much more than other women your age\"), \n         ordered = TRUE)\n\nswan_fct$HealthRank <- swan_fct$HealthRank %>% \n  factor(levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"),\n         labels = c(\"Excellent\", \n                    \"Very good\", \n                    \"Good\", \n                    \"Fair\", \n                    \"Poor\"), \n         ordered = TRUE)\n\nswan_fct$Difficulty_Sleeping <- swan_fct$Difficulty_Sleeping %>% \n  factor(levels = c(\"1\", \"2\"), \n         labels = c(\"None\", \n                    \"Yes\"),\n         ordered = TRUE) \n\nswan_fct$Smoke <- swan_fct$Smoke %>% \n  factor(levels = c(\"1\", \"2\", \"3\"), \n         labels = c(\"Never smoked\",\n                    \"Former smoker\", \n                    \"Current smoker\"),\n         ordered = TRUE)\n\n\n# check work - shows how many factor levels there are, and the amount of observations per factor level\nfct_count(swan_fct$Financial_Strain)\n\n# A tibble: 3 × 2\n  f                      n\n  <ord>              <int>\n1 Somewhat/Very hard  1967\n2 Not hard            1310\n3 <NA>                  22"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#distribution-of-crp",
    "href": "posts/Logistic Regression Project 22/index.html#distribution-of-crp",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Distribution of CRP",
    "text": "Distribution of CRP"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#boxplots",
    "href": "posts/Logistic Regression Project 22/index.html#boxplots",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Boxplots",
    "text": "Boxplots"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#density-plots",
    "href": "posts/Logistic Regression Project 22/index.html#density-plots",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Density plots",
    "text": "Density plots\n\nCheck for multicollinearity\n\n\n# any VIF value > 5 suggests multicollinearity\ncar::vif(baseline_model)\n\n   Financial_Strain                Race           Education      Marital_Status \n           1.180696            1.088712            1.195049            1.028614 \n              Smoke                 Age                 BMI                 SBP \n           1.028287            1.034975            1.292750            1.210437 \n  Physical_Activity          HealthRank Difficulty_Sleeping \n           1.259768            1.392629            1.070196"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#baseline-model-conclusion",
    "href": "posts/Logistic Regression Project 22/index.html#baseline-model-conclusion",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Baseline model conclusion",
    "text": "Baseline model conclusion\nRegression Formula\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\textrm{Financial Strain} + \\beta_2 \\textrm{Race/Ethnicity} + \\beta_3  \\textrm{Education}+ \\beta_4 \\textrm{Marital Status} + \\\\ \\beta_5 \\textrm{Smoking Status} + \\beta_6 \\textrm{Age} + \\beta_7 \\textrm{BMI} + \\beta_8 \\textrm{SBP} + \\beta_9 \\textrm{Physical Activity} + \\\\ \\beta_{10} \\textrm{Difficulty Sleeping} + \\beta_{11} \\textrm{Health Rank} + \\epsilon\\]\nBaseline Hypothesis\n\\[H_0: \\beta_{1}=\\beta_{2}=.....\\beta_{11}=0 \\quad \\text{vs.} \\quad H_A: \\text{At least one of } \\beta_{j} \\neq 0\\]\n\nCheck for multicollinearity\n\n\n# any VIF value > 5 suggests multicollinearity\ncar::vif(baseline_model)\n\n   Financial_Strain                Race           Education      Marital_Status \n           1.180696            1.088712            1.195049            1.028614 \n              Smoke                 Age                 BMI                 SBP \n           1.028287            1.034975            1.292750            1.210437 \n  Physical_Activity          HealthRank Difficulty_Sleeping \n           1.259768            1.392629            1.070196 \n\n\nAt this stage in the multiple linear regression analysis project, we can state there is no sign of multicollinearity and will will treat all variables as variables of interest. Below, we will determine the parsimonious (most reliable) model using Mallow’s Cp Criterion and comparing that model to what we find after determining the parsimonious model “by-hand”, which is building on, adding a new independent variable with each step."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#forward-elimination-procedure",
    "href": "posts/Logistic Regression Project 22/index.html#forward-elimination-procedure",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Forward Elimination Procedure",
    "text": "Forward Elimination Procedure\n\nNote: The code below (with the exception of the covariates added) is solely a gift from my previous professor. \n\n# another method for determining parsimonious model\nmodel <- regsubsets(CRP ~ \n              Financial_Strain + \n              Race + Education + \n              Marital_Status + \n              Smoke + Age + BMI + \n              SBP + \n              Physical_Activity + \n              HealthRank + \n              Difficulty_Sleeping, \n                         data = SWANish, \n                         nvmax = 13,\n                    method=\"forward\")\n\np.val <- 0; i <- 1; vars_last <- NULL\n\n#----------------------------\n# do not edit code within the \"while\" loop\"\nwhile(p.val <= 0.1){\n  vars_current <- names(coef(model,i))[-1]\n  var_add <- vars_current[!vars_current %in% vars_last]\n  coef <- summary(lm(as.formula(paste(\"CRP ~ \", paste(vars_current, collapse =\" + \"), sep = \"\")),  \n                  data = SWANish))$coefficients\n  \n  p.val <- coef[var_add,\"Pr(>|t|)\"]\n  vars_last <- vars_current\n  i=i + 1\n  print(var_add) # print the variable being added \n  print(coef) #print the coefficients and p-value of new model\n}\n\n[1] \"BMI\"\n              Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) -7.2246767 0.37353368 -19.34143  9.166705e-79\nBMI          0.4031809 0.01333142  30.24291 3.287630e-176\n[1] \"Smoke\"\n              Estimate Std. Error    t value      Pr(>|t|)\n(Intercept) -8.0330103  0.4167640 -19.274721  3.489860e-78\nSmoke        0.5474543  0.1189348   4.602979  4.336138e-06\nBMI          0.4012350  0.0135023  29.716055 1.620836e-170\n[1] \"SBP\"\n                Estimate  Std. Error    t value      Pr(>|t|)\n(Intercept) -10.12333941 0.656060665 -15.430493  9.249083e-52\nSmoke         0.51976089 0.119381335   4.353787  1.382613e-05\nBMI           0.38227247 0.014353787  26.632168 1.610528e-140\nSBP           0.02271891 0.005562449   4.084335  4.535400e-05\n[1] \"HealthRank\"\n                Estimate  Std. Error    t value      Pr(>|t|)\n(Intercept) -10.32713860 0.658203408 -15.689889  2.178707e-53\nSmoke         0.50692563 0.119274332   4.250082  2.201800e-05\nBMI           0.37152877 0.014769088  25.155837 6.517520e-127\nSBP           0.02122697 0.005576864   3.806256  1.438950e-04\nHealthRank    0.29529144 0.095400031   3.095297  1.984225e-03\n[1] \"Race\"\n               Estimate Std. Error    t value      Pr(>|t|)\n(Intercept) -9.37704108 0.75369495 -12.441427  1.092894e-34\nRace        -0.17371276 0.06733664  -2.579766  9.933716e-03\nSmoke        0.51630714 0.11921799   4.330782  1.534328e-05\nBMI          0.36772807 0.01482861  24.798552 1.079844e-123\nSBP          0.01856742 0.00566621   3.276868  1.061567e-03\nHealthRank   0.27865504 0.09552851   2.916983  3.560496e-03\n[1] \"Physical_Activity\"\n                     Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept)       -8.54195765 0.905491077 -9.433508  7.766347e-21\nRace              -0.19359742 0.068284184 -2.835172  4.611254e-03\nSmoke              0.54688434 0.120612497  4.534226  6.011602e-06\nBMI                0.36264845 0.015437444 23.491482 5.093549e-112\nSBP                0.01687031 0.005731595  2.943388  3.271718e-03\nPhysical_Activity -0.10669600 0.099170608 -1.075883  2.820674e-01\nHealthRank         0.21552263 0.102363350  2.105467  3.533494e-02"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#summary-of-the-parsimonious-model",
    "href": "posts/Logistic Regression Project 22/index.html#summary-of-the-parsimonious-model",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Summary of the parsimonious model",
    "text": "Summary of the parsimonious model\nThe parsimonious model did not include Financial Strain, but since that is the primary predictor for this regression analysis project, we decided to include it back into the model.\nModel table\n\nWhat is blocked out: a table of the coefficient estimates from the parsimonious model using the forward elimination procedure.\n\nRegression table\n\nCheck again for multicollinearity\n\n\n# any VIF value > 5 suggests multicollinearity\ncar::vif(parsimonious_model)\n\n Financial_Strain              Race             Smoke               BMI \n         1.092213          1.083085          1.017671          1.288503 \n              SBP Physical_Activity        HealthRank \n         1.178972          1.250491          1.290492 \n\n\nThis was done prior to building the parsimonious model, but the multicollinearity test was ran again just to ensure that after removing a couple of variables, the linearity between independent variables didn’t change. All vif values are less than 5, suggesting that there is no collinearity between predictor variables in our parsimonious model."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#concluding-model---no-interactions-just-additions",
    "href": "posts/Logistic Regression Project 22/index.html#concluding-model---no-interactions-just-additions",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Concluding Model - No interactions, just additions",
    "text": "Concluding Model - No interactions, just additions\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\cdot \\textrm{Financial Strain} + \\beta_2 \\cdot \\textrm{Race/Ethnicity} + \\\\ \\beta_3 \\cdot \\textrm{Education} + \\beta_4 \\cdot \\textrm{Smoking Status} + \\beta_5 \\cdot \\textrm{BMI} + \\beta_6 \\cdot \\textrm{SBP} +\\\\ \\beta_7 \\cdot \\textrm{Health Rank} + \\epsilon\\]"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#parsimonious-model-conclusion-after-testing-for-interactions",
    "href": "posts/Logistic Regression Project 22/index.html#parsimonious-model-conclusion-after-testing-for-interactions",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Parsimonious Model Conclusion: After Testing for Interactions",
    "text": "Parsimonious Model Conclusion: After Testing for Interactions\n\\[\\textrm{CRP} = \\beta_0 + \\beta_1 \\cdot \\textrm{Financial Strain} + \\beta_2 \\cdot \\textrm{Race/Ethnicity} + \\\\ \\beta_3 \\cdot \\textrm{Education} + \\beta_4 \\cdot \\textrm{Smoking Status} + \\beta_5 \\cdot \\textrm{BMI} + \\beta_6 \\cdot \\textrm{SBP} +\\\\ \\beta_7 \\cdot \\textrm{Health Rank} + \\epsilon\\]"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#testing-normality",
    "href": "posts/Logistic Regression Project 22/index.html#testing-normality",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Testing Normality",
    "text": "Testing Normality\nBelow, we are taking the residuals of the parsimonious model that we established above and looked to see if this model meets the normality assumptions.\nVisualization: Assessing Normality\nTraditional Method: Assessing Normality, Shapiro Wilkes Test\nNote: residuals are normal but there is not solid linearity (as seen in the Normal Q-Q plot) so more needs to be explored\n\nTransforming CRP\n\nNote: I decided to show the transformation of the outcome variable since that is ideally something we should have checked earlier on. I also just like how satisfying it is to see the before and after effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy log-transforming CRP, the regression model has more normality than prior to the transformation. The Residuals vs Leverage plot indicates there’s more to look into to see if further transformations could be done. There are two continuous variables that could be possibly be transformed, which will be assessed below.\n\n\nAssessing Continuous Variables\nThere are two continuous variables in this dataset: BMI and SBP\nBMI Variable\nSBP Variable\nAfter performing log-transformations on the BMI and SBP, the density plots have shown that these variables appear more like a normal distribution."
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#packages",
    "href": "posts/Logistic Regression Project 22/index.html#packages",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\nlibrary(readxl)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(ggplot2)\nlibrary(lattice)\nlibrary(psych)\nlibrary(describedata) \nlibrary(ggfortify)\nlibrary(plotly)\nlibrary(GGally) \nlibrary(dplyr)\nlibrary(rsq)\nlibrary(gtsummary)\nlibrary(tinytex)\nlibrary(car)  # vif(), car::Anova()\nlibrary(cowplot)\nlibrary(moderndive)\nlibrary(forcats)\nlibrary(leaps)\nlibrary(ggridges)"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#loading-data",
    "href": "posts/Logistic Regression Project 22/index.html#loading-data",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Loading Data",
    "text": "Loading Data\n\n# baseline data\nload(\"C:/Users/mckjo/OneDrive/Desktop/GitBlog/posts/Logistic Regression Project 22/ICPSR_04368/ICPSR_04368/DS0001/04368-0001-Data.rda\")\n\n# cross-sectional data \nload(\"C:/Users/mckjo/OneDrive/Desktop/GitBlog/posts/Logistic Regression Project 22/ICPSR_28762/ICPSR_28762/DS0001/28762-0001-Data.rda\")"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#merging-datasets-renaming-variables",
    "href": "posts/Logistic Regression Project 22/index.html#merging-datasets-renaming-variables",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Merging Datasets, Renaming Variables",
    "text": "Merging Datasets, Renaming Variables"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#primary-predictor-as-binary",
    "href": "posts/Logistic Regression Project 22/index.html#primary-predictor-as-binary",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Primary Predictor as Binary",
    "text": "Primary Predictor as Binary"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#assessing-outliers-cooks-distance",
    "href": "posts/Logistic Regression Project 22/index.html#assessing-outliers-cooks-distance",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Assessing Outliers: Cook’s Distance",
    "text": "Assessing Outliers: Cook’s Distance\n\nWhat is blocked out: we performed a visual assessment to show the influential outliers using cook’s distance with the cut-off point of \\(4 \\div 2928\\) along the slope and removed the rows which had those outliers.\n\n\nBaseline Model: Reassessment\n\nWhat is blocked out: we then assessed linearity of our baseline model with the influential points removed. The assumptions for linearity were most definitely still not met at this point.\nAssumptions for linearity:\n* Linearity: nope\n* Independence: yep\n* Homoscedasticity: definitely not\n* Normality: not quite"
  },
  {
    "objectID": "posts/Logistic Regression Project 22/index.html#descriptive-statistics-table",
    "href": "posts/Logistic Regression Project 22/index.html#descriptive-statistics-table",
    "title": "Logistic Regression Project Winter 2022",
    "section": "Descriptive Statistics Table",
    "text": "Descriptive Statistics Table\n\nWhat is blocked out: a bunch of code converting the covariates to factored variables.\n\n\nTable 1\n\nNote: I was emotionally attached to this table since this was one of the learning curves I endured combining various gt libraries and functions so I’m including it since this was my baby at the time.\n\n\n# list of all variables for easy input \n\n# Financial_Strain, Race, Education, Marital_Status, Smoke, Age, BMI, SBP, Physical_Activity, HealthRank, Difficulty_Sleeping\n\nswan_fct %>% tbl_summary(\n  by = Financial_Strain, # stratifying by this binary variable (primary predictor)\n  label = list(                       # row name appearance\n    Financial_Strain ~ \"Financial Strain\",\n    Race ~ \"Race/Ethnicity\",\n    Marital_Status ~ \"Marital Status\", \n    Smoke ~ \"Smoking Status\", \n    Age ~ \"Age\",\n    BMI ~ \"Body Mass Index (BMI)\",\n    SBP ~ \"Systolic Blood Pressure (SBP)\",\n    Physical_Activity ~ \"Physical Activity\", \n    HealthRank ~ \"Health Status\", \n    Difficulty_Sleeping ~ \"Difficulty Sleeping\"\n  ),\n  include = c(     # which variables from dataset to include in table \n    Financial_Strain, Age, Race, Education, Marital_Status, SBP, Smoke, BMI, Physical_Activity, HealthRank, Difficulty_Sleeping)) %>% \n  #add_overall() %>% ## this didn't run for us\n  modify_header(label ~ \"Variable\") %>%\n  modify_footnote(\n    all_stat_cols() ~ \"Median (IQR); Frequency (%)\") %>%\n  modify_caption(\"Table 1.\") %>%\n bold_labels() %>% \n## to use gt() functions, add `as_gt()` then include gt() functions\n as_gt() %>% \n  tab_header(\n    title = \"Participants' characteristics, stratified by Financial Strain\",\n    subtitle = \"Financial Strain: How hard is it to afford basic living expenses?\") %>% \n  tab_options(heading.title.font.size = \"small\",\n              heading.title.font.weight = \"bold\",\n              heading.subtitle.font.size = \"large\",\n              heading.subtitle.font.weight = \"80\",\n              heading.align = \"right\") %>% \n  opt_table_outline(style = \"solid\", width = px(5)) %>% \n  opt_stylize(style = 6, color = \"gray\")\n\n\n\n\n\n  Table 1.\n  \n    \n      Participants' characteristics, stratified by Financial Strain\n    \n    \n      Financial Strain: How hard is it to afford basic living expenses?\n    \n    \n      Variable\n      Somewhat/Very hard, N = 1,9671\n      Not hard, N = 1,3101\n    \n  \n  \n    Age\n46.00 (44.00, 48.00)\n45.00 (43.00, 48.00)\n    Race/Ethnicity\n\n\n        Black/African American\n497 (25%)\n431 (33%)\n        Chinese/Chinese American\n180 (9.2%)\n70 (5.3%)\n        Japanese/Japanese American\n196 (10.0%)\n84 (6.4%)\n        Caucasian/White Non-Hispanic\n1,049 (53%)\n495 (38%)\n        Hispanic\n45 (2.3%)\n230 (18%)\n    Education\n\n\n        Less than high school\n45 (2.3%)\n191 (15%)\n        High school graduate\n278 (14%)\n300 (23%)\n        Some college/technical school\n594 (30%)\n454 (35%)\n        College graduate\n448 (23%)\n211 (16%)\n        Post graduate education\n594 (30%)\n143 (11%)\n        Unknown\n8\n11\n    Marital Status\n\n\n        Single, never married\n259 (13%)\n178 (14%)\n        Currently married/living as married\n1,399 (72%)\n742 (58%)\n        Separated or divorced\n265 (14%)\n327 (25%)\n        Widowed\n24 (1.2%)\n41 (3.2%)\n        Unknown\n20\n22\n    Systolic Blood Pressure (SBP)\n112 (104, 123)\n118 (108, 128)\n        Unknown\n7\n12\n    Smoking Status\n\n\n        Never smoked\n1,152 (60%)\n713 (56%)\n        Former smoker\n525 (27%)\n268 (21%)\n        Current smoker\n259 (13%)\n298 (23%)\n        Unknown\n31\n31\n    Body Mass Index (BMI)\n25 (22, 29)\n27 (23, 32)\n        Unknown\n46\n94\n    Physical Activity\n\n\n        Much less than other women your age\n60 (3.1%)\n87 (7.0%)\n        Somewhat less than other women your age\n263 (14%)\n228 (18%)\n        About the same as other women your age\n742 (39%)\n525 (42%)\n        Somewhat more than other women your age\n566 (30%)\n259 (21%)\n        Much more than other women your age\n283 (15%)\n141 (11%)\n        Unknown\n53\n70\n    Health Status\n\n\n        Excellent\n517 (27%)\n175 (14%)\n        Very good\n790 (41%)\n382 (30%)\n        Good\n481 (25%)\n464 (36%)\n        Fair\n143 (7.3%)\n223 (17%)\n        Poor\n15 (0.8%)\n46 (3.6%)\n        Unknown\n21\n20\n    Difficulty Sleeping\n\n\n        None\n1,194 (77%)\n664 (73%)\n        Yes\n362 (23%)\n245 (27%)\n        Unknown\n411\n401\n  \n  \n  \n    \n      1 Median (IQR); Frequency (%)"
  }
]